---
title: "Case Study 2 - Marketing"
author: "Lijju Mathew"
date: "7/23/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries Required
```{r library}
library(gdata)
library(caret)
library(car)
library(tidyverse)
library(ggplot2)
library(gplots)
library(caTools)
library(dplyr)
library(magrittr)
library(readr)
library(survival)
library(nlme)
library(gridExtra) #grid.arrange()
library(class)
library(forcats)
library(MASS)
library(GGally)
library(tidyr)
library(maps)
library(mapproj)
library(stringr)
library(rmarkdown)
library(knitr)
library(jsonlite)
library(RCurl)
library(class)
library(httr)
library(mice)
library(corrplot)
library(GoodmanKruskal) 
library(plyr)
library(ggpubr)
library(rsample)
library(DMwR)
library(ROSE)
library(InformationValue)
```

## Reading Data
```{r Read Data}
mktg_raw <- read.table('/Users/lijjumathew/Library/Mobile Documents/com~apple~CloudDocs/Lijju/SMU/Courses/Applied Statistics/Project/Project2/bank-additional-full.csv', na.strings=c("unknown"),sep=";", header=TRUE)
# mktg_raw <- read.csv(file.choose())
head(mktg_raw)
View(mktg_raw)
summary(mktg_raw)
str(mktg_raw)
```

## Missing  Values & Imputation
```{r Missing values - unknowns}

# Find the no of attributes with missing values
sort(sapply(mktg_raw, function(x) sum(is.na(x))), decreasing = T)

#Missing data and percentage plot
missing.values <- mktg_raw %>%
  gather(key = "key", value = "val") %>%
  dplyr::mutate(isna = is.na(val)) %>%
  dplyr::group_by(key) %>%
  dplyr::mutate(total = n()) %>%
  dplyr::group_by(key, total, isna)%>%
  dplyr::summarise(num.isna = n())%>%
  dplyr::mutate(pct = num.isna / total * 100)

levels <- (missing.values %>% filter(isna == T) %>% arrange(desc(pct)))$key

percentage.plot <- missing.values %>%
      ggplot() + geom_bar(aes(x = reorder(key, desc(pct)), y = pct, fill=isna), stat = 'identity', alpha=0.8) +
      scale_x_discrete(limits = levels) +
      scale_fill_manual(name = "", values = c('steelblue', 'tomato3'), labels = c("Present", "Missing")) +
      coord_flip() + 
      labs(title = "Percentage of missing/unknown values", x = 'Variable', y = "% of missing/unknown values")
percentage.plot

tempData <- mice(mktg_raw,m=1,maxit=0,meth='fastpmm',seed=500)
mktg_comp <- complete(tempData,1)
drop <- c("default")
mktg_imp= mktg_comp[,!(names(mktg_comp) %in% drop)]
summary(mktg_imp)
str(mktg_imp)
sort(sapply(mktg_imp, function(x) sum(is.na(x))), decreasing = T)
glimpse(mktg_imp)
```

## EDA Box plots, Scatter plots, Correlation plots
```{r EDA - Box plots, Scatter plots, Correlation plots}

summary(mktg_raw)
str(mktg_raw)
summary(mktg_imp)
mktg_imp_conti <- mktg_imp[, !sapply(mktg_imp, is.factor)]
mktg_imp_categ <- mktg_imp[, sapply(mktg_imp, is.factor)]

# Box plots to find outliers
boxplot(mktg_imp_conti)
ggplot(stack(mktg_imp_conti),aes(x = ind, y = values) ) + geom_boxplot()
boxplot(mktg_imp_conti$age)
boxplot(mktg_imp_conti$pdays)

# Pdays is number of days passed after client was last contacted from previous campaign.
# Value of 999 means the client was not contacted. This values is sticking out as an outlier. Hence recoding the value to -1
mktg_imp_conti$pdays <- as.integer(revalue(as.character(mktg_imp_conti$pdays), c("999" = "-1")))
boxplot(mktg_imp_conti$pdays)

boxplot(mktg_imp_conti$duration)

# ScatterPlot
head(mktg_imp)
#pairs(mktg_imp_conti, pch=19)

# Computing the p value of correlations
cor.mtest <- function(mat, ...) {
    mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat<- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
        for (j in (i + 1):n) {
            tmp <- cor.test(mat[, i], mat[, j], ...)
            p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
        }
    }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
p.mat <- cor.mtest(mktg_imp_conti)
correlation <- cor(mktg_imp_conti)
# Correlation plot with significance level of 0.05
corrplot(correlation, type="upper", order="hclust", p.mat = p.mat, sig.level = 0.05)


cat_cor <- GKtauDataframe(mktg_imp_categ)
plot(cat_cor, corrColors = "blue", type="upper")

# Heat map to find correlation
heatmap.2(correlation,col=redgreen(75), 
          density.info="none", trace="none", dendrogram=c("row"), 
          symm=F,symkey=T,symbreaks=T, scale="none")


ggscatter(mktg_imp_conti, x = "pdays", y = "previous", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Pdays", ylab = "Previous Contacts")


attach(mktg_imp)
summary(mktg_imp)
str(mktg_imp)
plot(mktg_imp$y~mktg_imp$marital,col=c("red","blue"))
plot(mktg_imp$y~mktg_imp$job,col=c("red","blue"))
plot(mktg_imp$y~mktg_imp$education,col=c("red","blue"))
plot(mktg_imp$y~mktg_imp$housing,col=c("red","blue"))
plot(mktg_imp$y~mktg_imp$loan,col=c("red","blue"))
plot(mktg_imp$y~mktg_imp$contact,col=c("red","blue"))
plot(mktg_imp$y~mktg_imp$poutcome,col=c("red","blue"))
plot(mktg_imp$marital~mktg_imp$job,col=c("red","blue","green"))
plot(mktg_imp$housing~mktg_imp$loan,col=c("red","blue","green"))

```

From the correlation plot for continuous variables below variables are correlated.
1) pdays -> previous, nr.employed
2) previous -> pdays, nr.employed, emp.var.rate, euribor3m
3) cons.price.id -> nr.employed, emp.var.rate, euribor3m
4) nr.employed -> emp.var.rate, euribor3m
5) emp.var.rate -> euribor3m

From the correlation plot for continuous variables below variables are correlated.
1) poutcome -> y (if the previous outcome of the campaign is success then more subscribes)

## EDA PCA
```{r EDA- PCA}
pc.result<-prcomp(mktg_imp_conti,scale.=TRUE)
pc.result
pc.scores<-pc.result$x
pc.scores<-data.frame(pc.scores)
pc.scores$y<-mktg_imp$y

# variance
pr_var <- (pc.result$sdev)^2
# % of variance explained
prop_varex <- pr_var/sum(pr_var)
# show percentage of variance of each component
plot(prop_varex, xlab = "Principal Component", ylab = "Proportion of Variance Explained", type = "b" )
# Scree Plot
plot(cumsum(prop_varex), xlab = "Principal Component", ylab = "Cumulative Proportion of Variance Explained", type = "b" )
# first 6 components are responsible for around 90% of variance
cumsum(prop_varex)
# This result means that the majority of information contained in 11 numeric variables can be reduced to 6 principal components.

ggplot(data = pc.scores, aes(x = PC4, y = PC5)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Auto")

```


## Splitting and Down Sampling
```{r train test split with down sampling}
dim(mktg_imp)
prop.table(table(mktg_imp$y))

set.seed(1234)
split <- initial_split(mktg_imp, prop = .8, strata = y)
train <- training(split)
test <- testing(split)

table(train$y)
prop.table(table(train$y))

table(test$y)
prop.table(table(test$y))


# 50-50 down sampling
train_down_s_50 <- SMOTE(form = y~.,data = train, k = 5, perc.over = 100)
attach(train_down_s_50)
table(train_down_s_50$y)
prop.table(table(train_down_s_50$y))

# 60-40 down sampling
train_bal_s_60 <- SMOTE(form = y~.,data = train, k = 5, perc.over = 390)
table(train_bal_s_60$y)
prop.table(table(train_bal_s_60$y))

# Ovun down sampling
train_bal_o <-ovun.sample(y ~ .,data = train, method = "under", N = 9280)$data
table(train_bal1$y)
prop.table(table(train_bal1$y))

# ROSE Down sampling
train_bal_r <- ROSE(y ~ ., data = train)$data
table(train_bal_r$y)
prop.table(table(train_bal_r$y))

```

## Logit - Full Model - Original data 90.8%
```{r}
set.seed(1234)

logit.os.Train <- train
logit.os.Test <- test
logit.os.data$y <- as.factor(logit.os.Train$y)

# Build Full Model
full.os.log<-glm(y~.,family="binomial",data=logit.os.Train)

# Step Model
step.os.log<-full.os.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os.log)

# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))

# VIF Scores
vif(step.os.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
```

## Logit - Full Model - SMOTE 50- 50 Sampled data 86.6%
```{r}
set.seed(1234)

logit.os.Train <- train_down_s_50
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
attach(logit.os.Train)
# Build Full Model
full.os.log<-glm(y~.,family="binomial",data=logit.os.Train)

# Step Model
step.os.log<-full.os.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os.log)

# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))

# VIF Scores
vif(step.os.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
```

## Logit - Full Model - SMOTE 60- 40 Sampled data 88.6%
```{r}
set.seed(1234)

logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
attach(logit.os.Train)
# Build Full Model
full.os.log<-glm(y~.,family="binomial",data=logit.os.Train)

# Step Model
step.os.log<-full.os.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os.log)

# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))

# VIF Scores
vif(step.os.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
```

## Logit - Full Model - ROSE 85.5%
```{r}
set.seed(1234)

logit.os.Train <- train_bal_r
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
attach(logit.os.Train)
# Build Full Model
full.os.log<-glm(y~.,family="binomial",data=logit.os.Train)

# Step Model
step.os.log<-full.os.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os.log)

# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))

# VIF Scores
vif(step.os.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
```





1. Using SMOTE 60-40 sampled data as the train data
2. From the full model and the significant predictors from step wise model and taking into account the correlated predictors
   below are the some of the models.


## Logit - Reduced Model 1(job,month,campaign,pdays,poutcome,emp.var.rate,cons.price.idx) 
## Accuracy - 85.3%, Sensitivity - 0.1109591, Specificity - 0.4228695
```{r reduced model 1}
set.seed(1234)

logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
attach(logit.os.Train)
# Build Full Model
red.mod1.log <- glm(y~ job + month + campaign + pdays + poutcome + emp.var.rate + cons.price.idx ,family="binomial",data=logit.os.Train)

# Step Model
step.os.log <- red.mod1.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os.log)

# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))

# VIF Scores
vif(step.os.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test, type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
print("Sensitivity")
sensitivity(class.step,logit.os.Test$y,threshold = cutoff)
print("Specificity")
specificity(class.step,logit.os.Test$y,threshold = cutoff)
```


## Logit - Reduced Model 2(age,job,contact,day_of_week,campaign,pdays,poutcome,emp.var.rate,nr.employed)
## Accuracy - 84.4%, Sensitivity - 0.1156109, Specificity - 0.4692557
```{r reduced model 2}
set.seed(1234)

logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod2.log <- glm(y ~ age + job + contact + day_of_week + campaign + pdays + poutcome + emp.var.rate + nr.employed , family="binomial",data=logit.os.Train)

# Step Model
step.os.log <- red.mod2.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os.log)

# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))

# VIF Scores
vif(step.os.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
print("Sensitivity")
sensitivity(class.step,logit.os.Test$y,threshold = cutoff)
print("Specificity")
specificity(class.step,logit.os.Test$y,threshold = cutoff)
```


## Logit - Reduced Model 3(job,contact,day_of_week,campaign,pdays,poutcome,euribor3m,nr.employed)
## Accuracy - 86.2%, Sensitivity - 0.09057327, Specificity - 0.5091694
```{r reduced model 3}
set.seed(1234)

logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod3.log <- glm(y ~ job + contact + day_of_week + campaign + pdays + poutcome + euribor3m + nr.employed , family="binomial",data=logit.os.Train)

# Step Model
step.os.log <- red.mod3.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os.log)

# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))

# VIF Scores
vif(step.os.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
print("Sensitivity")
sensitivity(class.step,logit.os.Test$y,threshold = cutoff)
print("Specificity")
specificity(class.step,logit.os.Test$y,threshold = cutoff)
```


## Logit - Reduced Model 4(job,contact,day_of_week,campaign,pdays,poutcome,euribor3m)
## Accuracy - 79.7%, Sensitivity - 0.1752634, Specificity - 0.4142395
```{r reduced model 4}
set.seed(1234)

logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod4.log <- glm(y ~ job + contact + day_of_week + campaign + pdays + poutcome + euribor3m , family="binomial",data=logit.os.Train)

# Step Model
step.os.log <- red.mod4.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os.log)

# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))

# VIF Scores
vif(step.os.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"),levels=c("no","yes"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
print("Sensitivity")
sensitivity(class.step,logit.os.Test$y,threshold = cutoff)
print("Specificity")
specificity(class.step,logit.os.Test$y,threshold = cutoff)
```


## Logit - Reduced Model 5(job,contact,education,day_of_week,campaign,pdays,poutcome,euribor3m,nr.employed)
## Accuracy - 84.4%, Sensitivity - 0.1156109, Specificity - 0.4692557
```{r reduced model 5}
set.seed(1234)

logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod5.log <- glm(y ~ job + education + contact + day_of_week + campaign + pdays + poutcome + euribor3m + nr.employed , family="binomial",data=logit.os.Train)

# Step Model
step.os.log <- red.mod5.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os.log)

# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))

# VIF Scores
vif(step.os.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
print("Sensitivity")
sensitivity(class.step,logit.os.Test$y,threshold = cutoff)
print("Specificity")
specificity(class.step,logit.os.Test$y,threshold = cutoff)
```


## Logit - Reduced Model 6(job,education,campaign,poutcome,nr.employed)
## Accuracy - 86.0%, Sensitivity - 0.09276235, Specificity - 0.5080906
```{r reduced model 6}
set.seed(1234)

logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod6.log <- glm(y ~ job + education  +  campaign  + poutcome + nr.employed , family="binomial",data=logit.os.Train)

# Step Model
step.os.log <- red.mod6.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os.log)

# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))

# VIF Scores
vif(step.os.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
print("Sensitivity")
sensitivity(class.step,logit.os.Test$y,threshold = cutoff)
print("Specificity")
specificity(class.step,logit.os.Test$y,threshold = cutoff)
```


## Logit - Reduced Model 7(job,education,campaign,pdays,poutcome,nr.employed, emp.var.rate)
## Accuracy - 84.2%, Sensitivity - 88, Specificity - 63
```{r reduced model 7}
set.seed(1234)

logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod7.log <- glm(y ~ job + education + campaign + pdays + poutcome + nr.employed + emp.var.rate, family="binomial",data=logit.os.Train)

# Step Model
step.os.log <- red.mod7.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os.log)

# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))

# VIF Scores
vif(step.os.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff <- 0.5
class.step <- factor(ifelse(fit.pred.step > cutoff,"yes","no"))
conf.step <- table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
print("Sensitivity")
sensitivity(class.step, logit.os.Test$y,threshold = cutoff)
print("Specificity")
specificity(class.step,logit.os.Test$y,threshold = cutoff)
plotROC(logit.os.Test$y, fit.pred.step)


confusionMatrix(data=class.step , reference=logit.os.Test$y)

library(ROCR)
pr <- prediction(fit.pred.step, logit.os.Test$y)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
```