---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

## Load libraries
```{r}
library(GGally)
library(scales)
library(formattable)
library(leaps)
library(car)
library(lme4)
library(mlbench)
library(caret)
library(tidyr)
library(MASS)
library(corrr)
library(randomForest)
library(ROSE)
library(arm)
library(glmnet)
library(imputeMissings)
library(ggvis)
library(mice)
library(ISLR)
library(plyr)
library(dplyr)
library(ISOweek)
library(corrplot)
library(PerformanceAnalytics)
library(psych)
library(psychTools)
library(mda)
library(klaR)
library(aod)
library(ggfortify)
library(factoextra)
library(rgl)
library(tree)
library(randomForest)
library(ridge)
library(ggplot2)
library(RColorBrewer)
library(corrplot)
library(reshape2)
library(pls)
library(bestNormalize)
library(ResourceSelection)
```

## Load in dataset and look at data
```{r}
# import data
datain <- read.csv("D:/MS Data Science/SMU/6372 - Applied Stats/Project 2/bank-additional-full.csv", header = TRUE)

# View data
str(datain)
summary(datain)

# convert to dataframe
datain <- as.data.frame(datain)

# Convert all "unknown" to NA
datain[datain == "unknown"] <- NA

# Count NA values in each column
na_count <-sapply(datain, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
na_count

# Count number of yes and no
y_count <-sapply(datain, function(y) sum(length(which(y=="yes"))))
y_count <- data.frame(y_count)
y_count

n_count <-sapply(datain, function(y) sum(length(which(y=="no"))))
n_count <- data.frame(n_count)
n_count

# there are 36,548 No's and 4,640 Yes's.
```

## Impute missing data & convert to numeric
```{r}
# convert to factor for imputation
datain$job <- as.factor(datain$job)
datain$marital <- as.factor(datain$marital)
datain$education <- as.factor(datain$education)
datain$default <- as.factor(datain$default)
datain$housing <- as.factor(datain$housing)
datain$loan <- as.factor(datain$loan)
datain$contact <- as.factor(datain$contact)
datain$month <- sapply(datain$month,function(x) grep(paste("(?i)",x,sep=""),month.abb))
datain$month <- as.factor(datain$month)
datain$day_of_week <- dplyr::recode(datain$day_of_week, 
       "mon"="1",
       "tue"="2",
       "wed"="3",
       "thu"="4",
       "fri"="5",
       "sat"="6",
       "sun"="7")
datain$day_of_week <- as.factor(datain$day_of_week)
datain$poutcome <- as.factor(datain$poutcome)

# setting up parms
init = mice(datain, maxit=0) 
meth = init$method
predM = init$predictorMatrix

# impute the following columns
meth[c("job", "marital","education","default","housing", "loan")]="polyreg"

# imputate & new dataset
set.seed(123)
imputed = mice(datain, method=meth, predictorMatrix=predM, m=1)
imputed <- complete(imputed)
sapply(imputed, function(x) sum(is.na(x)))

# convert to numeric
imputed$job <- as.numeric(imputed$job)
imputed$marital <- as.numeric(imputed$marital)
imputed$education <- as.numeric(imputed$education)
imputed$default <- as.numeric(imputed$default)
imputed$housing <- as.numeric(imputed$housing)
imputed$loan <- as.numeric(imputed$loan)
imputed$month <- as.numeric(imputed$month)
imputed$contact <- as.numeric(imputed$contact)
imputed$poutcome <- as.numeric(imputed$poutcome)
imputed$day_of_week <- as.numeric(imputed$day_of_week)

EDAD <- imputed
```

## EDA (don't bulk run. Open and pick plot)
```{r}
EDAD$y <- as.factor(EDAD$y)
EDAD$y <- as.numeric(EDAD$y)

# Histogram of each variable
par(mfrow = c(5, 4))
MASS::truehist(EDAD$age)
MASS::truehist(EDAD$job)
MASS::truehist(EDAD$marital)
MASS::truehist(EDAD$education)
MASS::truehist(EDAD$default)
MASS::truehist(EDAD$housing)
MASS::truehist(EDAD$loan)
MASS::truehist(EDAD$contact)
MASS::truehist(EDAD$month)
MASS::truehist(EDAD$day_of_week)
MASS::truehist(EDAD$duration)
MASS::truehist(EDAD$campaign)
MASS::truehist(EDAD$pdays)
MASS::truehist(EDAD$previous)
MASS::truehist(EDAD$poutcome)
MASS::truehist(EDAD$emp.var.rate)
MASS::truehist(EDAD$cons.price.idx)
MASS::truehist(EDAD$cons.conf.idx)
MASS::truehist(EDAD$euribor3m)
MASS::truehist(EDAD$nr.employed)

# Correlation to y variable
x <- EDAD %>% correlate() %>% focus(y)
x

# Pairs plot. Colored by Yes/No (LONG RUN TIME)
#pairs(EDAD[1:20],main="Whole Dataset", pch=19, col=as.numeric(EDAD$y)+1)
#mtext("y column: red-> Yes; green-> No", 1, line=3.7,cex=.8)
```

## Normalization Techniques - EDA
```{r}
BNAge <- bestNormalize(as.numeric(EDAD$age, allow_orderNorm = TRUE, out_of_sample = FALSE))
BNAge
# For Age, no transform is neccessary

BNJob <- bestNormalize(as.numeric(EDAD$job,allow_orderNorm = FALSE, out_of_sample = FALSE))
BNJob
# For Job, no transform is neccessary

BNCam <- bestNormalize(as.numeric(EDAD$campaign,allow_orderNorm = FALSE, out_of_sample = FALSE))
BNCam
# For Campaign, no transform is neccessary

BNConsPrice <- bestNormalize(as.numeric(EDAD$cons.price.idx,allow_orderNorm = FALSE, out_of_sample = FALSE))
BNConsPrice
# For cons.price.idx, no transform is neccessary

BNConsConf <- bestNormalize(as.numeric(EDAD$cons.conf.idx,allow_orderNorm = FALSE, out_of_sample = FALSE))
BNConsConf
# For cons.conf.idx, no transform is neccessary

BNNREmp <- bestNormalize(as.numeric(EDAD$nr.employed,allow_orderNorm = FALSE, out_of_sample = FALSE))
BNNREmp
# For nr.employed, no transform is neccessary
```

# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

# Objective 1 - Simple Logistic Regression
## Final Datasets to be used in objective 1
```{r}
set.seed(123)

# Select transformations
EDAD2 <- EDAD

# EDA dataset - narrowed down to columns to be used in regression
edadata <- EDAD2
edadata <- edadata %>%
  select(age, job, marital, education, contact, month, day_of_week, campaign, pdays, previous,
         poutcome, cons.price.idx, cons.conf.idx, y)

obj1.data <- edadata
```

## Logit with original data 89.69%
```{r}
set.seed(123)

logit.os.data <- obj1.data
logit.os.data$y <- as.factor(logit.os.data$y)

# Test & train
trainIndex <- createDataPartition(logit.os.data$y, p = .7,
                                  list = FALSE,
                                  times = 1)

logit.os.Train <- logit.os.data[ trainIndex,]
logit.os.Test  <- logit.os.data[-trainIndex,]

# Build Full Model
full.os.log <- glm( y ~ .,family = binomial(link="logit"),data=logit.os.Train)

# Forward Model
fwd.os.log <- step(full.os.log,
     direction="forward",
     data=logit.os.Train)

# Backward Model
bck.os.log <- step(full.os.log,
     direction="backward",
     data=logit.os.Train)

# Stepwise Model
step.os.log <- step(full.os.log,
     direction="both",
     data=logit.os.Train)

# Model Fit
hoslem.test(step.os.log$y, fitted(step.os.log), g=14)
hoslem.test(fwd.os.log$y, fitted(fwd.os.log), g=14)
hoslem.test(bck.os.log$y, fitted(bck.os.log), g=14)

# Model Plots
#plot(fwd.os.log)

# Model Summary
summary(step.os.log)
summary(fwd.os.log)
summary(bck.os.log)

# Error Metrics
step.os.aic <- step.os.log$aic
fwd.os.aic <- fwd.os.log$aic
bck.os.aic <- bck.os.log$aic

# Confidence Intervals
step.os.ci <- exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
fwd.os.ci <- exp(cbind("Odds ratio" = coef(fwd.os.log), confint.default(fwd.os.log, level = 0.95)))
bck.os.ci <- exp(cbind("Odds ratio" = coef(bck.os.log), confint.default(bck.os.log, level = 0.95)))

# VIF Scores
step.os.vif <- vif(step.os.log)
fwd.os.vif <- vif(fwd.os.log)
bck.os.vif <- vif(bck.os.log)

# Predictions & Accuracy - Stepwise
cutoff <- 0.5
fit.pred.os.step <- predict(step.os.log,newdata=logit.os.Test,type="response")
class.os.step <- factor(ifelse(fit.pred.os.step>cutoff,"yes","no"))
step.os.cm <- confusionMatrix(class.os.step, logit.os.Test$y)
step.os.cm

step.os.logit.acc <- 0.8969
step.os.logit.sens <- 0.9882
step.os.logit.spec <- 0.1774

# Predictions & Accuracy - Forward
cutoff <- 0.5
fit.pred.os.fwd <- predict(fwd.os.log,newdata=logit.os.Test,type="response")
class.os.fwd <- factor(ifelse(fit.pred.os.fwd>cutoff,"yes","no"))
fwd.os.cm <- confusionMatrix(class.os.fwd, logit.os.Test$y)
fwd.os.cm

fwd.os.logit.acc <- 0.8969
fwd.os.logit.sens <- 0.9882
fwd.os.logit.spec <- 0.1774

# Predictions & Accuracy - Backward
cutoff <- 0.5
fit.pred.os.bck <- predict(bck.os.log,newdata=logit.os.Test,type="response")
class.os.bck <- factor(ifelse(fit.pred.os.bck>cutoff,"yes","no"))
bck.os.cm <- confusionMatrix(class.os.bck, logit.os.Test$y)
bck.os.cm

bck.os.logit.acc <- 0.8969
bck.os.logit.sens <- 0.9882
bck.os.logit.spec <- 0.1774

# SLR Table
slr.os.acc <- rbind(step.os.logit.acc, fwd.os.logit.acc, bck.os.logit.acc)
slr.os.aics <- rbind(step.os.aic, fwd.os.aic, bck.os.aic)
slr.os.sens <- rbind(step.os.logit.sens, fwd.os.logit.sens, bck.os.logit.sens)
slr.os.spec <- rbind(step.os.logit.spec, fwd.os.logit.spec, bck.os.logit.spec)
slr.os.results <- cbind(slr.os.acc, slr.os.aics, slr.os.sens, slr.os.spec)
colnames(slr.os.results) <- c("Accuracy", "AIC", "Sensitivity", "Specificity")
rownames(slr.os.results) <- c("Stepwise", "Forward", "Backward")
slr.os.results
```
## Logit with balanced data 69.47
```{r}
set.seed(123)

logit.bs.data <- obj1.data
logit.bs.data$y <- as.factor(logit.os.data$y)

# Train Index
trainIndex <- createDataPartition(logit.bs.data$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj1.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj1.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.factor(balanced_split_data$y)

# Generate train/test datasets
logit.bs.Train <- balanced_split_data
logit.bs.Test  <- logit.bs.data[-trainIndex,]

# Build Full Model
full.bs.log <- glm( y ~ .,family = binomial(link="logit"),data=logit.bs.Train)

# Forward Model
fwd.bs.log <- step(full.bs.log,
     direction="forward",
     data=logit.bs.Train)

# Backward Model
bck.bs.log <- step(full.bs.log,
     direction="backward",
     data=logit.bs.Train)

# Stepwise Model
step.bs.log <- step(full.bs.log,
     direction="both",
     data=logit.bs.Train)

# Model Fit
hoslem.test(step.bs.log$y, fitted(step.bs.log), g=14)
hoslem.test(fwd.bs.log$y, fitted(fwd.bs.log), g=14)
hoslem.test(bck.bs.log$y, fitted(bck.bs.log), g=14)

# Model Plots
#plot(fwd.bs.log)

# Model Summary
summary(step.bs.log)
summary(fwd.bs.log)
summary(bck.bs.log)

# Error Metrics
step.bs.aic <- step.bs.log$aic
fwd.bs.aic <- fwd.bs.log$aic
bck.bs.aic <- bck.bs.log$aic

# Confidence Intervals
step.bs.ci <- exp(cbind("Odds ratio" = coef(step.bs.log), confint.default(step.bs.log, level = 0.95)))
fwd.bs.ci <- exp(cbind("Odds ratio" = coef(fwd.bs.log), confint.default(fwd.bs.log, level = 0.95)))
bck.bs.ci <- exp(cbind("Odds ratio" = coef(bck.bs.log), confint.default(bck.bs.log, level = 0.95)))

# VIF Scores
step.bs.vif <- vif(step.bs.log)
fwd.bs.vif <- vif(fwd.bs.log)
bck.bs.vif <- vif(bck.bs.log)

# Predictions & Accuracy - Stepwise
cutoff <- 0.5
fit.pred.bs.step <- predict(step.bs.log,newdata=logit.bs.Test,type="response")
class.bs.step <- factor(ifelse(fit.pred.bs.step>cutoff,"yes","no"),levels=c("no","yes"))
conf.bs.step <- table(class.bs.step,logit.bs.Test$y)
print("Confusion matrix for Stepwise")
conf.bs.step

step.bs.logit.acc <- sum(diag(conf.bs.step))/sum(conf.bs.step)
step.bs.logit.sens <- (7705)/(7705+513)
step.bs.logit.spec <- (879)/(879+3259)

# Predictions & Accuracy - Forward
cutoff <- 0.5
fit.pred.bs.fwd <- predict(fwd.bs.log,newdata=logit.bs.Test,type="response")
class.bs.fwd <- factor(ifelse(fit.pred.bs.fwd>cutoff,"yes","no"),levels=c("no","yes"))
conf.bs.fwd <- table(class.bs.fwd,logit.bs.Test$y)
print("Confusion matrix for Forward")
conf.bs.fwd

fwd.bs.logit.acc <- sum(diag(conf.bs.fwd))/sum(conf.bs.fwd)
fwd.bs.logit.sens <- (7718)/(7718+516)
fwd.bs.logit.spec <- (876)/(879+3246)

# Predictions & Accuracy - Backward
cutoff <- 0.5
fit.pred.bs.bck <- predict(bck.bs.log,newdata=logit.bs.Test,type="response")
class.bs.bck <- factor(ifelse(fit.pred.bs.bck>cutoff,"yes","no"),levels=c("no","yes"))
conf.bs.bck <- table(class.bs.bck,logit.bs.Test$y)
print("Confusion matrix for Backward")
conf.bs.bck

bck.bs.logit.acc <- sum(diag(conf.bs.bck))/sum(conf.bs.bck)
bck.bs.logit.sens <- (7705)/(7705+513)
bck.bs.logit.spec <- (879)/(879+3259)

# SLR Table
slr.bs.acc <- rbind(step.bs.logit.acc, fwd.bs.logit.acc, bck.bs.logit.acc)
slr.bs.aics <- rbind(step.bs.aic, fwd.bs.aic, bck.bs.aic)
slr.bs.sens <- rbind(step.bs.logit.sens, fwd.bs.logit.sens, bck.bs.logit.sens)
slr.bs.spec <- rbind(step.bs.logit.spec, fwd.bs.logit.spec, bck.bs.logit.spec)
slr.bs.results <- cbind(slr.bs.acc, slr.bs.aics, slr.bs.sens, slr.bs.spec)
colnames(slr.bs.results) <- c("Accuracy", "AIC", "Sensitivity", "Specificity")
rownames(slr.bs.results) <- c("Stepwise", "Forward", "Backward")
slr.bs.results
```
## Logit with rose data 69.90%
```{r}
set.seed(123)

logit.rs.data <- obj1.data
logit.rs.data$y <- as.factor(logit.rs.data$y)

# Test & train
trainIndex <- createDataPartition(logit.rs.data$y, p = .7,
                                  list = FALSE,
                                  times = 1)

logit.rs.Train <- ROSE(y ~ .,data = logit.rs.data[ trainIndex,])$data
logit.rs.Test  <- logit.rs.data[-trainIndex,]

# Build Full Model
full.rs.log <- glm( y ~ .,family = binomial(link="logit"),data=logit.rs.Train)

# Forward Model
fwd.rs.log <- step(full.rs.log,
     direction="forward",
     data=logit.rs.Train)

# Backward Model
bck.rs.log <- step(full.rs.log,
     direction="backward",
     data=logit.rs.Train)

# Stepwise Model
step.rs.log <- step(full.rs.log,
     direction="both",
     data=logit.rs.Train)

# Model Fit
hoslem.test(step.rs.log$y, fitted(step.rs.log), g=14)
hoslem.test(fwd.rs.log$y, fitted(fwd.rs.log), g=14)
hoslem.test(bck.rs.log$y, fitted(bck.rs.log), g=14)

# Model Plots
#plot(fwd.rs.log)

# Model Summary
summary(step.rs.log)
summary(fwd.rs.log)
summary(bck.rs.log)

# Error Metrics
step.rs.aic <- step.rs.log$aic
fwd.rs.aic <- fwd.rs.log$aic
bck.rs.aic <- bck.rs.log$aic

# Confidence Intervals
step.rs.ci <- exp(cbind("Odds ratio" = coef(step.rs.log), confint.default(step.rs.log, level = 0.95)))
fwd.rs.ci <- exp(cbind("Odds ratio" = coef(fwd.rs.log), confint.default(fwd.rs.log, level = 0.95)))
bck.rs.ci <- exp(cbind("Odds ratio" = coef(bck.rs.log), confint.default(bck.rs.log, level = 0.95)))

# VIF Scores
step.rs.vif <- vif(step.rs.log)
fwd.rs.vif <- vif(fwd.rs.log)
bck.rs.vif <- vif(bck.rs.log)

# Predictions & Accuracy - Stepwise
cutoff <- 0.5
fit.pred.rs.step <- predict(step.rs.log,newdata=logit.rs.Test,type="response")
class.rs.step <- factor(ifelse(fit.pred.rs.step>cutoff,"yes","no"),levels=c("no","yes"))
conf.rs.step <- table(class.rs.step,logit.rs.Test$y)
print("Confusion matrix for Stepwise")
conf.rs.step

step.rs.logit.acc <- sum(diag(conf.rs.step))/sum(conf.rs.step)
step.rs.logit.sens <- (7806)/(7806+560)
step.rs.logit.spec <- (832)/(832+3158)

# Predictions & Accuracy - Forward
cutoff <- 0.5
fit.pred.rs.fwd <- predict(fwd.rs.log,newdata=logit.rs.Test,type="response")
class.rs.fwd <- factor(ifelse(fit.pred.rs.fwd>cutoff,"yes","no"),levels=c("no","yes"))
conf.rs.fwd <- table(class.rs.fwd,logit.rs.Test$y)
print("Confusion matrix for Forward")
conf.rs.fwd

fwd.rs.logit.acc <- sum(diag(conf.rs.fwd))/sum(conf.rs.fwd)
fwd.rs.logit.sens <- (7806)/(7806+560)
fwd.rs.logit.spec <- (832)/(832+3158)

# Predictions & Accuracy - Backward
cutoff <- 0.5
fit.pred.rs.bck <- predict(bck.rs.log,newdata=logit.rs.Test,type="response")
class.rs.bck <- factor(ifelse(fit.pred.rs.bck>cutoff,"yes","no"),levels=c("no","yes"))
conf.rs.bck <- table(class.rs.bck,logit.rs.Test$y)
print("Confusion matrix for Backward")
conf.rs.bck

bck.rs.logit.acc <- sum(diag(conf.rs.bck))/sum(conf.rs.bck)
bck.rs.logit.sens <- (7806)/(7806+560)
bck.rs.logit.spec <- (832)/(832+3158)

# SLR Table
slr.rs.acc <- rbind(step.rs.logit.acc, fwd.rs.logit.acc, bck.rs.logit.acc)
slr.rs.aics <- rbind(step.rs.aic, fwd.rs.aic, bck.rs.aic)
slr.rs.sens <- rbind(step.rs.logit.sens, fwd.rs.logit.sens, bck.rs.logit.sens)
slr.rs.spec <- rbind(step.rs.logit.spec, fwd.rs.logit.spec, bck.rs.logit.spec)
slr.rs.results <- cbind(slr.rs.acc, slr.rs.aics, slr.rs.sens, slr.rs.spec)
colnames(slr.rs.results) <- c("Accuracy", "AIC", "Sensitivity", "Specificity")
rownames(slr.rs.results) <- c("Stepwise", "Forward", "Backward")
slr.rs.results
```

## Objective 1 results table
```{r}
obj1.results <- rbind(slr.os.results, slr.bs.results, slr.rs.results)
rownames(obj1.results) <- c("Stepwise Orig", "Forward Orig", "Backward Orig",
                           "Stepwise Bal", "Forward Bal", "Backward Bal",
                           "Stepwise Rose", "Forward Rose", "Backward Rose")
obj1.results
```

# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

# Objective 2
## Final Datasets to be used in objective 2
```{r}
set.seed(123)

# Select transformations
EDAD2 <- EDAD
EDAD2$pdays <- ifelse(EDAD2$pdays == 999, -1, EDAD2$pdays)
EDAD2$age <- log(EDAD2$age)


# EDA dataset - narrowed down to columns to be used in regression
edadata <- EDAD2
edadata <- edadata %>%
  select(age, job, marital, education, contact, month, day_of_week, campaign, pdays, previous,
         poutcome, cons.price.idx, cons.conf.idx, y)

obj2.data <- edadata
```

# --------------------------------------------------------

## Simple with original 87.85%
```{r}
set.seed(123)

simple.o.lr <- obj2.data
simple.o.lr$y <- as.numeric(as.factor(simple.o.lr$y))

# Create Training and Test data -
simple.index <- sample(1:nrow(simple.o.lr), 0.7*nrow(simple.o.lr))  # row indices for training data
simple.o.train <- simple.o.lr[simple.index, ]  # model training data
simple.o.test  <- simple.o.lr[-simple.index, ]   # test data

# Build the model on training data -
simple.o.lm <- lm(y ~ ., data=simple.o.train)  # build the model
simple.o.pred <- as.numeric(round(predict(simple.o.lm, simple.o.test)))  # predict y

# AIC & BIC
simple.o.aic <- AIC(simple.o.lm)
simple.o.bic <- BIC(simple.o.lm)
simple.o.aic
simple.o.bic

# Summary
summary(simple.o.lm)

# Prediction accuracy and eror rates
actuals_preds <- data.frame(cbind(actuals=simple.o.test$y, predicteds=simple.o.pred))
min_max_accuracy <- mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals)
simple.o.acc <- min_max_accuracy
simple.o.acc
mape

# Error
simple.o.error <- mean(simple.o.test$y != simple.o.pred)
simple.o.error

# Confusion Matrix
simple.o.cm <- confusionMatrix(simple.o.pred, simple.o.test$y)
simple.o.cm

# Sensitivity and Specificity
simple.o.sens <- (251)/(251+147)
simple.o.sens
simple.o.spec <- (10858)/(10858+1101)
simple.o.spec

vif(simple.o.lm)
```
## Simple with balanced data 84.09%
```{r}
set.seed(123)

simple.b.lr <- obj2.data
simple.b.lr$y <- as.numeric(as.factor(simple.b.lr$y))

# Train Index
trainIndex <- createDataPartition(simple.b.lr$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.numeric(as.factor(balanced_split_data$y))

# Generate train/test datasets
simple.b.train <- balanced_split_data
simple.b.test  <- simple.b.lr[-simple.index, ]   # test data

# Build the model on training data -
simple.b.lm <- lm(y ~ ., data=simple.b.train)  # build the model
simple.b.pred <- predict(simple.b.lm, simple.b.test)  # predict y
simple.b.pred <- round(simple.b.pred)

# AIC & BIC
simple.b.aic <- AIC(simple.b.lm)
simple.b.bic <- BIC(simple.b.lm)

# Summary
summary(simple.b.lm)

# Prediction accuracy and eror rates
actuals_preds <- data.frame(cbind(actuals=simple.b.test$y, predicteds=simple.b.pred))
min_max_accuracy <- mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals)
simple.b.acc <- min_max_accuracy
simple.b.acc
mape

# Error
simple.b.error <- mean(simple.b.test$y != simple.b.pred)
simple.b.error

# Confusion Matrix
table(simple.b.test$y, simple.b.pred)

# Sensitivity and Specificity
simple.b.sens <- (7589)/(7589+3416)
simple.b.sens
simple.b.spec <- (837)/(837+515)
simple.b.spec

vif(simple.b.lm)
```
## Simple with rose data 83.28
```{r}
set.seed(123)

simple.r.lr <- obj2.data
simple.r.lr$y <- as.numeric(as.factor(simple.r.lr$y))

# Create Training and Test data -
simple.index <- sample(1:nrow(simple.r.lr), 0.7*nrow(simple.r.lr))  # row indices for training data
simple.r.train <- ROSE(y ~ .,data = simple.r.lr[simple.index, ])$data
simple.r.test  <- simple.r.lr[-simple.index, ]   # test data

# Build the model on training data -
simple.r.lm <- lm(y ~ ., data=simple.r.train)  # build the model
simple.r.pred <- predict(simple.r.lm, simple.r.test)  # predict y
simple.r.pred <- round(simple.r.pred)

# AIC & BIC
simple.r.aic <- AIC(simple.r.lm)
simplr.r.bic <- BIC(simple.r.lm)
simple.r.aic

# Summary
summary(simple.r.lm)

# Error
simple.r.error <- mean(simple.r.test$y != simple.r.pred)
simple.r.error

# Prediction accuracy and eror rates
actuals_preds <- data.frame(cbind(actuals=simple.r.test$y, predicteds=simple.r.pred))
min_max_accuracy <- mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals)
simple.r.acc <- min_max_accuracy
simple.r.acc
mape

# Confusion Matrix
table(simple.r.test$y, simple.r.pred)

# Sensitivity and Specificity
simple.r.sens <- (7380)/(7380+3625)
simple.r.sens
simple.r.spec <- (845)/(845+507)
simple.r.spec

vif(simple.r.lm)
```

# --------------------------------------------------------

## Logit with original data 89.69% (NEED INTERACTION TERMS AND COMPLEXITY)
```{r}
set.seed(123)

logit.os.data <- obj2.data
logit.os.data$y <- as.factor(logit.os.data$y)

# Test & train
trainIndex <- createDataPartition(logit.os.data$y, p = .7,
                                  list = FALSE,
                                  times = 1)

logit.os.Train <- logit.os.data[ trainIndex,]
logit.os.Test  <- logit.os.data[-trainIndex,]

# Build Full Model
full.os.log<-glm(y~.,family="binomial",data=logit.os.Train)

# Step Model
step.os.log<-full.os.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os.log)

# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))

# VIF Scores
vif(step.os.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"),levels=c("no","yes"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
step.os.logit.sens <- (10856)/(10856+1163)
step.os.logit.sens
step.os.logit.spec <- (229)/(229+108)
step.os.logit.spec


```
## Logit with balanced ata 71.49 (NEED INTERACTION TERMS AND COMPLEXITY)
```{r}
set.seed(123)

logit.bs.data <- obj2.data
logit.bs.data$y <- as.factor(logit.bs.data$y)

# Test & train
trainIndex <- createDataPartition(logit.bs.data$y, p = .7,
                                  list = FALSE,
                                  times = 1)

logit.bs.Train <- ovun.sample(y ~ .,data = logit.bs.data[ trainIndex,], method = "under")$data
logit.bs.Test  <- logit.bs.data[-trainIndex,]

# Build Full Model
full.bs.log<-glm(y~.,family="binomial",data=logit.bs.Train)

# Step Model
step.bs.log<-full.bs.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.bs.log)

# Error Metrics
step.bs.aic <- step.bs.log$aic
step.bs.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.bs.log), confint.default(step.bs.log, level = 0.95)))

# VIF Scores
vif(step.bs.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.bs.log,newdata=logit.bs.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"),levels=c("no","yes"))
conf.step<-table(class.step,logit.bs.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.bs.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.bs.logit.acc
step.bs.logit.sens <- (7698)/(7698+552)
step.bs.logit.sens
step.bs.logit.spec <- (840)/(840+2996)
step.bs.logit.spec
```
## Logit with rose data 69.90% (NEED INTERACTION TERMS AND COMPLEXITY)
```{r}
set.seed(123)

logit.rs.data <- obj2.data
logit.rs.data$y <- as.factor(logit.rs.data$y)

# Test & train
trainIndex <- createDataPartition(logit.rs.data$y, p = .7,
                                  list = FALSE,
                                  times = 1)

logit.rs.Train <- ROSE(y ~ .,data = logit.rs.data[ trainIndex,])$data
logit.rs.Test  <- logit.rs.data[-trainIndex,]

# Build Full Model
full.rs.log<-glm(y~.,family="binomial",data=logit.rs.Train)

# Step Model
step.rs.log<-full.rs.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.rs.log)

# Error Metrics
step.rs.aic <- step.rs.log$aic
step.rs.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.rs.log), confint.default(step.rs.log, level = 0.95)))

# VIF Scores
vif(step.rs.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.rs.log,newdata=logit.rs.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"),levels=c("no","yes"))
conf.step<-table(class.step,logit.rs.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.rs.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.rs.logit.acc
step.rs.logit.sens <- (7706)/(7706+554)
step.rs.logit.sens
step.rs.logit.spec <- (838)/(838+3258)
step.rs.logit.spec
```

# --------------------------------------------------------

## LDA with original data 89.69%
```{r}
set.seed(123)

# Split the data into training (70%) and test set (30%)
lda.o.data <- obj2.data
training.samples <- lda.o.data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
LDA.o.train.data <- lda.o.data[training.samples, ]
LDA.o.test.data <- lda.o.data[-training.samples, ]

# Estimate preprocessing parameters
o.preproc.param <- LDA.o.train.data %>% 
  preProcess(method = c("center", "scale"))
# Transform the data using the estimated parameters
LDA.o.train.transformed <- o.preproc.param %>% predict(LDA.o.train.data)
LDA.o.test.transformed <- o.preproc.param %>% predict(LDA.o.test.data)

# Model
lda.o.model <- lda(y ~ ., data = LDA.o.train.transformed)
lda.o.model

# Make predictions
lda.o.predictions <- lda.o.model %>% predict(LDA.o.test.transformed)

# Error
lda.o.error <- mean(LDA.o.test.data$y != lda.o.predictions$class)
lda.o.error

# Confusion Matrix
lda.o.cm <- confusionMatrix(lda.o.predictions$class, as.factor(LDA.o.test.data$y))
lda.o.cm

# Accuracy
lda.o.acc <- 0.897
lda.o.acc

# Sensitivity and Specificity
lda.o.sens <- 0.9857
lda.o.sens
lda.o.spec <- 0.1983
lda.o.spec
```
## LDA with balanced data 67.16%
```{r}
set.seed(123)

# Split the data into training (70%) and test set (30%)
set.seed(123)
lda.b.data <- obj2.data

# Train Index
trainIndex <- createDataPartition(lda.b.data$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.factor(balanced_split_data$y)

# Generate train/test datasets
LDA.b.train.data <- balanced_split_data
LDA.b.test.data <- lda.b.data[-training.samples, ]

# Estimate preprocessing parameters
b.preproc.param <- LDA.b.train.data %>% 
  preProcess(method = c("center", "scale"))

# Transform the data using the estimated parameters
LDA.b.train.transformed <- b.preproc.param %>% predict(LDA.b.train.data)
LDA.b.test.transformed <- b.preproc.param %>% predict(LDA.b.test.data)

# Model
lda.b.model <- lda(y ~ ., data = LDA.b.train.transformed)
lda.b.model

# Make predictions
lda.b.predictions <- lda.b.model %>% predict(LDA.b.test.transformed)

# Error
lda.b.error <- mean(LDA.b.test.data$y != lda.b.predictions$class)
lda.b.error

# Confusion Matrix
lda.b.cm <- confusionMatrix(lda.b.predictions$class, as.factor(LDA.b.test.data$y))
lda.b.cm

# Accuracy
lda.b.acc <- 0.6843
lda.b.acc

# Sensitivity and Specificity
lda.b.sens <- 0.6904
lda.b.sens
lda.b.spec <- 0.6365
lda.b.spec
```
## LDA with rose data 69.25%
```{r}
set.seed(123)

# Split the data into training (70%) and test set (30%)
set.seed(123)
lda.r.data <- obj2.data
training.samples <- lda.r.data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
LDA.r.train.data <- ROSE(y ~ .,data = lda.r.data[training.samples, ])$data
LDA.r.test.data <- lda.r.data[-training.samples, ]

# Estimate preprocessing parameters
r.preproc.param <- LDA.r.train.data %>% 
  preProcess(method = c("center", "scale"))
# Transform the data using the estimated parameters
LDA.r.train.transformed <- r.preproc.param %>% predict(LDA.r.train.data)
LDA.r.test.transformed <- r.preproc.param %>% predict(LDA.r.test.data)

# Model
lda.r.model <- lda(y ~ ., data = LDA.r.train.transformed)
lda.r.model

# Make predictions
lda.r.predictions <- lda.r.model %>% predict(LDA.r.test.transformed)

# Error
lda.r.error <- mean(LDA.r.test.data$y != lda.r.predictions$class)
lda.r.error

# Confusion Matrix
lda.r.cm <- confusionMatrix(lda.r.predictions$class, as.factor(LDA.r.test.data$y))
lda.r.cm

# Accuracy
lda.r.acc <- 0.6925
lda.r.acc

# Sensitivity and Specificity
lda.r.sens <- 0.7045
lda.r.sens
lda.r.spec <- 0.5984
lda.r.spec
```

# --------------------------------------------------------

## QDA with original data 88.17%
```{r}
set.seed(123)

QDA.o.Data <- obj2.data

QDA.o.Data$y <- as.numeric(as.factor(QDA.o.Data$y))

training.samples <- QDA.o.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
QDA.o.train.data <- QDA.o.Data[training.samples, ]
QDA.o.test.data <- QDA.o.Data[-training.samples, ]

# Fit the model
qda.o.model <- qda(y ~ ., data = QDA.o.train.data)
qda.o.model

# Make predictions
qda.o.predictions <- qda.o.model %>% predict(QDA.o.test.data)

# Error
qda.o.error <- mean(QDA.o.test.data$y != qda.o.predictions$class)
qda.o.error

# Confusion Matrix
qda.o.cm <- confusionMatrix(qda.o.predictions$class, as.factor(QDA.o.test.data$y))
qda.o.cm

# Model accuracy
qda.o.acc <- 0.8818
qda.o.acc

# Sensitivity and Specificity
qda.o.sens <- 0.9525
qda.o.sens
qda.o.spec <- 0.3062
qda.o.spec
```
## QDA with balanced data 87.39%
```{r}
set.seed(123)

QDA.b.Data <- obj2.data

QDA.b.Data$y <- as.numeric(as.factor(QDA.b.Data$y))

# Train Index
trainIndex <- createDataPartition(QDA.b.Data$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.numeric(as.factor(balanced_split_data$y))

# Generate train/test datasets
QDA.b.train.data <- balanced_split_data
QDA.b.test.data <- QDA.b.Data[-training.samples, ]

# Fit the model
qda.b.model <- qda(y ~ ., data = QDA.b.train.data)
qda.b.model

# Make predictions
qda.b.predictions <- qda.b.model %>% predict(QDA.b.test.data)

# Error
qda.b.error <- mean(QDA.b.test.data$y != qda.b.predictions$class)
qda.b.error

# Confusion Matrix
qda.b.cm <- confusionMatrix(qda.b.predictions$class, as.factor(QDA.b.test.data$y))
qda.b.cm

# Model accuracy
qda.o.acc <- 0.875
qda.o.acc

# Sensitivity and Specificity
qda.b.sens <- 0.9339
qda.b.sens
qda.b.spec <- 0.3950
qda.b.spec
```
## QDA with rose data 87.93%
```{r}
set.seed(123)

QDA.r.Data <- obj2.data

QDA.r.Data$y <- as.numeric(as.factor(QDA.r.Data$y))

training.samples <- QDA.r.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
QDA.r.train.data <- ROSE(y ~ .,data = QDA.r.Data[training.samples, ])$data
QDA.r.test.data <- QDA.r.Data[-training.samples, ]

# Fit the model
qda.r.model <- qda(y ~ ., data = QDA.r.train.data)
qda.r.model

# Make predictions
qda.r.predictions <- qda.r.model %>% predict(QDA.r.test.data)

# Error
qda.r.error <- mean(QDA.r.test.data$y != qda.r.predictions$class)
qda.r.error

# Confusion Matrix
qda.r.cm <- confusionMatrix(qda.r.predictions$class, as.factor(QDA.r.test.data$y))
qda.r.cm

# Model accuracy
qda.o.acc <- 0.8793
qda.o.acc

# Sensitivity and Specificity
qda.r.sens <- 0.9443
qda.r.sens
qda.r.spec <- 0.3506
qda.r.spec
```

# --------------------------------------------------------

## MDA with original data 88.69%
```{r}
set.seed(123)

MDA.o.Data <- obj2.data
MDA.o.Data$y <- as.numeric(as.factor(MDA.o.Data$y))

training.samples <- MDA.o.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)

MDA.o.train.data <- MDA.o.Data[training.samples, ]
MDA.o.test.data <- MDA.o.Data[-training.samples, ]

# Fit the model
mda.o.model <- mda(y ~ ., data = MDA.o.train.data)
mda.o.model

# Make predictions
mda.o.predictions <- mda.o.model %>% predict(MDA.o.test.data)

# Error
mda.o.error <- mean(MDA.o.test.data$y != mda.o.predictions)
mda.o.error

# Confusion Matrix
mda.o.cm <- confusionMatrix(mda.o.predictions, as.factor(MDA.o.test.data$y))
mda.o.cm

# Model accuracy
mda.o.acc <- 0.8868
mda.o.acc

# Sensitivity and Specificity
mda.o.sens <- 0.9589
mda.o.sens
mda.o.spec <- 0.2996
mda.o.spec
```
## MDA with balanced data 73.14%
```{r}
set.seed(123)

MDA.b.Data <- obj2.data
MDA.b.Data$y <- as.numeric(as.factor(MDA.b.Data$y))

# Train Index
trainIndex <- createDataPartition(MDA.b.Data$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.numeric(as.factor(balanced_split_data$y))

# Generate train/test datasets
MDA.b.train.data <- balanced_split_data
MDA.b.test.data <- MDA.b.Data[-training.samples, ]

# Fit the model
mda.b.model <- mda(y ~ ., data = MDA.b.train.data)
mda.b.model

# Make predictions
mda.b.predictions <- mda.b.model %>% predict(MDA.b.test.data)

# Error
mda.b.error <- mean(MDA.b.test.data$y != mda.b.predictions)
mda.b.error

# Confusion Matrix
mda.b.cm <- confusionMatrix(mda.b.predictions, as.factor(MDA.b.test.data$y))
mda.b.cm

# Model accuracy
mda.b.acc <- 0.7334
mda.b.acc

# Sensitivity and Specificity
mda.b.sens <- 0.7486
mda.b.sens
mda.b.spec <- 0.6095
mda.b.spec
```
## MDA with rose data 76.65%
```{r}
set.seed(123)

MDA.r.Data <- obj2.data
MDA.r.Data$y <- as.numeric(as.factor(MDA.r.Data$y))

training.samples <- MDA.r.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
MDA.r.train.data <- ROSE(y ~ .,data = MDA.r.Data[training.samples, ])$data
MDA.r.test.data <- MDA.r.Data[-training.samples, ]

# Fit the model
mda.r.model <- mda(y ~ ., data = MDA.r.train.data)
mda.r.model

# Make predictions
mda.r.predictions <- mda.r.model %>% predict(MDA.r.test.data)

# Error
mda.r.error <- mean(MDA.r.test.data$y != mda.r.predictions )
mda.r.error

# Confusion Matrix
mda.r.cm <- confusionMatrix(mda.r.predictions, as.factor(MDA.r.test.data$y))
mda.r.cm

# Model accuracy
mda.r.acc <- 0.7666
mda.r.acc

# Sensitivity and Specificity
mda.r.sens <- 0.7901
mda.r.sens
mda.r.spec <- 0.5754
mda.r.spec
```

# --------------------------------------------------------

## FDA with original data 89.83%
```{r}
set.seed(123)

FDA.o.Data <- obj2.data
FDA.o.Data$y <- as.numeric(as.factor(FDA.o.Data$y))

training.samples <- FDA.o.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)

FDA.o.train.data <- FDA.o.Data[training.samples, ]
FDA.o.test.data <- FDA.o.Data[-training.samples, ]

# Fit the model
fda.o.model <- fda(y ~ ., data = FDA.o.train.data)

# Make predictions
fda.o.predictions <- fda.o.model %>% predict(FDA.o.test.data)

# Error
fda.o.error <- mean(FDA.o.test.data$y != fda.o.predictions)
fda.o.error

# Confusion Matrix
fda.o.cm <- confusionMatrix(fda.o.predictions, as.factor(FDA.o.test.data$y))
fda.o.cm

# Model accuracy
fda.o.acc <- 0.8983
fda.o.acc

# Sensitivity and Specificity
fda.o.sens <- 0.9827
fda.o.sens
fda.o.spec <- 0.2115
fda.o.spec
```
## FDA with balanced data 67.88%
```{r}
set.seed(123)

FDA.b.Data <- obj2.data
FDA.b.Data$y <- as.numeric(as.factor(FDA.b.Data$y))

# Train Index
trainIndex <- createDataPartition(FDA.b.Data$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.numeric(as.factor(balanced_split_data$y))

# Generate train/test datasets
FDA.b.train.data <- balanced_split_data
FDA.b.test.data <- FDA.b.Data[-training.samples, ]

# Fit the model
fda.b.model <- fda(y ~ ., data = FDA.b.train.data)

# Make predictions
fda.b.predictions <- fda.b.model %>% predict(FDA.b.test.data)

# Error
fdab.error <- mean(FDA.b.test.data$y != fda.b.predictions)
fdab.error

# Confusion Matrix
fda.b.cm <- confusionMatrix(fda.b.predictions, as.factor(FDA.b.test.data$y))
fda.b.cm

# Model accuracy
fda.b.acc <- 0.6819
fda.b.acc

# Sensitivity and Specificity
fda.b.sens <- 0.6896
fda.b.sens
fda.b.spec <- 0.6191
fda.b.spec
```
## FDA with rose data 67.36
```{r}
set.seed(123)

FDA.r.Data <- obj2.data
FDA.r.Data$y <- as.numeric(as.factor(FDA.r.Data$y))

training.samples <- FDA.r.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
FDA.r.train.data <- ROSE(y ~ .,data = FDA.r.Data[training.samples, ])$data
FDA.r.test.data <- FDA.r.Data[-training.samples, ]

# Fit the model
fda.r.model <- fda(y ~ ., data = FDA.r.train.data)

# Make predictions
fda.r.predictions <- fda.r.model %>% predict(FDA.r.test.data)

# Error
fda.r.error <- mean(FDA.r.test.data$y != fda.r.predictions )
fda.r.error

# Confusion Matrix
fda.r.cm <- confusionMatrix(fda.r.predictions, as.factor(FDA.r.test.data$y))
fda.r.cm

# Model accuracy
fda.r.acc <- 0.6737
fda.r.acc

# Sensitivity and Specificity
fda.r.sens <- 0.6799
fda.r.sens
fda.r.spec <- 0.6228
fda.r.spec
```

# --------------------------------------------------------

## RDA with original data 89.85%
```{r}
set.seed(123)

RDA.o.Data <- obj2.data
RDA.o.Data$y <- as.numeric(as.factor(RDA.o.Data$y))

training.samples <- RDA.o.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
RDA.o.train.data <- RDA.o.Data[training.samples, ]
RDA.o.test.data <- RDA.o.Data[-training.samples, ]

# Fit the model
rda.o.model <- rda(y ~ ., data = RDA.o.train.data)

# Make predictions
rda.o.predictions <- rda.o.model %>% predict(RDA.o.test.data)

# Error
rda.o.error <- mean(RDA.o.test.data$y != rda.o.predictions$class)
rda.o.error

# Confusion Matrix
rda.o.cm <- confusionMatrix(rda.o.predictions$class, as.factor(RDA.o.test.data$y))
rda.o.cm

# Model accuracy
rda.o.acc <- 0.8986
rda.o.acc

# Sensitivity and Specificity
rda.o.sens <- 0.9840
rda.o.sens
rda.o.spec <- 0.2034
rda.o.spec
```
## RDA with balanced data 85.36
```{r}
set.seed(123)

RDA.b.Data <- obj2.data
RDA.b.Data$y <- as.numeric(as.factor(RDA.b.Data$y))

# Train Index
trainIndex <- createDataPartition(RDA.b.Data$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.numeric(as.factor(balanced_split_data$y))

# Generate train/test datasets
RDA.b.train.data <- balanced_split_data
RDA.b.test.data <- RDA.b.Data[-training.samples, ]

# Fit the model
rda.b.model <- rda(y ~ ., data = RDA.b.train.data)

# Make predictions
rda.b.predictions <- rda.b.model %>% predict(RDA.b.test.data)

# Error
rda.b.error <- mean(RDA.b.test.data$y != rda.b.predictions$class)
rda.b.error

# Confusion Matrix
rda.b.cm <- confusionMatrix(rda.b.predictions$class, as.factor(RDA.b.test.data$y))
rda.b.cm

# Model accuracy
rda.b.acc <- 0.8595
rda.b.acc

# Sensitivity and Specificity
rda.b.sens <- 0.9209
rda.b.sens
rda.b.spec <- 0.3757
rda.b.spec
```
## RDA with rose data 80.67%
```{r}
set.seed(123)

RDA.r.Data <- obj2.data
RDA.r.Data$y <- as.numeric(as.factor(RDA.r.Data$y))

training.samples <- RDA.r.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
RDA.r.train.data  <- ROSE(y ~ .,data = RDA.r.Data[training.samples, ])$data
RDA.r.test.data <- RDA.r.Data[-training.samples, ]

# Fit the model
rda.r.model <- rda(y ~ ., data = RDA.r.train.data)

# Make predictions
rda.r.predictions <- rda.r.model %>% predict(RDA.r.test.data)

# Error
rda.r.error <- mean(RDA.r.test.data$y != rda.r.predictions$class)
rda.r.error

# Confusion Matrix
rda.r.cm <- confusionMatrix(rda.r.predictions$class, as.factor(RDA.r.test.data$y))
rda.r.cm

# Model accuracy
rda.r.acc <- 0.8833
rda.r.acc

# Sensitivity and Specificity
rda.r.sens <- 0.9526
rda.r.sens
rda.r.spec <- 0.3195
rda.r.spec
```

# --------------------------------------------------------

## RF with original data 89.64%
```{r}
# Split the data into training (70%) and test set (30%)
set.seed(123)
rf.o.data <- obj2.data
training.samples <- rf.o.data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
rf.o.train.data <- rf.o.data[training.samples, ]
rf.o.test.data <- rf.o.data[-training.samples, ]

# Converting ‘y’ to a factor
rf.o.train.data$y <- factor(rf.o.train.data$y)

# set cross validation parameters
trControl <- trainControl(method = "cv",
    number = 10,
    search = "grid")

# Training using ‘random forest’ algorithm
rf.o.model <- train(y~.,data = rf.o.train.data, method = "rf", 
                    metric = "Accuracy", importance = TRUE,
                    trControl = trControl,  ntree = 50)

print(rf.o.model)
plot(rf.o.model)

rf.o.model$bestTune$mtry
rf.o.acc <- max(rf.o.model$results$Accuracy)
rf.o.acc

# Predict
rf.o.predictions <- rf.o.model %>% predict(rf.o.test.data)

# Error
rf.o.error <- mean(rf.o.test.data$y != rf.o.predictions)
rf.o.error

# Confusion Matrix
rf.o.cm <- confusionMatrix(rf.o.predictions, as.factor(rf.o.test.data$y))
rf.o.cm

# Model accuracy
rf.o.acc <- 0.8978
rf.o.acc

# Sensitivity and Specificity
rf.o.sens <- 0.9877
rf.o.sens
rf.o.spec <- 0.1897
rf.o.spec
```
## RF with balanced data 73.88%
```{r}
# Split the data into training (70%) and test set (30%)
set.seed(123)
rf.b.data <- obj2.data

# Train Index
trainIndex <- createDataPartition(rf.b.Data$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.factor(balanced_split_data$y)

# Generate train/test datasets
rf.b.train.data <- balanced_split_data
rf.b.test.data <- rf.b.data[-training.samples, ]

# Converting ‘y’ to a factor
rf.b.train.data$y <- factor(rf.b.train.data$y)

# set cross validation parameters
trControl <- trainControl(method = "cv",
    number = 10,
    search = "grid")

# Training using ‘random forest’ algorithm
rf.b.model <- train(y~.,data = rf.b.train.data, method = "rf", 
                    metric = "Accuracy", importance = TRUE,
                    trControl = trControl, ntree = 50)

print(rf.b.model)
plot(rf.b.model)

rf.b.model$bestTune$mtry

# Predict
rf.b.predictions <- rf.b.model %>% predict(rf.b.test.data)

# Error
rf.b.error <- mean(rf.b.test.data$y != rf.b.predictions)
rf.b.error

# Confusion Matrix
rf.b.cm <- confusionMatrix(rf.b.predictions, as.factor(rf.b.test.data$y))
rf.b.cm

# Model accuracy
rf.b.acc <- 0.8479
rf.b.acc

# Sensitivity and Specificity
rf.b.sens <- 0.8722
rf.b.sens
rf.b.spec <- 0.6566
rf.b.spec
```
## RF with rose data 88.68%
```{r}
# Split the data into training (70%) and test set (30%)
set.seed(123)
rf.r.data <- obj2.data
training.samples <- rf.r.data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
rf.r.train.data  <- ROSE(y ~ .,data = rf.r.data[training.samples, ])$data
rf.r.test.data <- rf.r.data[-training.samples, ]

# Converting ‘y’ to a factor
rf.r.train.data$y <- factor(rf.r.train.data$y)

# set cross validation parameters
trControl <- trainControl(method = "cv",
    number = 10,
    search = "grid")

# Training using ‘random forest’ algorithm
rf.r.model <- train(y~.,data = rf.r.train.data, method = "rf", 
                    metric = "Accuracy", importance = TRUE,
                    trControl = trControl, ntree = 50)

print(rf.r.model)
plot(rf.r.model)

rf.r.model$bestTune$mtry

# Predict
rf.r.predictions <- rf.r.model %>% predict(rf.r.test.data)

# Error
rf.r.error <- mean(rf.r.test.data$y != rf.r.predictions)
rf.r.error

# Confusion Matrix
rf.r.cm <- confusionMatrix(rf.r.predictions, as.factor(rf.r.test.data$y))
rf.r.cm

# Model accuracy
rf.r.acc <- 0.8926
rf.r.acc

# Sensitivity and Specificity
rf.r.sens <- 0.9737
rf.r.sens
rf.r.spec <- 0.2536
rf.r.spec
```

# --------------------------------------------------------

## Ridge with original data 87.85%
```{r}
set.seed(123)

ridge.o.lr <- obj2.data

ridge.o.lr$y <- as.numeric(as.factor(ridge.o.lr$y))

ridge.Index <- sample(1:nrow(ridge.o.lr), 0.7*nrow(ridge.o.lr)) # indices for 70% training data
ridge.o.train <- ridge.o.lr[ridge.Index, ] # training data
ridge.o.test <- ridge.o.lr[-ridge.Index, ] # test data

ridge.o.model <- linearRidge(y ~ ., data = ridge.o.train)

summary(ridge.o.model)

ridge.o.pred <- as.numeric(predict(ridge.o.model, ridge.o.test))  # predict on test data
ridge.o.pred <- round(ridge.o.pred)
ridge.o.compare <- cbind (actual=ridge.o.test$y, ridge.o.pred)  # combine

# Accuracy
ridge.o.acc <- mean(apply(ridge.o.compare, 1, min)/apply(ridge.o.compare, 1, max)) 
ridge.o.acc

# Confusion Matrix
ridge.o.cm <- confusionMatrix(as.factor(ridge.o.pred), as.factor(ridge.o.test$y))
ridge.o.cm

# Model accuracy
ridge.o.acc <- 0.899
ridge.o.acc

# Sensitivity and Specificity
ridge.o.sens <- 0.9868
ridge.o.sens
ridge.o.spec <- 0.1842
ridge.o.spec
```
## Ridge with balanced data 74.58%
```{r}
set.seed(123)

ridge.b.lr <- obj2.data

ridge.b.lr$y <- as.numeric(as.factor(ridge.b.lr$y))

# Train Index
ridge.Index <- createDataPartition(ridge.b.lr$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.numeric(as.factor(balanced_split_data$y))

# Generate train/test datasets
ridge.b.train <- balanced_split_data
ridge.b.test <- ridge.b.lr[-ridge.Index, ] # test data

ridge.b.model <- linearRidge(y ~ ., data = ridge.b.train)

summary(ridge.b.model)

ridge.b.pred <- predict(ridge.b.model, ridge.b.test)  # predict on test data
ridge.b.pred <- round(ridge.b.pred)
ridge.b.compare <- cbind (actual=ridge.b.test$y, ridge.b.pred)  # combine

# Confusion Matrix
ridge.b.cm <- confusionMatrix(as.factor(ridge.b.pred), as.factor(ridge.b.test$y))
ridge.b.cm

# Model accuracy
ridge.b.acc <- 0.6819
ridge.b.acc

# Sensitivity and Specificity
ridge.b.sens <- 0.6895
ridge.b.sens
ridge.b.spec <- 0.6206
ridge.b.spec
```
## Ridge with rose data 74.18%
```{r}
set.seed(123)

ridge.r.lr <- obj2.data

ridge.r.lr$y <- as.numeric(as.factor(ridge.r.lr$y))

ridge.Index <- sample(1:nrow(ridge.r.lr), 0.7*nrow(ridge.r.lr)) # indices for 70% training data
ridge.r.train <- ROSE(y ~ .,data = ridge.r.lr[ridge.Index, ])$data
ridge.r.test <- ridge.r.lr[-ridge.Index, ] # test data

ridge.r.model <- linearRidge(y ~ ., data = ridge.r.train)

summary(ridge.r.model)

ridge.r.pred <- predict(ridge.r.model, ridge.r.test)  # predict on test data
ridge.r.pred <- round(ridge.r.pred)
ridge.r.compare <- cbind (actual=ridge.r.test$y, ridge.r.pred)  # combine

# Confusion Matrix
ridge.r.cm <- confusionMatrix(as.factor(ridge.r.pred), as.factor(ridge.r.test$y))
ridge.r.cm

# Model accuracy
ridge.r.acc <- 0.6586
ridge.r.acc

# Sensitivity and Specificity
ridge.r.sens <- 0.6611
ridge.r.sens
ridge.r.spec <- 0.6390
ridge.r.spec
```

# --------------------------------------------------------

## LASSO Regression original data 89.68%
```{r}
set.seed(123)

lasso.o.lr <- obj2.data
lasso.o.lr$y <- as.factor(lasso.o.lr$y)

training.samples <- lasso.o.lr$y %>%
  createDataPartition(p = 0.7, list = FALSE)
lasso.o.train.data <- lasso.o.lr[training.samples, ]
lasso.o.test.data <- lasso.o.lr[-training.samples, ]

dat.o.train.x <- model.matrix(y~.,lasso.o.train.data)
dat.o.train.y <- lasso.o.train.data[,14]

cvfit <- cv.glmnet(dat.o.train.x, dat.o.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")

#CV misclassification error rate 
print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

#For final model predictions go ahead and refit lasso using entire
lasso.o.model<-glmnet(dat.o.train.x, dat.o.train.y, family = "binomial", type.measure = "class", nlambda = 1001)

dat.o.test.x<-model.matrix(y~.,lasso.o.test.data)
fit.o.pred.lasso <- predict(lasso.o.model, newx = dat.o.test.x, type = "response", s = cvfit$lambda.min, exact = FALSE)
fit.o.pred.lasso <- round(fit.o.pred.lasso)

#Lets use the predicted probablities to classify the observations and make a final confusion matrix for the two models.
cutoff<-0.5
class.o.lasso<-factor(ifelse(fit.o.pred.lasso>cutoff,"yes","no"),levels=c("no","yes"))

# Confusion Matrix
lasso.o.cm <- confusionMatrix(as.factor(class.o.lasso), as.factor(lasso.o.test.data$y))
lasso.o.cm

# Model accuracy
lasso.o.acc <- 0.8969
lasso.o.acc

# Sensitivity and Specificity
lasso.o.sens <- 0.9903
lasso.o.sens
lasso.o.spec <- 0.1609
lasso.o.spec
```
## LASSO Regression balanced data 60.04%
```{r}
set.seed(123)

lasso.b.lr <- obj2.data
lasso.b.lr$y <- as.factor(lasso.b.lr$y)

# Train Index
training.samples <- createDataPartition(lasso.b.lr$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.numeric(as.factor(balanced_split_data$y))

# Generate train/test datasets
lasso.b.train.data <- balanced_split_data
lasso.b.test.data <- lasso.b.lr[-training.samples, ]

dat.b.train.x <- model.matrix(y~.,lasso.b.train.data)
dat.b.train.y <- lasso.b.train.data[,14]

cvfit <- cv.glmnet(dat.b.train.x, dat.b.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#CV misclassification error rate 
print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

#For final model predictions go ahead and refit lasso using entire
lasso.b.model<-glmnet(dat.b.train.x, dat.b.train.y, family = "binomial",lambda=cvfit$lambda.min, type.measure = "class", nlambda = 1001)

dat.b.test.x<-model.matrix(y~.,lasso.b.test.data)
fit.b.pred.lasso <- predict(lasso.b.model, newx = dat.b.test.x, type = "response", type = "response", s = cvfit$lambda.min, exact = FALSE)

#Lets use the predicted probablities to classify the observations and make a final confusion matrix for the two models.  We can use it to calculate error metrics.
#Lets use a cutoff of 0.5 to make the classification.
cutoff<-0.5
class.b.lasso<-factor(ifelse(fit.b.pred.lasso>cutoff,"yes","no"))

# Confusion Matrix
lasso.b.cm <- confusionMatrix(as.factor(class.b.lasso), as.factor(lasso.b.test.data$y))
lasso.b.cm

# Model accuracy
lasso.b.acc <- 0.6571
lasso.b.acc

# Sensitivity and Specificity
lasso.b.sens <- 0.6649
lasso.b.sens
lasso.b.spec <- 0.5955
lasso.b.spec
```
## LASSO Regression Rose data 82.74
```{r}
set.seed(123)

lasso.r.lr <- obj2.data
lasso.r.lr$y <- as.factor(lasso.r.lr$y)

training.samples <- lasso.r.lr$y %>%
  createDataPartition(p = 0.7, list = FALSE)
lasso.r.train.data <- ROSE(y ~ .,data = lasso.r.lr[training.samples, ])$data
lasso.r.test.data <- lasso.r.lr[-training.samples, ]

dat.r.train.x <- model.matrix(y~.,lasso.r.train.data)
dat.r.train.y <- lasso.r.train.data[,14]

cvfit <- cv.glmnet(dat.r.train.x, dat.r.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#CV misclassification error rate is little below .1
print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

#For final model predictions go ahead and refit lasso using entire
lasso.r.model<-glmnet(dat.r.train.x, dat.r.train.y, family = "binomial",lambda=cvfit$lambda.min)

dat.r.test.x<-model.matrix(y~.,lasso.r.test.data)
fit.r.pred.lasso <- predict(lasso.r.model, newx = dat.r.test.x, type = "response")

#Lets use the predicted probablities to classify the observations and make a final confusion matrix for the two models.  
cutoff<-0.5
class.r.lasso<-factor(ifelse(fit.r.pred.lasso>cutoff,"yes","no"))

# Confusion Matrix
lasso.r.cm <- confusionMatrix(as.factor(class.r.lasso), as.factor(lasso.r.test.data$y))
lasso.r.cm

# Model accuracy
lasso.r.acc <- 0.8275
lasso.r.acc

# Sensitivity and Specificity
lasso.r.sens <- 0.8951
lasso.r.sens
lasso.r.spec <- 0.2945
lasso.r.spec
```

# --------------------------------------------------------

## Net Regression original data 89.81%
```{r}
set.seed(123)

net.o.lr <- obj2.data
net.o.lr$y <- as.factor(net.o.lr$y)

index = sample(1:nrow(net.o.lr), 0.7*nrow(net.o.lr)) 

train = net.o.lr[index,] # Create the training data 
test = net.o.lr[-index,] # Create the test data

dummies <- dummyVars(y ~ ., data = net.o.lr)
train_dummies = predict(dummies, newdata = train)
test_dummies = predict(dummies, newdata = test)

x = as.matrix(train_dummies)
y_train = train$y

x_test = as.matrix(test_dummies)
y_test = test$y

# Set training control
train_cont <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 5,
                              search = "random",
                              verboseIter = TRUE)

# Train the model
elastic_reg <- train(y ~ .,
                           data = train,
                           method = "glmnet",
                           preProcess = c("center", "scale"),
                           tuneLength = 10,
                           trControl = train_cont)


# Best tuning parameter
elastic_reg$bestTune

# Make predictions on test set
net.o.pred <- predict(elastic_reg, x_test)

# Confusion Matrix
net.o.cm <- confusionMatrix(net.o.pred, as.factor(y_test))
net.o.cm

# Model accuracy
net.o.acc <- 0.8981
net.o.acc

# Sensitivity and Specificity
net.o.sens <- 0.9887
net.o.sens
net.o.spec <- 0.1605
net.o.spec
```
## Net Regression balanced data 89.81%
```{r}
set.seed(123)

net.b.lr <- obj2.data
net.b.lr$y <- as.factor(net.b.lr$y)

# Train Index
index <- createDataPartition(net.b.lr$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.factor(balanced_split_data$y)

# Generate train/test datasets
train <- balanced_split_data
test = net.b.lr[-index,] # Create the test data

dummies <- dummyVars(y ~ ., data = net.b.lr)
train_dummies = predict(dummies, newdata = train)
test_dummies = predict(dummies, newdata = test)

x = as.matrix(train_dummies)
y_train = train$y

x_test = as.matrix(test_dummies)
y_test = test$y

# Set training control
train_cont <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 5,
                              search = "random",
                              verboseIter = TRUE)

# Train the model
elastic_reg <- train(y ~ .,
                           data = train,
                           method = "glmnet",
                           preProcess = c("center", "scale"),
                           tuneLength = 10,
                           trControl = train_cont)


# Best tuning parameter
elastic_reg$bestTune


# Make predictions on test set
net.b.pred <- predict(elastic_reg, x_test)

# Confusion Matrix
net.b.cm <- confusionMatrix(net.b.pred, as.factor(y_test))
net.b.cm

# Model accuracy
net.b.acc <- 0.6834
net.b.acc

# Sensitivity and Specificity
net.b.sens <- 0.6891
net.b.sens
net.b.spec <- 0.6386
net.b.spec
```
## Net Regression rose data 67.99
```{r}
set.seed(123)

net.r.lr <- obj2.data
net.r.lr$y <- as.factor(net.r.lr$y)

index = sample(1:nrow(net.r.lr), 0.7*nrow(net.r.lr)) 

lasso.r.train.data <- ROSE(y ~ .,data = net.r.lr[training.samples, ])$data
test = net.r.lr[-index,] # Create the test data

dummies <- dummyVars(y ~ ., data = net.r.lr)
train_dummies = predict(dummies, newdata = train)
test_dummies = predict(dummies, newdata = test)

x = as.matrix(train_dummies)
y_train = train$y

x_test = as.matrix(test_dummies)
y_test = test$y

# Set training control
train_cont <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 5,
                              search = "random",
                              verboseIter = TRUE)

# Train the model
elastic_reg <- train(y ~ .,
                           data = train,
                           method = "glmnet",
                           preProcess = c("center", "scale"),
                           tuneLength = 10,
                           trControl = train_cont)


# Best tuning parameter
elastic_reg$bestTune


# Make predictions on test set
net.r.pred <- predict(elastic_reg, x_test)

# Confusion Matrix
net.r.cm <- confusionMatrix(net.r.pred, as.factor(y_test))
net.r.cm

# Model accuracy
net.r.acc <- 0.6825
net.r.acc

# Sensitivity and Specificity
net.r.sens <- 0.6894
net.r.sens
net.r.spec <- 0.6265
net.r.spec
```

# --------------------------------------------------------

## KNN original data (not working)
```{r}
set.seed(123)

knn.o.data <- obj2.data
knn.o.data$y <- as.numeric(as.factor(knn.o.data$y))

# Test/Train Data
training.samples <- knn.o.data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
default_trn = knn.o.data[training.samples, ]
default_tst = knn.o.data[-training.samples, ]
default_cl = default_trn$y

# training data
X_default_trn = default_trn[, -14]
y_default_trn = default_trn$y

# testing data
X_default_tst = default_tst[, -14]
y_default_tst = default_tst$y

knn.o.model <- knn3Train(default_trn, default_tst, default_cl, k = 51, prob = TRUE)
cm = as.matrix(table(default_cl, knn.o.model))
```


# --------------------------------------------------------

## Objective 2 Results
```{r}
SR.O.results <- cbind("Simple LR Original",simple.o.acc, simple.o.sens, simple.o.spec)
SR.B.results <- cbind("Simple LR Balanced",simple.b.acc, simple.b.sens, simple.b.spec)
SR.R.results <- cbind("Simple LR Rose",simple.r.acc, simple.r.sens, simple.r.spec)
LDA.O.results <- cbind("LDA Original",lda.o.acc, lda.o.sens, lda.o.spec)
LDA.B.results <- cbind("LDA Balanced",lda.b.acc, lda.b.sens, lda.b.spec)
LDA.R.results <- cbind("LDA Rose",lda.r.acc, lda.r.sens, lda.r.spec)
QDA.O.results <- cbind("QDA Original",qda.o.acc, qda.o.sens, qda.o.spec)
QDA.B.results <- cbind("QDA Balanced",qda.b.acc, qda.b.sens, qda.b.spec)
QDA.R.results <- cbind("QDA Rose",qda.r.acc, qda.r.sens, qda.r.spec)
MDA.O.results <- cbind("MDA Original",mda.o.acc, mda.o.sens, mda.o.spec)
MDA.B.results <- cbind("MDA Balanced",mda.b.acc, mda.b.sens, mda.b.spec)
MDA.R.results <- cbind("MDA Rose",mda.r.acc, mda.r.sens, mda.r.spec)
FDA.O.results <- cbind("FDA Original",fda.o.acc, fda.o.sens, fda.o.spec)
FDA.B.results <- cbind("FDA Balanced",fda.b.acc, fda.b.sens, fda.b.spec)
FDA.R.results <- cbind("FDA Rose",fda.r.acc, fda.r.sens, fda.r.spec)
RDA.O.results <- cbind("RDA Original",rda.o.acc, rda.o.sens, rda.o.spec)
RDA.B.results <- cbind("RDA Balanced",rda.b.acc, rda.b.sens, rda.b.spec)
RDA.R.results <- cbind("RDA Rose",rda.r.acc, rda.r.sens, rda.r.spec)
RIDGE.O.results <- cbind("Ridge Original",ridge.o.acc, ridge.o.sens, ridge.o.spec)
RIDGE.B.results <- cbind("Ridge Balanced",ridge.b.acc, ridge.b.sens, ridge.b.spec)
RIDGE.R.results <- cbind("Ridge Rose",ridge.r.acc, ridge.r.sens, ridge.r.spec)
LASSO.O.results <- cbind("Lasso Original",lasso.o.acc, lasso.o.sens, lasso.o.spec)
LASSO.B.results <- cbind("Lasso Balanced",lasso.b.acc, lasso.b.sens, lasso.b.spec)
LASSO.R.results <- cbind("Lasso Rose",lasso.r.acc, lasso.r.sens, lasso.r.spec)
NET.O.results <- cbind("Net Original",net.o.acc, net.o.sens, net.o.spec)
NET.B.results <- cbind("Net Balanced",net.b.acc, net.b.sens, net.b.spec)
NET.R.results <- cbind("Net Rose",net.r.acc, net.r.sens, net.r.spec)
RF.O.results <- cbind("RF Original",rf.o.acc, rf.o.sens, rf.o.spec)
RF.B.results <- cbind("RF Balanced",rf.b.acc, rf.b.sens, rf.b.spec)
RF.R.results <- cbind("RF Rose",rf.r.acc, rf.r.sens, rf.r.spec)

accuracy <- rbind(SR.O.results, SR.B.results, SR.R.results,
                  LDA.O.results, LDA.B.results, LDA.R.results,
                  QDA.O.results, QDA.B.results, QDA.R.results,
                  MDA.O.results, MDA.B.results, MDA.R.results,
                  FDA.O.results, FDA.B.results, FDA.R.results,
                  RDA.O.results, RDA.B.results, RDA.R.results,
                  RIDGE.O.results, RIDGE.B.results, RIDGE.R.results,
                  LASSO.O.results, LASSO.B.results, LASSO.R.results,
                  NET.O.results, NET.B.results, NET.R.results,
                  RF.O.results, RF.B.results, RF.R.results)

colnames(accuracy) <- c("Method","Accuracy", "Sensitivity", "Specificity")

accuracytbl <- as.data.frame(accuracy)
accuracytbl$Accuracy <- as.numeric(accuracytbl$Accuracy)
accuracytbl$Sensitivity <- as.numeric(accuracytbl$Sensitivity)
accuracytbl$Specificity <- as.numeric(accuracytbl$Specificity)


# Neat table
customGreen = "#009900"
customRed = "#ff7f7f"

formattable(accuracytbl, 
            align = c("l",rep("r", NCOL(accuracytbl) - 1)),
            list(`Method` = formatter("span", style = ~ style(color = "black", font.weight = "bold")), 
                 'Accuracy' = percent, 
                 'Sensitivity' = percent,
                 'Specificity' = percent,
                 'Accuracy' = color_text(customRed, customGreen),
                 'Sensitivity' = color_text(customRed, customGreen),
                 "Specificity" = color_text(customRed, customGreen)))
```






```{r}
library(smotefamily)
library(DMwR)
## Splitting and Down Sampling
dim(imputed)
prop.table(table(imputed$y))
imputed$y <- as.factor(imputed$y)
set.seed(1234)
training.samples <- imputed$y %>%
  createDataPartition(p = 0.7, list = FALSE)
train <- imputed[training.samples, ]
test <- imputed[-training.samples, ]
table(train$y)
prop.table(table(train$y))
table(test$y)
prop.table(table(test$y))
# 50-50 down sampling
train_down_s_50 <- SMOTE(form = y~.,data = train, k = 5, perc.over = 100)
attach(train_down_s_50)
table(train_down_s_50$y)
prop.table(table(train_down_s_50$y))
# 60-40 down sampling
train_bal_s_60 <- SMOTE(form = y~.,data = train, k = 5, perc.over = 390)
table(train_bal_s_60$y)
prop.table(table(train_bal_s_60$y))
# Ovun down sampling
train_bal_o <-ovun.sample(y ~ .,data = train, method = "under", N = 9280)$data
table(train_bal1$y)
prop.table(table(train_bal1$y))
# ROSE Down sampling
train_bal_r <- ROSE(y ~ ., data = train)$data
table(train_bal_r$y)
prop.table(table(train_bal_r$y))
```

## Logit - Full Model - Original data 90.8%
```{r}
set.seed(1234)
logit.os.Train <- train
logit.os.Test <- test
logit.os.data$y <- as.factor(logit.os.Train$y)
# Build Full Model
full.os.log<-glm(y~.,family="binomial",data=logit.os.Train)
# Step Model
step.os.log<-full.os.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
```

## Logit - Full Model - SMOTE 50- 50 Sampled data 86.6%
```{r}
set.seed(1234)
logit.os.Train <- train_down_s_50
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
attach(logit.os.Train)
# Build Full Model
full.os.log<-glm(y~.,family="binomial",data=logit.os.Train)
# Step Model
step.os.log<-full.os.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
```

## Logit - Full Model - SMOTE 60- 40 Sampled data 88.6%
```{r}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
attach(logit.os.Train)
# Build Full Model
full.os.log<-glm(y~.,family="binomial",data=logit.os.Train)
# Step Model
step.os.log<-full.os.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
```

## Logit - Full Model - ROSE 85.5%
```{r}
set.seed(1234)
logit.os.Train <- train_bal_r
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
attach(logit.os.Train)
# Build Full Model
full.os.log<-glm(y~.,family="binomial",data=logit.os.Train)
# Step Model
step.os.log<-full.os.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
```





1. Using SMOTE 60-40 sampled data as the train data
2. From the full model and the significant predictors from step wise model and taking into account the correlated predictors
   below are the some of the models.

## Logit - Reduced Model 7(job,education,campaign,pdays,poutcome,nr.employed, emp.var.rate)
## Accuracy - 84.2%, Sensitivity - 89.4, Specificity - 44.1, MC - 0.157332
```{r reduced model 7}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod7.log <- glm(y ~ job + education + campaign + pdays + poutcome + nr.employed + emp.var.rate, family="binomial",data=logit.os.Train)
# Step Model
step.os.log <- red.mod7.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff <- 0.5
class.step <- factor(ifelse(fit.pred.step > cutoff,"yes","no"))
print("Confusion matrix for Stepwise")
confusionMatrix(as.factor(class.step), logit.os.Test$y)

#MisCalc Rates
misClasificError <- mean(class.step != logit.os.Test$y)
print("Miscalculation Rate")
misClasificError

# Metrics
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.sens <- sensitivity(class.step, logit.os.Test$y,threshold = cutoff)
step.os.logit.spec <- specificity(class.step,logit.os.Test$y,threshold = cutoff)

# Roc Curves & value
p <- predict(step.os.log, newdata=logit.os.Test, type="response")
pr <- prediction(p, logit.os.Test$y)
# TPR = sensitivity, FPR=specificity
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
plot(prf,
     main = "ROC Curve 
 AUC = 0.7119",
     col = "blue")
```

## Logit - Reduced Model 1(job,month,campaign,pdays,poutcome,emp.var.rate,cons.price.idx) 
## Accuracy - 81.1%, Sensitivity - 84.9, Specificity - 50.9, MC - 0.188653
```{r reduced model 1}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
attach(logit.os.Train)
# Build Full Model
red.mod1.log <- glm(y~ job + month + campaign + pdays + poutcome + emp.var.rate + cons.price.idx ,family="binomial",data=logit.os.Train)
# Step Model
step.os.log <- red.mod1.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test, type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"))
confusionMatrix(as.factor(class.step),logit.os.Test$y)

#MisCalc Rates
misClasificError <- mean(class.step != logit.os.Test$y)
print("Miscalculation Rate")
misClasificError

step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
print("Sensitivity")
sensitivity(class.step,logit.os.Test$y,threshold = cutoff)
print("Specificity")
specificity(class.step,logit.os.Test$y,threshold = cutoff)
```

## Logit - Reduced Model 2(age,job,contact,day_of_week,campaign,pdays,poutcome,emp.var.rate,nr.employed)
## Accuracy - 79.3%, Sensitivity - 83.6, Specificity - 45.7, MC - 0.206863
```{r reduced model 2}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod2.log <- glm(y ~ age + job + contact + day_of_week + campaign + pdays + poutcome + emp.var.rate + nr.employed , family="binomial",data=logit.os.Train)
# Step Model
step.os.log <- red.mod2.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"))
confusionMatrix(as.factor(class.step),logit.os.Test$y)

#MisCalc Rates
misClasificError <- mean(class.step != logit.os.Test$y)
print("Miscalculation Rate")
misClasificError

step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
print("Sensitivity")
sensitivity(class.step,logit.os.Test$y,threshold = cutoff)
print("Specificity")
specificity(class.step,logit.os.Test$y,threshold = cutoff)
```

## Logit - Reduced Model 4(job,contact,day_of_week,campaign,pdays,poutcome,euribor3m)
## Accuracy - 77.2%, Sensitivity - 81.7, Specificity - 41.8, MC - 0.228229
```{r reduced model 4}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod4.log <- glm(y ~ job + contact + day_of_week + campaign + pdays + poutcome + euribor3m , family="binomial",data=logit.os.Train)
# Step Model
step.os.log <- red.mod4.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"))
confusionMatrix(as.factor(class.step),logit.os.Test$y)

#MisCalc Rates
misClasificError <- mean(class.step != logit.os.Test$y)
print("Miscalculation Rate")
misClasificError

step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
print("Sensitivity")
sensitivity(class.step,logit.os.Test$y,threshold = cutoff)
print("Specificity")
specificity(class.step,logit.os.Test$y,threshold = cutoff)
```

## Logit - Reduced Model 6(job,education,campaign,poutcome,nr.employed)
## Accuracy - 80.7%, Sensitivity - 83.7, Specificity - 57.0, MC - 0.192780
```{r reduced model 6}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod6.log <- glm(y ~ job + education  +  campaign  + poutcome + nr.employed , family="binomial",data=logit.os.Train)
# Step Model
step.os.log <- red.mod6.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"))
print("Confusion matrix for Stepwise")
confusionMatrix(as.factor(class.step), logit.os.Test$y)

#MisCalc Rates
misClasificError <- mean(class.step != logit.os.Test$y)
print("Miscalculation Rate")
misClasificError

step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
print("Sensitivity")
sensitivity(class.step,logit.os.Test$y,threshold = cutoff)
print("Specificity")
specificity(class.step,logit.os.Test$y,threshold = cutoff)
```

## Logit - Reduced Model 3(job,contact,day_of_week,campaign,pdays,poutcome,euribor3m,nr.employed)
## Accuracy - 79.99%, Sensitivity - 84.3, Specificity - 45.7, VIF ISSUES
```{r reduced model 3}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod3.log <- glm(y ~ job + contact + day_of_week + campaign + pdays + poutcome + euribor3m + nr.employed , family="binomial",data=logit.os.Train)
# Step Model
step.os.log <- red.mod3.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"))
confusionMatrix(as.factor(class.step),logit.os.Test$y)

step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
print("Sensitivity")
sensitivity(class.step,logit.os.Test$y,threshold = cutoff)
print("Specificity")
specificity(class.step,logit.os.Test$y,threshold = cutoff)
```

## Logit - Reduced Model 5(job,contact,education,day_of_week,campaign,pdays,poutcome,euribor3m,nr.employed)
## Accuracy - 80.1%, Sensitivity - 84.5, Specificity - 45.1, VIF ISSUES
```{r reduced model 5}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod5.log <- glm(y ~ job + education + contact + day_of_week + campaign + pdays + poutcome + euribor3m + nr.employed , family="binomial",data=logit.os.Train)
# Step Model
step.os.log <- red.mod5.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"))
confusionMatrix(as.factor(class.step),logit.os.Test$y)


step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
print("Sensitivity")
sensitivity(class.step,logit.os.Test$y,threshold = cutoff)
print("Specificity")
specificity(class.step,logit.os.Test$y,threshold = cutoff)
```

