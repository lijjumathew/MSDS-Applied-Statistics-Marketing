---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

## Load libraries
```{r}
library(GGally)
library(scales)
library(formattable)
library(leaps)
library(car)
library(lme4)
library(mlbench)
library(caret)
library(tidyr)
library(MASS)
library(corrr)
library(randomForest)
library(ROSE)
library(arm)
library(glmnet)
library(imputeMissings)
library(ggvis)
library(mice)
library(ISLR)
library(plyr)
library(dplyr)
library(ISOweek)
library(corrplot)
library(PerformanceAnalytics)
library(psych)
library(psychTools)
library(mda)
library(klaR)
library(aod)
library(ggfortify)
library(factoextra)
library(rgl)
library(tree)
library(randomForest)
library(ridge)
library(ggplot2)
library(RColorBrewer)
library(corrplot)
library(reshape2)
library(pls)
library(bestNormalize)
```

## Load in dataset and look at data
```{r}
# import data
datain <- read.csv("D:/MS Data Science/SMU/6372 - Applied Stats/Project 2/bank-additional-full.csv", header = TRUE)

# View data
str(datain)
summary(datain)

# convert to dataframe
datain <- as.data.frame(datain)

# Convert all "unknown" to NA
datain[datain == "unknown"] <- NA

# Count NA values in each column
na_count <-sapply(datain, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
na_count

# Count number of yes and no
y_count <-sapply(datain, function(y) sum(length(which(y=="yes"))))
y_count <- data.frame(y_count)
y_count

n_count <-sapply(datain, function(y) sum(length(which(y=="no"))))
n_count <- data.frame(n_count)
n_count

# there are 36,548 No's and 4,640 Yes's.
```

## Impute missing data & convert to numeric
```{r}
# convert to factor for imputation
datain$job <- as.factor(datain$job)
datain$marital <- as.factor(datain$marital)
datain$education <- as.factor(datain$education)
datain$default <- as.factor(datain$default)
datain$housing <- as.factor(datain$housing)
datain$loan <- as.factor(datain$loan)
datain$contact <- as.factor(datain$contact)
datain$month <- sapply(datain$month,function(x) grep(paste("(?i)",x,sep=""),month.abb))
datain$day_of_week <- dplyr::recode(datain$day_of_week, 
       "mon"="1",
       "tue"="2",
       "wed"="3",
       "thu"="4",
       "fri"="5",
       "sat"="6",
       "sun"="7")
datain$day_of_week <- as.factor(datain$day_of_week)
datain$poutcome <- as.factor(datain$poutcome)

# setting up parms
init = mice(datain, maxit=0) 
meth = init$method
predM = init$predictorMatrix

# impute the following columns
meth[c("job", "marital","education","default","housing", "loan")]="polyreg"

# imputate & new dataset
set.seed(123)
imputed = mice(datain, method=meth, predictorMatrix=predM, m=1)
imputed <- complete(imputed)
sapply(imputed, function(x) sum(is.na(x)))

# convert to numeric
imputed$job <- as.numeric(imputed$job)
imputed$marital <- as.numeric(imputed$marital)
imputed$education <- as.numeric(imputed$education)
imputed$default <- as.numeric(imputed$default)
imputed$housing <- as.numeric(imputed$housing)
imputed$loan <- as.numeric(imputed$loan)
imputed$contact <- as.numeric(imputed$contact)
imputed$poutcome <- as.numeric(imputed$poutcome)
imputed$day_of_week <- as.numeric(imputed$day_of_week)
```

## EDA (don't bulk run. Open and pick plot)
```{r}
EDAD <- imputed
EDAD$y <- as.factor(EDAD$y)
EDAD$y <- as.numeric(EDAD$y)

# Histogram of each variable
par(mfrow = c(5, 4))
MASS::truehist(EDAD$age)
MASS::truehist(EDAD$job)
MASS::truehist(EDAD$marital)
MASS::truehist(EDAD$education)
MASS::truehist(EDAD$default)
MASS::truehist(EDAD$housing)
MASS::truehist(EDAD$loan)
MASS::truehist(EDAD$contact)
MASS::truehist(EDAD$month)
MASS::truehist(EDAD$day_of_week)
MASS::truehist(EDAD$duration)
MASS::truehist(EDAD$campaign)
MASS::truehist(EDAD$pdays)
MASS::truehist(EDAD$previous)
MASS::truehist(EDAD$poutcome)
MASS::truehist(EDAD$emp.var.rate)
MASS::truehist(EDAD$cons.price.idx)
MASS::truehist(EDAD$cons.conf.idx)
MASS::truehist(EDAD$euribor3m)
MASS::truehist(EDAD$nr.employed)

# Correlation to y variable
x <- EDAD %>% correlate() %>% focus(y)
x

# Pairs plot. Colored by Yes/No (LONG RUN TIME)
#pairs(EDAD[1:20],main="Whole Dataset", pch=19, col=as.numeric(EDAD$y)+1)
#mtext("y column: red-> Yes; green-> No", 1, line=3.7,cex=.8)
```

## Normalization Techniques - EDA
```{r}
BNAge <- bestNormalize(as.numeric(EDAD$age, allow_orderNorm = TRUE, out_of_sample = FALSE))
BNAge
# For Age, no transform is neccessary

BNJob <- bestNormalize(as.numeric(EDAD$job,allow_orderNorm = FALSE, out_of_sample = FALSE))
BNJob
# For Job, no transform is neccessary

BNCam <- bestNormalize(as.numeric(EDAD$campaign,allow_orderNorm = FALSE, out_of_sample = FALSE))
BNCam
# For Campaign, no transform is neccessary

BNConsPrice <- bestNormalize(as.numeric(EDAD$cons.price.idx,allow_orderNorm = FALSE, out_of_sample = FALSE))
BNConsPrice
# For cons.price.idx, no transform is neccessary

BNConsConf <- bestNormalize(as.numeric(EDAD$cons.conf.idx,allow_orderNorm = FALSE, out_of_sample = FALSE))
BNConsConf
# For cons.conf.idx, no transform is neccessary

BNNREmp <- bestNormalize(as.numeric(EDAD$nr.employed,allow_orderNorm = FALSE, out_of_sample = FALSE))
BNNREmp
# For nr.employed, no transform is neccessary
```

# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

# Objective 1 - Simple Logistic Regression
## Final Datasets to be used in objective 1
```{r}
set.seed(123)

# Select transformations
EDAD2 <- EDAD

# EDA dataset - narrowed down to columns to be used in regression
edadata <- EDAD2
edadata <- edadata %>%
  select(age, job, marital, education, contact, month, day_of_week, campaign, pdays, previous,
         poutcome, cons.price.idx, cons.conf.idx, y)

# Original unbalanced dataset
odata <- edadata 

# Balanced dataset using under weighting
bdata <-ovun.sample(y ~ .,data = edadata, method = "under", N = 9280)$data
prop.table(table(bdata$y))

# Balanced using ROSE package
rdata <- ROSE(y ~ ., data = edadata)$data
prop.table(table(rdata$y))
```

# ----------------------

## Logit with original data 89.69%
```{r}
set.seed(123)

logit.os.data <- odata
logit.os.data$y <- as.factor(logit.os.data$y)

# Test & train
trainIndex <- createDataPartition(logit.os.data$y, p = .7,
                                  list = FALSE,
                                  times = 1)

logit.os.Train <- logit.os.data[ trainIndex,]
logit.os.Test  <- logit.os.data[-trainIndex,]

# Build Full Model
full.os.log<-glm(y~.,family="binomial",data=logit.os.Train)

# Step Model
step.os.log<-full.os.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os.log)

# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))

# VIF Scores
vif(step.os.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
```

## Logit with balanced data 64.29%
```{r}
set.seed(123)

logit.bs.data <- bdata
logit.bs.data$y <- as.factor(logit.bs.data$y)

# Test & train
trainIndex <- createDataPartition(logit.bs.data$y, p = .7,
                                  list = FALSE,
                                  times = 1)

logit.bs.Train <- logit.bs.data[ trainIndex,]
logit.bs.Test  <- logit.bs.data[-trainIndex,]

# Build Full Model
full.bs.log<-glm(y~.,family="binomial",data=logit.bs.Train)

# Step Model
step.bs.log<-full.bs.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.bs.log)

# Error Metrics
step.bs.aic <- step.bs.log$aic
step.bs.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.bs.log), confint.default(step.bs.log, level = 0.95)))

# VIF Scores
vif(step.bs.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.bs.log,newdata=logit.bs.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.bs.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.bs.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.bs.logit.acc
```

## Logit with rose data 64.37%
```{r}
set.seed(123)

logit.rs.data <- rdata
logit.rs.data$y <- as.factor(logit.rs.data$y)

# Test & train
trainIndex <- createDataPartition(logit.rs.data$y, p = .7,
                                  list = FALSE,
                                  times = 1)

logit.rs.Train <- logit.rs.data[ trainIndex,]
logit.rs.Test  <- logit.rs.data[-trainIndex,]

# Build Full Model
full.rs.log<-glm(y~.,family="binomial",data=logit.rs.Train)

# Step Model
step.rs.log<-full.rs.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.rs.log)

# Error Metrics
step.rs.aic <- step.rs.log$aic
step.rs.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.rs.log), confint.default(step.rs.log, level = 0.95)))

# VIF Scores
vif(step.rs.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.rs.log,newdata=logit.rs.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.rs.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.rs.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.rs.logit.acc
```

# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

# Objective 2
## Final Datasets to be used in objective 2
```{r}
set.seed(123)

# Select transformations
EDAD2 <- EDAD
EDAD2$pdays <- ifelse(EDAD2$pdays == 999, -1, EDAD2$pdays)
EDAD2$age <- log(EDAD2$age)


# EDA dataset - narrowed down to columns to be used in regression
edadata <- EDAD2
edadata <- edadata %>%
  select(age, job, marital, education, contact, month, day_of_week, campaign, pdays, previous,
         poutcome, cons.price.idx, cons.conf.idx, y)

# Original unbalanced dataset
odata <- edadata 

# Balanced dataset using under weighting
bdata <-ovun.sample(y ~ .,data = edadata, method = "under", N = 9280)$data
prop.table(table(bdata$y))

# Balanced using ROSE package
rdata <- ROSE(y ~ ., data = edadata)$data
prop.table(table(rdata$y))
```

# ------------------------------------

## Simple with original 87.85%
```{r}
set.seed(123)

simple.o.lr <- odata
simple.o.lr$y <- as.numeric(as.factor(simple.o.lr$y))

# Create Training and Test data -
simple.index <- sample(1:nrow(simple.o.lr), 0.7*nrow(simple.o.lr))  # row indices for training data
simple.o.train <- simple.o.lr[simple.index, ]  # model training data
simple.o.test  <- simple.o.lr[-simple.index, ]   # test data

# Build the model on training data -
simple.o.lm <- lm(y ~ ., data=simple.o.train)  # build the model
simple.o.pred <- predict(simple.o.lm, simple.o.test)  # predict y

# AIC & BIC
simple.o.aic <- AIC(simple.o.lm)
simple.o.bic <- BIC(simple.o.lm)
simple.o.aic
simple.o.bic

# Summary
summary(simple.o.lm)

# Prediction accuracy and eror rates
actuals_preds <- data.frame(cbind(actuals=simple.o.test$y, predicteds=simple.o.pred))
min_max_accuracy <- mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals)
simple.o.acc <- min_max_accuracy
simple.o.acc
mape

vif(simple.o.lm)
```

## Simple with balanced data 74.61%
```{r}
set.seed(123)

simple.b.lr <- bdata
simple.b.lr$y <- as.numeric(as.factor(simple.b.lr$y))

# Create Training and Test data -
simple.index <- sample(1:nrow(simple.b.lr), 0.7*nrow(simple.b.lr))  # row indices for training data
simple.b.train <- simple.b.lr[simple.index, ]  # model training data
simple.b.test  <- simple.b.lr[-simple.index, ]   # test data

# Build the model on training data -
simple.b.lm <- lm(y ~ ., data=simple.b.train)  # build the model
simple.b.pred <- predict(simple.b.lm, simple.b.test)  # predict y

# AIC & BIC
simple.b.aic <- AIC(simple.b.lm)
simple.b.bic <- BIC(simple.b.lm)
simple.b.aic

# Summary
summary(simple.b.lm)

# Prediction accuracy and eror rates
actuals_preds <- data.frame(cbind(actuals=simple.b.test$y, predicteds=simple.b.pred))
min_max_accuracy <- mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals)
simple.b.acc <- min_max_accuracy
simple.b.acc
mape

vif(simple.b.lm)
```

## Simple with rose data 74.19%
```{r}
set.seed(123)

simple.r.lr <- rdata
simple.r.lr$y <- as.numeric(as.factor(simple.r.lr$y))

# Create Training and Test data -
simple.index <- sample(1:nrow(simple.r.lr), 0.7*nrow(simple.r.lr))  # row indices for training data
simple.r.train <- simple.r.lr[simple.index, ]  # model training data
simple.r.test  <- simple.r.lr[-simple.index, ]   # test data

# Build the model on training data -
simple.r.lm <- lm(y ~ ., data=simple.r.train)  # build the model
simple.r.pred <- predict(simple.r.lm, simple.r.test)  # predict y

# AIC & BIC
simple.r.aic <- AIC(simple.r.lm)
simplr.r.bic <- BIC(simple.r.lm)
simple.r.aic

# Summary
summary(simple.r.lm)

# Prediction accuracy and eror rates
actuals_preds <- data.frame(cbind(actuals=simple.r.test$y, predicteds=simple.r.pred))
min_max_accuracy <- mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals)
simple.r.acc <- min_max_accuracy
simple.r.acc
mape

vif(simple.r.lm)
```

# -------------------------------------

## Logit with original data 89.69% (NEED INTERACTION TERMS OR NEW VARS)
```{r}
set.seed(123)

logit.o.data <- odata
logit.o.data$y <- as.factor(logit.o.data$y)

# Test & train
trainIndex <- createDataPartition(logit.o.data$y, p = .7,
                                  list = FALSE,
                                  times = 1)

logit.o.Train <- logit.o.data[ trainIndex,]
logit.o.Test  <- logit.o.data[-trainIndex,]

# Build Full Model
full.o.log<-glm(y ~ age + job + marital + education + contact + month + day_of_week + campaign + pdays + previous +
         poutcome + cons.price.idx + cons.conf.idx, family="binomial",data=logit.o.Train)

# Step Model
step.o.log<-full.o.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.o.log)

# Error Metrics
step.o.aic <- step.o.log$aic
step.o.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.o.log), confint.default(step.o.log, level = 0.95)))

# VIF Scores
vif(step.o.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.o.log,newdata=logit.o.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.o.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.o.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.o.logit.acc
```

## Logit with balanced data 69.99% (NEED INTERACTION TERMS OR NEW VARS)
```{r}
set.seed(123)

logit.b.data <- bdata
logit.b.data$y <- as.factor(logit.b.data$y)

# Test & train
trainIndex <- createDataPartition(logit.b.data$y, p = .7,
                                  list = FALSE,
                                  times = 1)

logit.b.Train <- logit.b.data[ trainIndex,]
logit.b.Test  <- logit.b.data[-trainIndex,]

# Build Full Model
full.b.log<-glm(y~.,family="binomial",data=logit.b.Train)

# Step Model
step.b.log<-full.b.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.b.log)

# Error Metrics
step.b.aic <- step.b.log$aic
step.b.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.b.log), confint.default(step.b.log, level = 0.95)))

# VIF Scores
vif(step.b.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.b.log,newdata=logit.o.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.o.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.b.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.b.logit.acc
```

## Logit with rose data 68.46% (NEED INTERACTION TERMS OR NEW VARS)
```{r}
set.seed(123)

logit.r.data <- rdata
logit.r.data$y <- as.factor(logit.r.data$y)

# Test & train
trainIndex <- createDataPartition(logit.r.data$y, p = .7,
                                  list = FALSE,
                                  times = 1)

logit.r.Train <- logit.r.data[ trainIndex,]
logit.r.Test  <- logit.r.data[-trainIndex,]

# Build Full Model
full.r.log<-glm(y~.,family="binomial",data=logit.r.Train)

# Step Model
step.r.log<-full.r.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.r.log)

# Error Metrics
step.r.aic <- step.r.log$aic
step.r.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.r.log), confint.default(step.r.log, level = 0.95)))

# VIF Scores
vif(step.r.log)

# Predictions & Accuracy
fit.pred.step<-predict(step.r.log,newdata=logit.o.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.o.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.r.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.r.logit.acc
```

# ------------------

## LDA with original data 89.83%
```{r}
set.seed(123)

# Split the data into training (70%) and test set (30%)
lda.o.data <- odata
training.samples <- lda.o.data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
LDA.o.train.data <- lda.o.data[training.samples, ]
LDA.o.test.data <- lda.o.data[-training.samples, ]

# Estimate preprocessing parameters
o.preproc.param <- LDA.o.train.data %>% 
  preProcess(method = c("center", "scale"))
# Transform the data using the estimated parameters
LDA.o.train.transformed <- o.preproc.param %>% predict(LDA.o.train.data)
LDA.o.test.transformed <- o.preproc.param %>% predict(LDA.o.test.data)

# Model
lda.o.model <- lda(y ~ ., data = LDA.o.train.transformed)
lda.o.model

# Plot results
plot(lda.o.model)

# Make predictions
lda.o.predictions <- lda.o.model %>% predict(LDA.o.test.transformed)

# Accuracy
lda.o.acc <- mean(lda.o.predictions$class==LDA.o.test.transformed$y)
lda.o.acc
```

## LDA with balanced data 64.11%
```{r}
set.seed(123)

# Split the data into training (70%) and test set (30%)
set.seed(123)
lda.b.data <- bdata
training.samples <- lda.b.data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
LDA.b.train.data <- lda.b.data[training.samples, ]
LDA.b.test.data <- lda.b.data[-training.samples, ]

# Estimate preprocessing parameters
b.preproc.param <- LDA.b.train.data %>% 
  preProcess(method = c("center", "scale"))
# Transform the data using the estimated parameters
LDA.b.train.transformed <- b.preproc.param %>% predict(LDA.b.train.data)
LDA.b.test.transformed <- b.preproc.param %>% predict(LDA.b.test.data)

# Model
lda.b.model <- lda(y ~ ., data = LDA.b.train.transformed)
lda.b.model

# Plot results
plot(lda.b.model)

# Make predictions
lda.b.predictions <- lda.b.model %>% predict(LDA.b.test.transformed)

# Accuracy
lda.b.acc <- mean(lda.b.predictions$class==LDA.b.test.transformed$y)
lda.b.acc
```

## LDA with rose data 64.49%
```{r}
set.seed(123)

# Split the data into training (70%) and test set (30%)
set.seed(123)
lda.r.data <- rdata
training.samples <- lda.r.data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
LDA.r.train.data <- lda.r.data[training.samples, ]
LDA.r.test.data <- lda.r.data[-training.samples, ]

# Estimate preprocessing parameters
r.preproc.param <- LDA.r.train.data %>% 
  preProcess(method = c("center", "scale"))
# Transform the data using the estimated parameters
LDA.r.train.transformed <- r.preproc.param %>% predict(LDA.r.train.data)
LDA.r.test.transformed <- r.preproc.param %>% predict(LDA.r.test.data)

# Model
lda.r.model <- lda(y ~ ., data = LDA.r.train.transformed)
lda.r.model

# Plot results
plot(lda.r.model)

# Make predictions
lda.r.predictions <- lda.r.model %>% predict(LDA.r.test.transformed)

# Accuracy
lda.r.acc <- mean(lda.r.predictions$class==LDA.r.test.transformed$y)
lda.r.acc
```

# ------------------

## QDA with original data 88.17%
```{r}
set.seed(123)

QDA.o.Data <- odata

QDA.o.Data$y <- as.numeric(as.factor(QDA.o.Data$y))

training.samples <- QDA.o.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
QDA.o.train.data <- QDA.o.Data[training.samples, ]
QDA.o.test.data <- QDA.o.Data[-training.samples, ]

# Fit the model
qda.o.model <- qda(y ~ ., data = QDA.o.train.data)
qda.o.model

# Make predictions
qda.o.predictions <- qda.o.model %>% predict(QDA.o.test.data)

# Model accuracy
qda.o.acc <- mean(qda.o.predictions$class == QDA.o.test.data$y)
qda.o.acc
```

## QDA with balanced data 65.12%
```{r}
set.seed(123)

QDA.b.Data <- bdata

QDA.b.Data$y <- as.numeric(as.factor(QDA.b.Data$y))

training.samples <- QDA.b.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)

QDA.b.train.data <- QDA.b.Data[training.samples, ]
QDA.b.test.data <- QDA.b.Data[-training.samples, ]

# Fit the model
qda.b.model <- qda(y ~ ., data = QDA.b.train.data)
qda.b.model

# Make predictions
qda.b.predictions <- qda.b.model %>% predict(QDA.b.test.data)

# Model accuracy
qda.b.acc <- mean(qda.b.predictions$class == QDA.b.test.data$y)
qda.b.acc
```

## QDA with rose data 80.67%
```{r}
set.seed(123)

QDA.r.Data <- rdata

QDA.r.Data$y <- as.numeric(as.factor(QDA.r.Data$y))

training.samples <- QDA.r.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)

QDA.r.train.data <- QDA.r.Data[training.samples, ]
QDA.r.test.data <- QDA.r.Data[-training.samples, ]

# Fit the model
qda.r.model <- qda(y ~ ., data = QDA.r.train.data)
qda.r.model

# Make predictions
qda.r.predictions <- qda.r.model %>% predict(QDA.r.test.data)

# Model accuracy
qda.r.acc <- mean(qda.r.predictions$class == QDA.r.test.data$y)
qda.r.acc
```

# -----------------------------------------------------------------------------

## Ridge with original data 87.85%
```{r}
set.seed(123)

ridge.o.lr <- odata

ridge.o.lr$y <- as.numeric(as.factor(ridge.o.lr$y))

ridge.Index <- sample(1:nrow(ridge.o.lr), 0.7*nrow(ridge.o.lr)) # indices for 70% training data
ridge.o.train <- ridge.o.lr[ridge.Index, ] # training data
ridge.o.test <- ridge.o.lr[-ridge.Index, ] # test data

ridge.o.model <- linearRidge(y ~ ., data = ridge.o.train)

summary(ridge.o.model)

ridge.o.pred <- predict(ridge.o.model, ridge.o.test)  # predict on test data
ridge.o.compare <- cbind (actual=ridge.o.test$y, ridge.o.pred)  # combine

ridge.o.acc <- mean(apply(ridge.o.compare, 1, min)/apply(ridge.o.compare, 1, max)) 
ridge.o.acc

```

## Ridge with balanced data 74.58%
```{r}
set.seed(123)

ridge.b.lr <- bdata

ridge.b.lr$y <- as.numeric(as.factor(ridge.b.lr$y))

ridge.Index <- sample(1:nrow(ridge.b.lr), 0.7*nrow(ridge.b.lr)) # indices for 70% training data
ridge.b.train <- ridge.b.lr[ridge.Index, ] # training data
ridge.b.test <- ridge.b.lr[-ridge.Index, ] # test data

ridge.b.model <- linearRidge(y ~ ., data = ridge.b.train)
summary(ridge.b.model)

ridge.b.pred <- predict(ridge.b.model, ridge.b.test)  # predict on test data
ridge.b.compare <- cbind (actual=ridge.b.test$y, ridge.b.pred)  # combine

ridge.b.acc <- mean(apply(ridge.b.compare, 1, min)/apply(ridge.b.compare, 1, max))
ridge.b.acc
```

## Ridge with rose data 74.18%
```{r}
set.seed(123)

ridge.r.lr <- rdata

ridge.r.lr$y <- as.numeric(as.factor(ridge.r.lr$y))

ridge.Index <- sample(1:nrow(ridge.r.lr), 0.7*nrow(ridge.r.lr)) # indices for 70% training data
ridge.r.train <- ridge.r.lr[ridge.Index, ] # training data
ridge.r.test <- ridge.r.lr[-ridge.Index, ] # test data

ridge.r.model <- linearRidge(y ~ ., data = ridge.r.train)

summary(ridge.r.model)

ridge.r.pred <- predict(ridge.r.model, ridge.r.test)  # predict on test data
ridge.r.compare <- cbind (actual=ridge.r.test$y, ridge.r.pred)  # combine

ridge.r.acc <- mean(apply(ridge.r.compare, 1, min)/apply(ridge.r.compare, 1, max)) 
ridge.r.acc
```

# -------------------------------------------------------------------------------

## LASSO Regression original data 88.73%
```{r}
set.seed(123)

lasso.o.lr <- odata
lasso.o.lr$y <- as.factor(lasso.o.lr$y)

training.samples <- lasso.o.lr$y %>%
  createDataPartition(p = 0.7, list = FALSE)
lasso.o.train.data <- lasso.o.lr[training.samples, ]
lasso.o.test.data <- lasso.o.lr[-training.samples, ]

dat.o.train.x <- model.matrix(y~.,lasso.o.train.data)
dat.o.train.y <- lasso.o.train.data[,14]

cvfit <- cv.glmnet(dat.o.train.x, dat.o.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#CV misclassification error rate is little below .1
print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

#For final model predictions go ahead and refit lasso using entire
lasso.o.model<-glmnet(dat.o.train.x, dat.o.train.y, family = "binomial",lambda=cvfit$lambda.min)

dat.o.test.x<-model.matrix(y~.,lasso.o.test.data)
fit.o.pred.lasso <- predict(lasso.o.model, newx = dat.o.test.x, type = "response")

#Lets use the predicted probablities to classify the observations and make a final confusion matrix for the two models.  We can use it to calculate error metrics.
#Lets use a cutoff of 0.5 to make the classification.
cutoff<-0.5
class.o.lasso<-factor(ifelse(fit.o.pred.lasso>cutoff,"no","yes"),levels=c("yes","no"))

#Confusion Matrix 
conf.o.lasso<-table(class.o.lasso,lasso.o.test.data$y)
print("Confusion matrix for LASSO")
conf.o.lasso

#Accuracy 
lasso.o.acc <- sum(diag(conf.o.lasso))/sum(conf.o.lasso)
lasso.o.acc 
```

## LASSO Regression balanced data 60.04%
```{r}
set.seed(123)

lasso.b.lr <- bdata
lasso.b.lr$y <- as.factor(lasso.b.lr$y)

training.samples <- lasso.b.lr$y %>%
  createDataPartition(p = 0.7, list = FALSE)
lasso.b.train.data <- lasso.b.lr[training.samples, ]
lasso.b.test.data <- lasso.b.lr[-training.samples, ]

dat.b.train.x <- model.matrix(y~.,lasso.b.train.data)
dat.b.train.y <- lasso.b.train.data[,14]

cvfit <- cv.glmnet(dat.b.train.x, dat.b.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#CV misclassification error rate is little below .1
print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

#For final model predictions go ahead and refit lasso using entire
lasso.b.model<-glmnet(dat.b.train.x, dat.b.train.y, family = "binomial",lambda=cvfit$lambda.min)

dat.b.test.x<-model.matrix(y~.,lasso.b.test.data)
fit.b.pred.lasso <- predict(lasso.b.model, newx = dat.b.test.x, type = "response")

#Lets use the predicted probablities to classify the observations and make a final confusion matrix for the two models.  We can use it to calculate error metrics.
#Lets use a cutoff of 0.5 to make the classification.
cutoff<-0.5
class.b.lasso<-factor(ifelse(fit.b.pred.lasso>cutoff,"no","yes"),levels=c("yes","no"))

#Confusion Matrix 
conf.b.lasso<-table(class.b.lasso,lasso.b.test.data$y)
print("Confusion matrix for LASSO")
conf.b.lasso

#Accuracy 
lasso.b.acc <- sum(diag(conf.b.lasso))/sum(conf.b.lasso)
lasso.b.acc 
```

## LASSO Regression Rose data 65.46%
```{r}
set.seed(123)

lasso.r.lr <- rdata
lasso.r.lr$y <- as.factor(lasso.r.lr$y)

training.samples <- lasso.r.lr$y %>%
  createDataPartition(p = 0.7, list = FALSE)
lasso.r.train.data <- lasso.r.lr[training.samples, ]
lasso.r.test.data <- lasso.r.lr[-training.samples, ]

dat.r.train.x <- model.matrix(y~.,lasso.r.train.data)
dat.r.train.y <- lasso.r.train.data[,14]

cvfit <- cv.glmnet(dat.r.train.x, dat.r.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#CV misclassification error rate is little below .1
print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

#For final model predictions go ahead and refit lasso using entire
lasso.r.model<-glmnet(dat.r.train.x, dat.r.train.y, family = "binomial",lambda=cvfit$lambda.min)

dat.r.test.x<-model.matrix(y~.,lasso.r.test.data)
fit.r.pred.lasso <- predict(lasso.r.model, newx = dat.r.test.x, type = "response")

#Lets use the predicted probablities to classify the observations and make a final confusion matrix for the two models.  We can use it to calculate error metrics.
#Lets use a cutoff of 0.5 to make the classification.
cutoff<-0.5
class.r.lasso<-factor(ifelse(fit.r.pred.lasso>cutoff,"no","yes"),levels=c("yes","no"))

#Confusion Matrix 
conf.r.lasso<-table(class.r.lasso,lasso.r.test.data$y)
print("Confusion matrix for LASSO")
conf.r.lasso

#Accuracy 
lasso.r.acc <- sum(diag(conf.r.lasso))/sum(conf.r.lasso)
lasso.r.acc 
```

# -------------------------------------------------------------------------------

## Net Regression balanced data 66.63% (Giving weird accuracy metric... not sure)
```{r}
set.seed(123)

net.o.lr <- odata
net.o.lr$y <- as.factor(net.o.lr$y)

training.samples <- net.o.lr$y %>%
  createDataPartition(p = 0.7, list = FALSE)
net.o.train.data <- net.o.lr[training.samples, ]
net.o.test.data <- net.o.lr[-training.samples, ]

dat.o.train.x <- model.matrix(y~.,net.o.train.data)
dat.o.train.y <- net.o.train.data[,14]

cvfit <- cv.glmnet(dat.o.train.x, dat.o.train.y, family = "binomial", type.measure = "class",
                   alpha = 0.5,  nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")

#CV misclassification error rate
print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

#For final model predictions go ahead and refit net using entire
net.o.model<-glmnet(dat.o.train.x, dat.o.train.y, family = "binomial", alpha = 0.5, lambda=cvfit$lambda.min)

dat.o.test.x<-model.matrix(y~.,net.o.test.data)
fit.o.pred.net <- predict(net.o.model, newx = dat.o.test.x, type = "response")

#Lets use the predicted probablities to classify the observations and make a final confusion matrix for the two models.  We can use it to calculate error metrics.
#Lets use a cutoff of 0.5 to make the classification.
cutoff<-0.5
class.o.net<-factor(ifelse(fit.o.pred.net>cutoff,"no","yes"),levels=c("yes","no"))

#Confusion Matrix 
conf.o.net<-table(class.o.net,net.o.test.data$y)
print("Confusion matrix for net")
conf.o.net

#Accuracy 
net.o.acc <- sum(diag(conf.o.net))/sum(conf.o.net)
net.o.acc 
```

## Net Regression balanced data 66.63%
```{r}
set.seed(123)

net.b.lr <- bdata
net.b.lr$y <- as.factor(net.b.lr$y)

training.samples <- net.b.lr$y %>%
  createDataPartition(p = 0.7, list = FALSE)
net.b.train.data <- net.b.lr[training.samples, ]
net.b.test.data <- net.b.lr[-training.samples, ]

dat.b.train.x <- model.matrix(y~.,net.b.train.data)
dat.b.train.y <- net.b.train.data[,14]

cvfit <- cv.glmnet(dat.b.train.x, dat.b.train.y, family = "binomial", type.measure = "class",
                   alpha = 0.5,  nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#CV misclassification error rate
print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

#For final model predictions go ahead and refit net using entire
net.b.model<-glmnet(dat.b.train.x, dat.b.train.y, family = "binomial", alpha = 0.5, lambda=cvfit$lambda.min)

dat.b.test.x<-model.matrix(y~.,net.b.test.data)
fit.b.pred.net <- predict(net.b.model, newx = dat.b.test.x, type = "response")

#Lets use the predicted probablities to classify the observations and make a final confusion matrix for the two models.  We can use it to calculate error metrics.
#Lets use a cutoff of 0.5 to make the classification.
cutoff<-0.5
class.b.net<-factor(ifelse(fit.b.pred.net>cutoff,"no","yes"),levels=c("yes","no"))

#Confusion Matrix 
conf.b.net<-table(class.b.net,net.b.test.data$y)
print("Confusion matrix for net")
conf.b.net

#Accuracy 
net.b.acc <- sum(diag(conf.b.net))/sum(conf.b.net)
net.b.acc 
```

## Net Regression Rose data 65.89%
```{r}
set.seed(123)

net.r.lr <- rdata
net.r.lr$y <- as.factor(net.r.lr$y)

training.samples <- net.r.lr$y %>%
  createDataPartition(p = 0.7, list = FALSE)
net.r.train.data <- net.r.lr[training.samples, ]
net.r.test.data <- net.r.lr[-training.samples, ]

dat.r.train.x <- model.matrix(y~.,net.r.train.data)
dat.r.train.y <- net.r.train.data[,14]

cvfit <- cv.glmnet(dat.r.train.x, dat.r.train.y, family = "binomial", type.measure = "class",
                   alpha = 0.5,  nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#CV misclassification error rate
print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

#For final model predictions go ahead and refit net using entire
net.r.model<-glmnet(dat.r.train.x, dat.r.train.y, family = "binomial", alpha = 0.5, lambda=cvfit$lambda.min)

dat.r.test.x<-model.matrix(y~.,net.r.test.data)
fit.r.pred.net <- predict(net.r.model, newx = dat.r.test.x, type = "response")

#Lets use the predicted probablities to classify the observations and make a final confusion matrix for the two models.  We can use it to calculate error metrics.
#Lets use a cutoff of 0.5 to make the classification.
cutoff<-0.5
class.r.net<-factor(ifelse(fit.r.pred.net>cutoff,"no","yes"),levels=c("yes","no"))

#Confusion Matrix 
conf.r.net<-table(class.r.net,net.r.test.data$y)
print("Confusion matrix for net")
conf.r.net

#Accuracy 
net.r.acc <- sum(diag(conf.r.net))/sum(conf.r.net)
net.r.acc 
```

# -------------------------------------------------------------------------------

## MDA with original data 88.69%
```{r}
set.seed(123)

MDA.o.Data <- odata
MDA.o.Data$y <- as.numeric(as.factor(MDA.o.Data$y))

training.samples <- MDA.o.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)

MDA.o.train.data <- MDA.o.Data[training.samples, ]
MDA.o.test.data <- MDA.o.Data[-training.samples, ]

# Fit the model
mda.o.model <- mda(y ~ ., data = MDA.o.train.data)
mda.o.model
# Make predictions
mda.o.predicted.classes <- mda.o.model %>% predict(MDA.o.test.data)
# Model accuracy
mda.o.acc <- mean(mda.o.predicted.classes == MDA.o.test.data$y)
mda.o.acc
```

## MDA with balanced data 63.72%
```{r}
set.seed(123)

MDA.b.Data <- bdata
MDA.b.Data$y <- as.numeric(as.factor(MDA.b.Data$y))

training.samples <- MDA.b.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)

MDA.b.train.data <- MDA.b.Data[training.samples, ]
MDA.b.test.data <- MDA.b.Data[-training.samples, ]

# Fit the model
mda.b.model <- mda(y ~ ., data = MDA.b.train.data)
mda.b.model
# Make predictions
mda.b.predicted.classes <- mda.b.model %>% predict(MDA.b.test.data)
# Model accuracy
mda.b.acc <- mean(mda.b.predicted.classes == MDA.b.test.data$y)
mda.b.acc
```

## MDA with rose data 67.19%
```{r}
set.seed(123)

MDA.r.Data <- rdata
MDA.r.Data$y <- as.numeric(as.factor(MDA.r.Data$y))

training.samples <- MDA.r.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)

MDA.r.train.data <- MDA.r.Data[training.samples, ]
MDA.r.test.data <- MDA.r.Data[-training.samples, ]

# Fit the model
mda.r.model <- mda(y ~ ., data = MDA.r.train.data)
mda.r.model
# Make predictions
mda.r.predicted.classes <- mda.r.model %>% predict(MDA.r.test.data)
# Model accuracy
mda.r.acc <- mean(mda.r.predicted.classes == MDA.r.test.data$y)
mda.r.acc
```

# -------------------------------------------------------------------------------

## FDA with original data 89.83%
```{r}
set.seed(123)

FDA.o.Data <- odata
FDA.o.Data$y <- as.numeric(as.factor(FDA.o.Data$y))

training.samples <- FDA.o.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)

FDA.o.train.data <- FDA.o.Data[training.samples, ]
FDA.o.test.data <- FDA.o.Data[-training.samples, ]

# Fit the model
fda.o.model <- fda(y ~ ., data = FDA.o.train.data)
# Make predictions
fda.o.predicted.classes <- fda.o.model %>% predict(FDA.o.test.data)
# Model accuracy
fda.o.acc <- mean(fda.o.predicted.classes == FDA.o.test.data$y)
fda.o.acc
```

## FDA with balanced data 64.11%
```{r}
set.seed(123)

FDA.b.Data <- bdata
FDA.b.Data$y <- as.numeric(as.factor(FDA.b.Data$y))

training.samples <- FDA.b.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)

FDA.b.train.data <- FDA.b.Data[training.samples, ]
FDA.b.test.data <- FDA.b.Data[-training.samples, ]

# Fit the model
fda.b.model <- fda(y ~ ., data = FDA.b.train.data)
# Make predictions
fda.b.predicted.classes <- fda.b.model %>% predict(FDA.b.test.data)
# Model accuracy
fda.b.acc <- mean(fda.b.predicted.classes == FDA.b.test.data$y)
fda.b.acc
```

## FDA with rose data 64.49%
```{r}
set.seed(123)

FDA.r.Data <- rdata
FDA.r.Data$y <- as.numeric(as.factor(FDA.r.Data$y))

training.samples <- FDA.r.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)

FDA.r.train.data <- FDA.r.Data[training.samples, ]
FDA.r.test.data <- FDA.r.Data[-training.samples, ]

# Fit the model
fda.r.model <- fda(y ~ ., data = FDA.r.train.data)
# Make predictions
fda.r.predicted.classes <- fda.r.model %>% predict(FDA.r.test.data)
# Model accuracy
fda.r.acc <- mean(fda.r.predicted.classes == FDA.r.test.data$y)
fda.r.acc
```

# -------------------------------------------------------------------------------

## RDA with original data 89.85%
```{r}
set.seed(123)

RDA.o.Data <- odata
RDA.o.Data$y <- as.numeric(as.factor(RDA.o.Data$y))

training.samples <- RDA.o.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)

RDA.o.train.data <- RDA.o.Data[training.samples, ]
RDA.o.test.data <- RDA.o.Data[-training.samples, ]

# Fit the model
rda.o.model <- rda(y ~ ., data = RDA.o.train.data)
# Make predictions
rda.o.predictions <- rda.o.model %>% predict(RDA.o.test.data)
# Model accuracy
rda.o.acc <- mean(rda.o.predictions$class == RDA.o.test.data$y)
rda.o.acc
```

## RDA with balanced data 64.76%
```{r}
set.seed(123)

RDA.b.Data <- bdata
RDA.b.Data$y <- as.numeric(as.factor(RDA.b.Data$y))

training.samples <- RDA.b.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)

RDA.b.train.data <- RDA.b.Data[training.samples, ]
RDA.b.test.data <- RDA.b.Data[-training.samples, ]

# Fit the model
rda.b.model <- rda(y ~ ., data = RDA.b.train.data)
# Make predictions
rda.b.predictions <- rda.b.model %>% predict(RDA.b.test.data)
# Model accuracy
rda.b.acc <- mean(rda.b.predictions$class == RDA.b.test.data$y)
rda.b.acc
```

## RDA with rose data 80.67%
```{r}
set.seed(123)

RDA.r.Data <- rdata
RDA.r.Data$y <- as.numeric(as.factor(RDA.r.Data$y))

training.samples <- RDA.r.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)

RDA.r.train.data <- RDA.r.Data[training.samples, ]
RDA.r.test.data <- RDA.r.Data[-training.samples, ]

# Fit the model
rda.r.model <- rda(y ~ ., data = RDA.r.train.data)
# Make predictions
rda.r.predictions <- rda.r.model %>% predict(RDA.r.test.data)
# Model accuracy
rda.r.acc <- mean(rda.r.predictions$class == RDA.r.test.data$y)
rda.r.acc
```

# -------------------------------------------------------------------------------

## RF with original data 89.73% & 2 mtry
```{r}
# Split the data into training (70%) and test set (30%)
set.seed(123)
rf.o.data <- odata
training.samples <- rf.o.data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
rf.o.train.data <- rf.o.data[training.samples, ]
rf.o.test.data <- rf.o.data[-training.samples, ]

# Converting ‘y’ to a factor
rf.o.train.data$y <- factor(rf.o.train.data$y)

# set cross validation parameters
trControl <- trainControl(method = "cv",
    number = 10,
    search = "grid")

# Training using ‘random forest’ algorithm
rf.o.model <- train(y~.,data = rf.o.train.data, method = "rf", 
                    metric = "Accuracy", importance = TRUE,
                    trControl = trControl,  ntree = 50)

print(rf.o.model)
plot(rf.o.model)

rf.o.model$bestTune$mtry
rf.o.acc <- max(rf.o.model$results$Accuracy)
rf.o.acc
```

## RF with balanced data 74.06% & 2 mtry
```{r}
# Split the data into training (70%) and test set (30%)
set.seed(123)
rf.b.data <- bdata
training.samples <- rf.b.data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
rf.b.train.data <- rf.b.data[training.samples, ]
rf.b.test.data <- rf.b.data[-training.samples, ]

# Converting ‘y’ to a factor
rf.b.train.data$y <- factor(rf.b.train.data$y)

# set cross validation parameters
trControl <- trainControl(method = "cv",
    number = 10,
    search = "grid")

# Training using ‘random forest’ algorithm
rf.b.model <- train(y~.,data = rf.b.train.data, method = "rf", 
                    metric = "Accuracy", importance = TRUE,
                    trControl = trControl, ntree = 50)

print(rf.b.model)
plot(rf.b.model)

rf.b.model$bestTune$mtry
rf.b.acc <- max(rf.b.model$results$Accuracy)
rf.b.acc
```

## RF with rose data 88.68% & 2 mtry
```{r}
# Split the data into training (70%) and test set (30%)
set.seed(123)
rf.r.data <- rdata
training.samples <- rf.r.data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
rf.r.train.data <- rf.r.data[training.samples, ]
rf.r.test.data <- rf.r.data[-training.samples, ]

# Converting ‘y’ to a factor
rf.r.train.data$y <- factor(rf.r.train.data$y)

# set cross validation parameters
trControl <- trainControl(method = "cv",
    number = 10,
    search = "grid")

# Training using ‘random forest’ algorithm
rf.r.model <- train(y~.,data = rf.r.train.data, method = "rf", 
                    metric = "Accuracy", importance = TRUE,
                    trControl = trControl, ntree = 50)

print(rf.r.model)
plot(rf.r.model)

rf.r.model$bestTune$mtry
rf.r.acc <- max(rf.r.model$results$Accuracy)
rf.r.acc
```

# -------------------------------------------------------------------------------

## Accuracy table
```{r}
SIM.results <- cbind(simple.o.acc, simple.b.acc, simple.r.acc)
SLR.results <- cbind(step.os.logit.acc, step.bs.logit.acc, step.rs.logit.acc)
CLR.results <- cbind(step.o.logit.acc, step.b.logit.acc, step.r.logit.acc)
RR.results <- cbind(ridge.o.acc, ridge.b.acc, ridge.r.acc)
LSO.results <- cbind(lasso.o.acc, lasso.b.acc, lasso.r.acc)
EN.results <- cbind(net.o.acc, net.b.acc, net.r.acc)
LDA.results <- cbind(lda.o.acc, lda.b.acc, lda.r.acc)
QDA.results <- cbind(qda.o.acc, qda.b.acc, qda.r.acc)
MDA.results <- cbind(mda.o.acc, mda.b.acc, mda.r.acc)
FDA.results <- cbind(fda.o.acc, fda.b.acc, fda.r.acc)
RDA.results <- cbind(rda.o.acc, rda.b.acc, rda.r.acc)
RF.results <- cbind(rf.o.acc, rf.b.acc, rf.r.acc)

accuracy <- rbind(SIM.results, SLR.results, CLR.results, RR.results, 
                  LSO.results, EN.results, LDA.results, QDA.results, MDA.results,
                  FDA.results, RDA.results, RF.results)

colnames(accuracy) <- c("Unbalanced (Original)", "Under-balanced (9280)", "Rose-Balanced")
row.names(accuracy) <- c("Simple Regression","Stepwise Logistic", "Complex Logistic", 
                         "Ridge Regression", "Lasso Regression","Elastic Net","Linear Discrim", 
                         "Quadratic Discrim", "Multiple Discrim", "Functional Discrim", 
                         "Redundancy Analysis", "Random Forest")
accuracy
```

## AIC table
```{r}
SIM.aic.results <- cbind(simple.o.aic, simple.b.aic, simple.r.aic)
SLR.aic.results <- cbind(step.os.aic, step.bs.aic, step.rs.aic)
CLR.aic.results <- cbind(step.o.aic, step.b.aic, step.r.aic)
#RR.results <- cbind(ridge.o.acc, ridge.b.acc, ridge.r.acc)
#LSO.results <- cbind(lasso.o.acc, lasso.b.acc, lasso.r.acc)
#EN.results <- cbind(net.o.acc, net.b.acc, net.r.acc)
#FWD.results <- cbind(fwd.o.adjr2, fwd.b.adjr2, fwd.r.adjr2) 
#BCK.results <- cbind(bck.o.adjr2, bck.b.adjr2, bck.r.adjr2)
LDA.results <- cbind(lda.o.acc, lda.b.acc, lda.r.acc)
QDA.results <- cbind(qda.o.acc, qda.b.acc, qda.r.acc)
#MDA.results <- cbind(mda.o.acc, mda.b.acc, mda.r.acc)
#FDA.results <- cbind(fda.o.acc, fda.b.acc, fda.r.acc)
#RDA.results <- cbind(rda.o.acc, rda.b.acc, rda.r.acc)
#RF.results <- cbind(rf.o.acc, rf.b.acc, rf.r.acc)

aic.table <- rbind(SIM.aic.results, LR.aic.results, CLR.aic.results)

colnames(aic.table) <- c("Unbalanced", "Under-balanced", "Rose-Balanced")
row.names(aic.table) <- c("Simple Regression", "Stepwise Logistic", "Complex Logistic")
aic.table

```
