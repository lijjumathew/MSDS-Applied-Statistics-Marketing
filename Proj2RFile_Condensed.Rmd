---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

## Load libraries
```{r}
library(GGally)
library(scales)
library(formattable)
library(leaps)
library(car)
library(lme4)
library(mlbench)
library(caret)
library(tidyr)
library(MASS)
library(corrr)
library(randomForest)
library(taRifx)
library(ROSE)
library(arm)
library(glmnet)
library(imputeMissings)
library(ggvis)
library(mice)
library(ISLR)
library(plyr)
library(dplyr)
library(ISOweek)
library(corrplot)
library(PerformanceAnalytics)
library(psych)
library(psychTools)
library(mda)
library(klaR)
library(aod)
library(ggfortify)
library(factoextra)
#library(rgl)
library(tree)
library(randomForest)
library(ridge)
library(ggplot2)
library(RColorBrewer)
library(corrplot)
library(reshape2)
library(smotefamily)
library(DMwR)
library(pls)
library(bestNormalize)
library(ResourceSelection)
library(ROCR)
```
## Load in dataset and look at data
```{r}
# import data
datain <- read.csv("D:/MS Data Science/SMU/6372 - Applied Stats/Project 2/bank-additional-full.csv", header = TRUE)
#datain <- read.csv('/Users/lijjumathew/Library/Mobile Documents/com~apple~CloudDocs/Lijju/SMU/Courses/Applied Statistics/Project/Project2/bank-additional-full.csv', sep=";", header=TRUE)
# View data
str(datain)
summary(datain)

# convert to dataframe
datain <- as.data.frame(datain)

# Convert all "unknown" to NA
datain[datain == "unknown"] <- NA

# Count NA values in each column
na_count <-sapply(datain, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
na_count

# Count number of yes and no
y_count <-sapply(datain, function(y) sum(length(which(y=="yes"))))
y_count <- data.frame(y_count)
y_count

n_count <-sapply(datain, function(y) sum(length(which(y=="no"))))
n_count <- data.frame(n_count)
n_count

# there are 36,548 No's and 4,640 Yes's.
```
## Impute missing data & convert to numeric
```{r}
# convert to factor for imputation
datain$job <- as.factor(datain$job)
datain$marital <- as.factor(datain$marital)
datain$education <- as.factor(datain$education)
datain$default <- as.factor(datain$default)
datain$housing <- as.factor(datain$housing)
datain$loan <- as.factor(datain$loan)
datain$contact <- as.factor(datain$contact)
datain$month <- sapply(datain$month,function(x) grep(paste("(?i)",x,sep=""),month.abb))
datain$month <- as.factor(datain$month)
datain$day_of_week <- dplyr::recode(datain$day_of_week, 
       "mon"="1",
       "tue"="2",
       "wed"="3",
       "thu"="4",
       "fri"="5",
       "sat"="6",
       "sun"="7")
datain$day_of_week <- as.factor(datain$day_of_week)
datain$poutcome <- as.factor(datain$poutcome)

# setting up parms
init = mice(datain, maxit=0) 
meth = init$method
predM = init$predictorMatrix

# impute the following columns
meth[c("job", "marital","education","default","housing", "loan")]="polyreg"

# imputate & new dataset
set.seed(123)
imputed = mice(datain, method=meth, predictorMatrix=predM, m=1)
imputed <- complete(imputed)
sapply(imputed, function(x) sum(is.na(x)))

# convert to numeric
imputed$job <- as.numeric(imputed$job)
imputed$marital <- as.numeric(imputed$marital)
imputed$education <- as.numeric(imputed$education)
imputed$default <- as.numeric(imputed$default)
imputed$housing <- as.numeric(imputed$housing)
imputed$loan <- as.numeric(imputed$loan)
imputed$month <- as.numeric(imputed$month)
imputed$contact <- as.numeric(imputed$contact)
imputed$poutcome <- as.numeric(imputed$poutcome)
imputed$day_of_week <- as.numeric(imputed$day_of_week)

EDAD <- imputed

EDAD_conti <- datain[, !sapply(datain, is.factor)]
EDAD_categ <- datain[, sapply(datain, is.factor)]
```
## EDA (don't bulk run. Open and pick plot)
```{r}
EDAD$y <- as.factor(EDAD$y)
EDAD$y <- as.numeric(EDAD$y)

# Histogram of each variable
par(mfrow = c(5, 4))
MASS::truehist(EDAD$age)
MASS::truehist(EDAD$job)
MASS::truehist(EDAD$marital)
MASS::truehist(EDAD$education)
MASS::truehist(EDAD$default)
MASS::truehist(EDAD$housing)
MASS::truehist(EDAD$loan)
MASS::truehist(EDAD$contact)
MASS::truehist(EDAD$month)
MASS::truehist(EDAD$day_of_week)
MASS::truehist(EDAD$duration)
MASS::truehist(EDAD$campaign)
MASS::truehist(EDAD$pdays)
MASS::truehist(EDAD$previous)
MASS::truehist(EDAD$poutcome)
MASS::truehist(EDAD$emp.var.rate)
MASS::truehist(EDAD$cons.price.idx)
MASS::truehist(EDAD$cons.conf.idx)
MASS::truehist(EDAD$euribor3m)
MASS::truehist(EDAD$nr.employed)

# Correlation to y variable
x <- EDAD %>% correlate() %>% focus(y)
x

# Pairs plot. Colored by Yes/No (LONG RUN TIME)
#pairs(EDAD[1:20],main="Whole Dataset", pch=19, col=as.numeric(EDAD$y)+1)
#mtext("y column: red-> Yes; green-> No", 1, line=3.7,cex=.8)
```
## EDA Box plots, Scatter plots, Correlation plots
```{r EDA - Box plots, Scatter plots, Correlation plots}


# Box plots to find outliers
EDAD_conti <- dplyr::select(EDAD_conti, -11)
boxplot(EDAD_conti, main ='Box plot - all variables')
ggplot(stack(EDAD_conti),aes(x = ind, y = values) ) + geom_boxplot()
boxplot(EDAD_conti$pdays, main ='Box plot for pdays')

# Pdays is number of days passed after client was last contacted from previous campaign.
# Value of 999 means the client was not contacted. This values is sticking out as an outlier. Hence recoding the value to -1
EDAD_conti$pdays <- as.integer(revalue(as.character(EDAD_conti$pdays), c("999" = "-1")))
boxplot(EDAD_conti$pdays, main ='Box plot for pdays')

boxplot(EDAD_conti$duration)

# ScatterPlot
pairs(EDAD_conti, pch=19)

# Computing the p value of correlations
#cor.mtest <- function(mat, ...) {
#    mat <- as.matrix(mat)
#    n <- ncol(mat)
#    p.mat<- matrix(NA, n, n)
#    diag(p.mat) <- 0
#    for (i in 1:(n - 1)) {
#        for (j in (i + 1):n) {
#            tmp <- cor.test(mat[, i], mat[, j], ...)
#            p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
#        }
#    }
#  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
#  p.mat
#}
#p.mat <- cor.mtest(EDAD)
#correlation <- cor(EDAD)

# correlation
cor.data <- EDAD
cor.data$y <- as.numeric(as.factor(cor.data$y))
cor(cor.data[-21], cor.data$y) 

# Correlation plot with significance level of 0.05
#corrplot(correlation, type="upper", order="hclust", main = "Correlation Matrix", p.mat = p.mat, sig.level = 0.05)


#cat_cor <- GKtauDataframe(EDAD_categ)
p#lot(cat_cor, corrColors = "blue", type="upper")

library(gplots)
library(ggpubr)
# Heat map to find correlation
heatmap.2(correlation,col=redgreen(75), 
          density.info="none", trace="none", dendrogram=c("row"), 
          symm=F,symkey=T,symbreaks=T, scale="none")


ggscatter(EDAD_conti, x = "pdays", y = "previous", 
          title = "Correlation Matrix",
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Pdays", ylab = "Previous Contacts")


#attach(EDAD)
#summary(EDAD)
#str(EDAD)

```
## EDA PCA
```{r EDA- PCA}
pc.result<-prcomp(EDAD_conti,scale.=TRUE)
pc.result
pc.scores<-pc.result$x
pc.scores<-data.frame(pc.scores)
pc.scores$y<-EDAD$y

# variance
pr_var <- (pc.result$sdev)^2
# % of variance explained
prop_varex <- pr_var/sum(pr_var)
# show percentage of variance of each component
plot(prop_varex, xlab = "Principal Component", ylab = "Proportion of Variance Explained", type = "b" )
# Scree Plot
plot(cumsum(prop_varex), xlab = "Principal Component", ylab = "Cumulative Proportion of Variance Explained", type = "b" )
# first 6 components are responsible for around 90% of variance
cumsum(prop_varex)
# This result means that the majority of information contained in 11 numeric variables can be reduced to 6 principal components.

ggplot(data = pc.scores, aes(x = PC4, y = PC5)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Auto")

```

# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

# Objective 1 - Simple Logistic Regression
## Logit - Data splitting
```{r}
## Splitting and Down Sampling
dim(imputed)
prop.table(table(imputed$y))
imputed$y <- as.factor(imputed$y)
set.seed(1234)
training.samples <- imputed$y %>%
  createDataPartition(p = 0.7, list = FALSE)
train <- imputed[training.samples, ]
test <- imputed[-training.samples, ]
table(train$y)
prop.table(table(train$y))
table(test$y)
prop.table(table(test$y))
# 50-50 down sampling
train_down_s_50 <- SMOTE(form = y~.,data = train, k = 5, perc.over = 100)
attach(train_down_s_50)
table(train_down_s_50$y)
prop.table(table(train_down_s_50$y))
# 60-40 down sampling
train_bal_s_60 <- SMOTE(form = y~.,data = train, k = 5, perc.over = 390)
table(train_bal_s_60$y)
prop.table(table(train_bal_s_60$y))
# Ovun down sampling
train_bal_o <-ovun.sample(y ~ .,data = train, method = "under", N = 9280)$data
table(train_bal1$y)
prop.table(table(train_bal1$y))
# ROSE Down sampling
train_bal_r <- ROSE(y ~ ., data = train)$data
table(train_bal_r$y)
prop.table(table(train_bal_r$y))
```

## Logit - Full Model - SMOTE 60 - 40 Sampled data 88.6%
```{r}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
attach(logit.os.Train)
# Build Full Model
full.os.log<-glm(y~.,family="binomial",data=logit.os.Train)
# Step Model
step.os.log<-full.os.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
```

### Logit - Full Model - Original data 90.8%
```{r}
set.seed(1234)
logit.os.Train <- train
logit.os.Test <- test
logit.os.data$y <- as.factor(logit.os.Train$y)
# Build Full Model
full.os.log<-glm(y~.,family="binomial",data=logit.os.Train)
# Step Model
step.os.log<-full.os.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
```

### Logit - Full Model - SMOTE 50- 50 Sampled data 86.6%
```{r}
set.seed(1234)
logit.os.Train <- train_down_s_50
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
attach(logit.os.Train)
# Build Full Model
full.os.log<-glm(y~.,family="binomial",data=logit.os.Train)
# Step Model
step.os.log<-full.os.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
```

### Logit - Full Model - ROSE 85.5%
```{r}
set.seed(1234)
logit.os.Train <- train_bal_r
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
attach(logit.os.Train)
# Build Full Model
full.os.log<-glm(y~.,family="binomial",data=logit.os.Train)
# Step Model
step.os.log<-full.os.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
```

1. Using SMOTE 60-40 sampled data as the train data
2. From the full model and the significant predictors from step wise model and taking into account the correlated predictors
   below are the some of the models.

## Logit - Reduced Model 7(job,education,campaign,pdays,poutcome,nr.employed, emp.var.rate)
## Accuracy - 84.2%, Sensitivity - 89.4, Specificity - 44.1, MC - 0.157332, AUC - 0.7119
```{r reduced model 7}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod7.log <- glm(y ~ job + education + campaign + pdays + poutcome + nr.employed + emp.var.rate, family="binomial",data=logit.os.Train)
# Step Model
step.os.log <- red.mod7.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff <- 0.5
class.step <- factor(ifelse(fit.pred.step > cutoff,"yes","no"))
print("Confusion matrix for Stepwise")
confusionMatrix(as.factor(class.step), logit.os.Test$y)

obj1.mod.acc <- 0.842
obj1.mod.sens <- 0.894
obj1.mod.spec <- 0.441

#MisCalc Rates
misClasificError <- mean(class.step != logit.os.Test$y)
print("Miscalculation Rate")
misClasificError

# Roc Curves & value
p <- predict(step.os.log, newdata=logit.os.Test, type="response")
pr <- prediction(p, logit.os.Test$y)
# TPR = sensitivity, FPR=specificity
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
plot(prf, main = paste("ROC Curve  \n AUC = ", auc), col = "blue")

# Leverage plots
par(mfrow=c(2,2))
plot(step.os.log) 

```

## Logit - Reduced Model 1(job,month,campaign,pdays,poutcome,emp.var.rate,cons.price.idx) 
## Accuracy - 81.1%, Sensitivity - 84.9, Specificity - 50.9, MC - 0.188653, AUC - 0.7206
```{r reduced model 1}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
attach(logit.os.Train)
# Build Full Model
red.mod1.log <- glm(y~ job + month + campaign + pdays + poutcome + emp.var.rate + cons.price.idx ,family="binomial",data=logit.os.Train)
# Step Model
step.os.log <- red.mod1.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test, type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"))
confusionMatrix(as.factor(class.step),logit.os.Test$y)

#MisCalc Rates
misClasificError <- mean(class.step != logit.os.Test$y)
print("Miscalculation Rate")
misClasificError


# Roc Curves & value
p <- predict(step.os.log, newdata=logit.os.Test, type="response")
pr <- prediction(p, logit.os.Test$y)
# TPR = sensitivity, FPR=specificity
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
plot(prf, main = paste("ROC Curve  \n AUC = ", auc), col = "blue")
```

## Logit - Reduced Model 2(age,job,contact,day_of_week,campaign,pdays,poutcome,emp.var.rate,nr.employed)
## Accuracy - 79.3%, Sensitivity - 85.3, Specificity - 43.5, MC - 0.206863, AUC - 0.7011984
```{r reduced model 2}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod2.log <- glm(y ~ age + job + contact + day_of_week + campaign + pdays + poutcome + emp.var.rate + nr.employed , family="binomial",data=logit.os.Train)
# Step Model
step.os.log <- red.mod2.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"))
confusionMatrix(as.factor(class.step),logit.os.Test$y)

#MisCalc Rates
misClasificError <- mean(class.step != logit.os.Test$y)
print("Miscalculation Rate")
misClasificError

# Metrics
sum(diag(conf.step))/sum(conf.step)
sensitivity(class.step, logit.os.Test$y,threshold = cutoff)
specificity(class.step, logit.os.Test$y,threshold = cutoff)

# Roc Curves & value
p <- predict(step.os.log, newdata=logit.os.Test, type="response")
pr <- prediction(p, logit.os.Test$y)
# TPR = sensitivity, FPR=specificity
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
plot(prf, main = paste("ROC Curve  \n AUC = ", auc), col = "blue")
```

## Logit - Reduced Model 4(job,contact,day_of_week,campaign,pdays,poutcome,euribor3m)
## Accuracy - 80.5%, Sensitivity - 85.4, Specificity - 42.03, MC - 0.1947232, AUC - 0.6911033
```{r reduced model 4}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod4.log <- glm(y ~ job + contact + day_of_week + campaign + pdays + poutcome + euribor3m , family="binomial",data=logit.os.Train)
# Step Model
step.os.log <- red.mod4.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"))
confusionMatrix(as.factor(class.step),logit.os.Test$y)

#MisCalc Rates
misClasificError <- mean(class.step != logit.os.Test$y)
print("Miscalculation Rate")
misClasificError

# Metrics
sum(diag(conf.step))/sum(conf.step)
sensitivity(class.step, logit.os.Test$y,threshold = cutoff)
specificity(class.step,logit.os.Test$y,threshold = cutoff)

# Roc Curves & value
p <- predict(step.os.log, newdata=logit.os.Test, type="response")
pr <- prediction(p, logit.os.Test$y)
# TPR = sensitivity, FPR=specificity
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
plot(prf, main = paste("ROC Curve  \n AUC = ", auc), col = "blue")
```

## Logit - Reduced Model 6(job,education,campaign,poutcome,nr.employed)
## Accuracy - 80.7%, Sensitivity - 83.7, Specificity - 57.0, MC - 0.192780, AUC - 0.7387
```{r reduced model 6}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod6.log <- glm(y ~ job + education  +  campaign  + poutcome + nr.employed , family="binomial",data=logit.os.Train)
# Step Model
step.os.log <- red.mod6.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"))
print("Confusion matrix for Stepwise")
confusionMatrix(as.factor(class.step), logit.os.Test$y)

#MisCalc Rates
misClasificError <- mean(class.step != logit.os.Test$y)
print("Miscalculation Rate")
misClasificError

# Metrics
sum(diag(conf.step))/sum(conf.step)
sensitivity(class.step, logit.os.Test$y,threshold = cutoff)
specificity(class.step,logit.os.Test$y,threshold = cutoff)

# Roc Curves & value
p <- predict(step.os.log, newdata=logit.os.Test, type="response")
pr <- prediction(p, logit.os.Test$y)
# TPR = sensitivity, FPR=specificity
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
plot(prf, main = paste("ROC Curve  \n AUC = ", auc), col = "blue")
```

## Logit - Reduced Model 3(job,contact,day_of_week,campaign,pdays,poutcome,euribor3m,nr.employed)
## Accuracy - 79.99%, Sensitivity - 84.3, Specificity - 45.7, VIF ISSUES!
```{r reduced model 3}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod3.log <- glm(y ~ job + contact + day_of_week + campaign + pdays + poutcome + euribor3m + nr.employed , family="binomial",data=logit.os.Train)
# Step Model
step.os.log <- red.mod3.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"))
confusionMatrix(as.factor(class.step),logit.os.Test$y)

step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
print("Sensitivity")
sensitivity(class.step,logit.os.Test$y,threshold = cutoff)
print("Specificity")
specificity(class.step,logit.os.Test$y,threshold = cutoff)
```

## Logit - Reduced Model 5(job,contact,education,day_of_week,campaign,pdays,poutcome,euribor3m,nr.employed)
## Accuracy - 80.1%, Sensitivity - 84.5, Specificity - 45.1, VIF ISSUES!
```{r reduced model 5}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod5.log <- glm(y ~ job + education + contact + day_of_week + campaign + pdays + poutcome + euribor3m + nr.employed , family="binomial",data=logit.os.Train)
# Step Model
step.os.log <- red.mod5.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"yes","no"))
confusionMatrix(as.factor(class.step),logit.os.Test$y)


step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
print("Sensitivity")
sensitivity(class.step,logit.os.Test$y,threshold = cutoff)
print("Specificity")
specificity(class.step,logit.os.Test$y,threshold = cutoff)
```

# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

# Objective 2
## Final Datasets to be used in objective 2
```{r}
set.seed(123)

# Select transformations
EDAD2 <- EDAD
EDAD2$pdays <- ifelse(EDAD2$pdays == 999, -1, EDAD2$pdays)
EDAD2$age <- log(EDAD2$age)


# EDA dataset - narrowed down to columns to be used in regression
edadata <- EDAD2
edadata <- edadata %>%
  select(age, job, marital, education, contact, month, day_of_week, campaign, pdays, previous,
         poutcome, cons.price.idx, cons.conf.idx, y)

obj2.data <- edadata

# LDA & QDA Data
EDAD3 <- EDAD
EDAD3$pdays <- ifelse(EDAD3$pdays == 999, -1, EDAD3$pdays)
EDAD3$age <- log(EDAD3$age)

edadata2 <- EDAD3
edadata2 <- edadata2 %>%
  select(age, campaign, pdays, previous, emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed, y)
edadata2 <- japply(edadata2, which(sapply(edadata2, class)=="integer"), as.numeric )

lda.qda.data <- edadata2
```

# --------------------------------------------------------

## Logit Round 2 - Interactive Model (job,education,campaign,pdays,poutcome,nr.employed, emp.var.rate)
## Objective 1 model with added complexity
## Accuracy - 84.2%, Sensitivity - 89.4, Specificity - 44.1, MC - 0.157332, AUC - 0.7119
```{r Objective 1 complex model}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
# Build Full Model
red.mod7.log <- glm(y ~job + education + campaign + pdays + poutcome + nr.employed + emp.var.rate, family="binomial",data=logit.os.Train)
# Step Model
step.os.log <- red.mod7.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff <- 0.5
class.step <- factor(ifelse(fit.pred.step > cutoff,"yes","no"))
print("Confusion matrix for Stepwise")
confusionMatrix(as.factor(class.step), logit.os.Test$y)

obj1.mod2.acc <- 0.842
obj1.mod2.sens <- 0.894
obj1.mod2.spec <- 0.441

#MisCalc Rates
misClasificError <- mean(class.step != logit.os.Test$y)
print("Miscalculation Rate")
misClasificError

# Roc Curves & value
p <- predict(step.os.log, newdata=logit.os.Test, type="response")
pr <- prediction(p, logit.os.Test$y)
# TPR = sensitivity, FPR=specificity
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
plot(prf, main = paste("ROC Curve  \n AUC = ", auc), col = "blue")

# Leverage plots
par(mfrow=c(2,2))
plot(step.os.log) 

```

# --------------------------------------------------------

## Simple with original 87.85%
```{r}
set.seed(123)

simple.o.lr <- obj2.data
simple.o.lr$y <- as.numeric(as.factor(simple.o.lr$y))

# Create Training and Test data -
simple.index <- sample(1:nrow(simple.o.lr), 0.7*nrow(simple.o.lr))  # row indices for training data
simple.o.train <- simple.o.lr[simple.index, ]  # model training data
simple.o.test  <- simple.o.lr[-simple.index, ]   # test data

# Build the model on training data -
simple.o.lm <- lm(y ~ ., data=simple.o.train)  # build the model
simple.o.pred <- as.numeric(round(predict(simple.o.lm, simple.o.test)))  # predict y

# AIC & BIC
simple.o.aic <- AIC(simple.o.lm)
simple.o.bic <- BIC(simple.o.lm)
simple.o.aic
simple.o.bic

# Summary
summary(simple.o.lm)

# Prediction accuracy and eror rates
actuals_preds <- data.frame(cbind(actuals=simple.o.test$y, predicteds=simple.o.pred))
min_max_accuracy <- mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals)
simple.o.acc <- min_max_accuracy
simple.o.acc
mape

# Error
simple.o.error <- mean(simple.o.test$y != simple.o.pred)
simple.o.error

# Confusion Matrix
simple.o.cm <- confusionMatrix(simple.o.pred, simple.o.test$y)
simple.o.cm

# Sensitivity and Specificity
simple.o.sens <- (251)/(251+147)
simple.o.sens
simple.o.spec <- (10858)/(10858+1101)
simple.o.spec

# MisCalc Rates
simple.o.misc <- mean(simple.o.pred != simple.o.test$y)

# Roc Curves & value
p <- predict(simple.o.lm , newdata=simple.o.test, type="response")
pr <- prediction(p, simple.o.test$y)
# TPR = sensitivity, FPR=specificity
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
simple.o.auc <- performance(pr, measure = "auc")
simple.o.auc <- simple.o.auc@y.values[[1]]
simple.o.auc

vif(simple.o.lm)
```
## Simple with balanced data 84.09%
```{r}
set.seed(123)

simple.b.lr <- obj2.data
simple.b.lr$y <- as.numeric(as.factor(simple.b.lr$y))

# Train Index
trainIndex <- createDataPartition(simple.b.lr$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.numeric(as.factor(balanced_split_data$y))

# Generate train/test datasets
simple.b.train <- balanced_split_data
simple.b.test  <- simple.b.lr[-simple.index, ]   # test data

# Build the model on training data -
simple.b.lm <- lm(y ~ ., data=simple.b.train)  # build the model
simple.b.pred <- predict(simple.b.lm, simple.b.test)  # predict y
simple.b.pred <- round(simple.b.pred)

# AIC & BIC
simple.b.aic <- AIC(simple.b.lm)
simple.b.bic <- BIC(simple.b.lm)

# Summary
summary(simple.b.lm)

# Prediction accuracy and eror rates
actuals_preds <- data.frame(cbind(actuals=simple.b.test$y, predicteds=simple.b.pred))
min_max_accuracy <- mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals)
simple.b.acc <- min_max_accuracy
simple.b.acc
mape

# Error
simple.b.error <- mean(simple.b.test$y != simple.b.pred)
simple.b.error

# Confusion Matrix
table(simple.b.test$y, simple.b.pred)

# Sensitivity and Specificity
simple.b.sens <- (7589)/(7589+3416)
simple.b.sens
simple.b.spec <- (837)/(837+515)
simple.b.spec

# MisCalc Rates
simple.b.misc <- mean(simple.b.pred != simple.b.test$y)


# Roc Curves & value
p <- predict(simple.b.lm , newdata=simple.b.test, type="response")
pr <- prediction(p, simple.b.test$y)
# TPR = sensitivity, FPR=specificity
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
simple.b.auc <- performance(pr, measure = "auc")
simple.b.auc <- simple.b.auc@y.values[[1]]
simple.b.auc

vif(simple.b.lm)
```
## Simple with rose data 83.28
```{r}
set.seed(123)

simple.r.lr <- obj2.data
simple.r.lr$y <- as.numeric(as.factor(simple.r.lr$y))

# Create Training and Test data -
simple.index <- sample(1:nrow(simple.r.lr), 0.7*nrow(simple.r.lr))  # row indices for training data
simple.r.train <- ROSE(y ~ .,data = simple.r.lr[simple.index, ])$data
simple.r.test  <- simple.r.lr[-simple.index, ]   # test data

# Build the model on training data -
simple.r.lm <- lm(y ~ ., data=simple.r.train)  # build the model
simple.r.pred <- predict(simple.r.lm, simple.r.test)  # predict y
simple.r.pred <- round(simple.r.pred)

# AIC & BIC
simple.r.aic <- AIC(simple.r.lm)
simplr.r.bic <- BIC(simple.r.lm)
simple.r.aic

# Summary
summary(simple.r.lm)

# Error
simple.r.error <- mean(simple.r.test$y != simple.r.pred)
simple.r.error

# Prediction accuracy and eror rates
actuals_preds <- data.frame(cbind(actuals=simple.r.test$y, predicteds=simple.r.pred))
min_max_accuracy <- mean(apply(actuals_preds, 1, min) / apply(actuals_preds, 1, max))
mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals))/actuals_preds$actuals)
simple.r.acc <- min_max_accuracy
simple.r.acc
mape

# Confusion Matrix
table(simple.r.test$y, simple.r.pred)

# Sensitivity and Specificity
simple.r.sens <- (7380)/(7380+3625)
simple.r.sens
simple.r.spec <- (845)/(845+507)
simple.r.spec

# MisCalc Rates
simple.r.misc <- mean(simple.r.pred != simple.r.test$y)
simple.r.misc

# Roc Curves & value
p <- predict(simple.r.lm , newdata=simple.r.test, type="response")
pr <- prediction(p, simple.r.test$y)
# TPR = sensitivity, FPR=specificity
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
simple.r.auc <- performance(pr, measure = "auc")
simple.r.auc <- simple.r.auc@y.values[[1]]
simple.r.auc

vif(simple.r.lm)
```

# --------------------------------------------------------

## LDA with original data 89.69%... Continious ONLY!
```{r}
set.seed(123)

# Split the data into training (70%) and test set (30%)
lda.o.data <- lda.qda.data
lda.o.data$y <- as.numeric(as.factor(lda.o.data$y))
training.samples <- lda.o.data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
LDA.o.train.data <- lda.o.data[training.samples, ]
LDA.o.test.data <- lda.o.data[-training.samples, ]

# Estimate preprocessing parameters
o.preproc.param <- LDA.o.train.data %>% 
  preProcess(method = c("center", "scale"))

# Transform the data using the estimated parameters
LDA.o.train.transformed <- o.preproc.param %>% predict(LDA.o.train.data)
LDA.o.test.transformed <- o.preproc.param %>% predict(LDA.o.test.data)

# Model
lda.o.model <- lda(y ~ ., data = LDA.o.train.transformed)
lda.o.model

# Make predictions
lda.o.predictions <- lda.o.model %>% predict(LDA.o.test.transformed)
lda.o.predictions$class <- as.numeric(lda.o.predictions$class)

# Error
lda.o.error <- mean(LDA.o.test.data$y != lda.o.predictions$class)
lda.o.error

# Confusion Matrix
lda.o.cm <- caret::confusionMatrix(as.factor(lda.o.predictions$class), as.factor(LDA.o.test.data$y))
lda.o.cm

# Accuracy
lda.o.acc <- 0.8898
lda.o.acc

# Sensitivity and Specificity
lda.o.sens <- 0.9587
lda.o.sens 
lda.o.spec <- 0.3291
lda.o.spec
```
## LDA with balanced data 67.16%... Continious ONLY!
```{r}
set.seed(123)

# Split the data into training (70%) and test set (30%)
lda.b.data <- lda.qda.data
lda.b.data$y <- as.numeric(as.factor(lda.b.data$y))

# Train Index
training.samples <- lda.b.data$y %>%
  createDataPartition(p = 0.7, list = FALSE)

#Seperate all 4640 "yes" in dataset
Y_data <-filter(lda.qda.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(lda.qda.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.factor(balanced_split_data$y)

# Generate train/test datasets
LDA.b.train.data <- balanced_split_data
LDA.b.test.data <- lda.o.data[-training.samples, ]

# Estimate preprocessing parameters
o.preproc.param <- LDA.b.train.data %>% 
  preProcess(method = c("center", "scale"))

# Transform the data using the estimated parameters
LDA.b.train.transformed <- o.preproc.param %>% predict(LDA.b.train.data)
LDA.b.test.transformed <- o.preproc.param %>% predict(LDA.b.test.data)

# Model
lda.b.model <- lda(y ~ ., data = LDA.b.train.transformed)
lda.b.model

# Make predictions
lda.b.predictions <- lda.b.model %>% predict(LDA.b.test.transformed)
lda.b.predictions$class <- as.numeric(lda.b.predictions$class)

# Error
lda.b.error <- mean(LDA.b.test.data$y != lda.b.predictions$class)
lda.b.error

# Confusion Matrix
lda.b.cm <- caret::confusionMatrix(as.factor(lda.b.predictions$class), as.factor(LDA.b.test.data$y))
lda.b.cm

# Accuracy
lda.b.acc <- 0.7239
lda.b.acc

# Sensitivity and Specificity
lda.b.sens <- 0.7269
lda.b.sens
lda.b.spec <- 0.6997
lda.b.spec
```
## LDA with rose data 69.25%... Continious ONLY!
```{r}
set.seed(123)

# Split the data into training (70%) and test set (30%)
lda.r.data <- lda.qda.data
lda.r.data$y <- as.factor(lda.r.data$y)
training.samples <- lda.r.data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
LDA.r.train.data <- ROSE(y ~ .,data = lda.r.data[training.samples, ])$data
LDA.r.test.data <- lda.r.data[-training.samples, ]

# Estimate preprocessing parameters
r.preproc.param <- LDA.r.train.data %>% 
  preProcess(method = c("center", "scale"))

# Transform the data using the estimated parameters
LDA.r.train.transformed <- r.preproc.param %>% predict(LDA.r.train.data)
LDA.r.test.transformed <- r.preproc.param %>% predict(LDA.r.test.data)

# Model
lda.r.model <- lda(y ~ ., data = LDA.r.train.transformed)
lda.r.model

# Make predictions
lda.r.predictions <- lda.r.model %>% predict(LDA.r.test.transformed)

# Error
lda.r.error <- mean(LDA.r.test.data$y != lda.r.predictions$class)
lda.r.error

# Confusion Matrix
lda.r.cm <- caret::confusionMatrix(lda.r.predictions$class, as.factor(LDA.r.test.data$y))
lda.r.cm

# Accuracy
lda.r.acc <- 0.7252
lda.r.acc

# Sensitivity and Specificity
lda.r.sens <- 0.7260
lda.r.sens
lda.r.spec <- 0.7184
lda.r.spec
```

# --------------------------------------------------------

## QDA with original data 88.17%... Continious ONLY!
```{r}
set.seed(123)

QDA.o.Data <- lda.qda.data
QDA.o.Data$y <- as.numeric(as.factor(QDA.o.Data$y))

training.samples <- QDA.o.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
QDA.o.train.data <- QDA.o.Data[training.samples, ]
QDA.o.test.data <- QDA.o.Data[-training.samples, ]

# Fit the model
qda.o.model <- qda(y ~ ., data = QDA.o.train.data)
qda.o.model

# Make predictions
qda.o.predictions <- qda.o.model %>% predict(QDA.o.test.data)

# Error
qda.o.error <- mean(QDA.o.test.data$y != qda.o.predictions$class)
qda.o.error

# Confusion Matrix
qda.o.cm <- caret::confusionMatrix(as.factor(qda.o.predictions$class), as.factor(QDA.o.test.data$y))
qda.o.cm

# Model accuracy
qda.o.acc <- 0.875
qda.o.acc

# Sensitivity and Specificity
qda.o.sens <- 0.9224
qda.o.sens
qda.o.spec <- 0.4889
qda.o.spec
```
## QDA with balanced data 87.39%... Continious ONLY!
```{r}
set.seed(123)

QDA.b.Data <- lda.qda.data

QDA.b.Data$y <- as.numeric(as.factor(QDA.b.Data$y))

# Train Index
trainIndex <- createDataPartition(QDA.b.Data$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(lda.qda.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(lda.qda.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.numeric(as.factor(balanced_split_data$y))

# Generate train/test datasets
QDA.b.train.data <- balanced_split_data
QDA.b.test.data <- QDA.b.Data[-training.samples, ]

# Fit the model
qda.b.model <- qda(y ~ ., data = QDA.b.train.data)
qda.b.model

# Make predictions
qda.b.predictions <- qda.b.model %>% predict(QDA.b.test.data)

# Error
qda.b.error <- mean(QDA.b.test.data$y != qda.b.predictions$class)
qda.b.error

# Confusion Matrix
qda.b.cm <- caret::confusionMatrix(as.factor(qda.b.predictions$class), as.factor(QDA.b.test.data$y))
qda.b.cm

# Model accuracy
qda.b.acc <- 0.8712
qda.b.acc

# Sensitivity and Specificity
qda.b.sens <- 0.9127
qda.b.sens
qda.b.spec <- 0.5333
qda.b.spec
```
## QDA with rose data 87.93%... Continious ONLY!
```{r}
set.seed(123)

QDA.r.Data <- lda.qda.data

QDA.r.Data$y <- as.numeric(as.factor(QDA.r.Data$y))

training.samples <- QDA.r.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
QDA.r.train.data <- ROSE(y ~ .,data = QDA.r.Data[training.samples, ])$data
QDA.r.test.data <- QDA.r.Data[-training.samples, ]

# Fit the model
qda.r.model <- qda(y ~ ., data = QDA.r.train.data)
qda.r.model

# Make predictions
qda.r.predictions <- qda.r.model %>% predict(QDA.r.test.data)

# Error
qda.r.error <- mean(QDA.r.test.data$y != qda.r.predictions$class)
qda.r.error

# Confusion Matrix
qda.r.cm <- caret::confusionMatrix(as.factor(qda.r.predictions$class), as.factor(QDA.r.test.data$y))
qda.r.cm

# Model accuracy
qda.r.acc <- 0.876
qda.r.acc

# Sensitivity and Specificity
qda.r.sens <- 0.9269
qda.r.sens
qda.r.spec <- 0.4615
qda.r.spec
```

# --------------------------------------------------------

## MDA with original data 88.69%
```{r}
set.seed(123)

MDA.o.Data <- obj2.data
MDA.o.Data$y <- as.numeric(as.factor(MDA.o.Data$y))

training.samples <- MDA.o.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)

MDA.o.train.data <- MDA.o.Data[training.samples, ]
MDA.o.test.data <- MDA.o.Data[-training.samples, ]

# Fit the model
mda.o.model <- mda(y ~ ., data = MDA.o.train.data)
mda.o.model

# Make predictions
mda.o.predictions <- mda.o.model %>% predict(MDA.o.test.data)

# Error
mda.o.error <- mean(MDA.o.test.data$y != mda.o.predictions)
mda.o.error

# Confusion Matrix
mda.o.cm <- caret::confusionMatrix(mda.o.predictions, as.factor(MDA.o.test.data$y))
mda.o.cm

# Model accuracy
mda.o.acc <- 0.8868
mda.o.acc

# Sensitivity and Specificity
mda.o.sens <- 0.9589
mda.o.sens
mda.o.spec <- 0.2996
mda.o.spec
```
## MDA with balanced data 73.14%
```{r}
set.seed(123)

MDA.b.Data <- obj2.data
MDA.b.Data$y <- as.numeric(as.factor(MDA.b.Data$y))

# Train Index
trainIndex <- createDataPartition(MDA.b.Data$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.numeric(as.factor(balanced_split_data$y))

# Generate train/test datasets
MDA.b.train.data <- balanced_split_data
MDA.b.test.data <- MDA.b.Data[-training.samples, ]

# Fit the model
mda.b.model <- mda(y ~ ., data = MDA.b.train.data)
mda.b.model

# Make predictions
mda.b.predictions <- mda.b.model %>% predict(MDA.b.test.data)

# Error
mda.b.error <- mean(MDA.b.test.data$y != mda.b.predictions)
mda.b.error

# Confusion Matrix
mda.b.cm <- caret::confusionMatrix(mda.b.predictions, as.factor(MDA.b.test.data$y))
mda.b.cm

# Model accuracy
mda.b.acc <- 0.7334
mda.b.acc

# Sensitivity and Specificity
mda.b.sens <- 0.7486
mda.b.sens
mda.b.spec <- 0.6095
mda.b.spec
```
## MDA with rose data 76.65%
```{r}
set.seed(123)

MDA.r.Data <- obj2.data
MDA.r.Data$y <- as.numeric(as.factor(MDA.r.Data$y))

training.samples <- MDA.r.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
MDA.r.train.data <- ROSE(y ~ .,data = MDA.r.Data[training.samples, ])$data
MDA.r.test.data <- MDA.r.Data[-training.samples, ]

# Fit the model
mda.r.model <- mda(y ~ ., data = MDA.r.train.data)
mda.r.model

# Make predictions
mda.r.predictions <- mda.r.model %>% predict(MDA.r.test.data)

# Error
mda.r.error <- mean(MDA.r.test.data$y != mda.r.predictions )
mda.r.error

# Confusion Matrix
mda.r.cm <- caret::confusionMatrix(mda.r.predictions, as.factor(MDA.r.test.data$y))
mda.r.cm

# Model accuracy
mda.r.acc <- 0.7666
mda.r.acc

# Sensitivity and Specificity
mda.r.sens <- 0.7901
mda.r.sens
mda.r.spec <- 0.5754
mda.r.spec
```

# --------------------------------------------------------

## FDA with original data 89.83%
```{r}
set.seed(123)

FDA.o.Data <- obj2.data
FDA.o.Data$y <- as.numeric(as.factor(FDA.o.Data$y))

training.samples <- FDA.o.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)

FDA.o.train.data <- FDA.o.Data[training.samples, ]
FDA.o.test.data <- FDA.o.Data[-training.samples, ]

# Fit the model
fda.o.model <- fda(y ~ ., data = FDA.o.train.data)

# Make predictions
fda.o.predictions <- fda.o.model %>% predict(FDA.o.test.data)

# Error
fda.o.error <- mean(FDA.o.test.data$y != fda.o.predictions)
fda.o.error

# Confusion Matrix
fda.o.cm <- caret::confusionMatrix(fda.o.predictions, as.factor(FDA.o.test.data$y))
fda.o.cm

# Model accuracy
fda.o.acc <- 0.8983
fda.o.acc

# Sensitivity and Specificity
fda.o.sens <- 0.9827
fda.o.sens
fda.o.spec <- 0.2115
fda.o.spec
```
## FDA with balanced data 67.88%
```{r}
set.seed(123)

FDA.b.Data <- obj2.data
FDA.b.Data$y <- as.numeric(as.factor(FDA.b.Data$y))

# Train Index
trainIndex <- createDataPartition(FDA.b.Data$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.numeric(as.factor(balanced_split_data$y))

# Generate train/test datasets
FDA.b.train.data <- balanced_split_data
FDA.b.test.data <- FDA.b.Data[-training.samples, ]

# Fit the model
fda.b.model <- fda(y ~ ., data = FDA.b.train.data)

# Make predictions
fda.b.predictions <- fda.b.model %>% predict(FDA.b.test.data)

# Error
fdab.error <- mean(FDA.b.test.data$y != fda.b.predictions)
fdab.error

# Confusion Matrix
fda.b.cm <- caret::confusionMatrix(fda.b.predictions, as.factor(FDA.b.test.data$y))
fda.b.cm

# Model accuracy
fda.b.acc <- 0.6819
fda.b.acc

# Sensitivity and Specificity
fda.b.sens <- 0.6896
fda.b.sens
fda.b.spec <- 0.6191
fda.b.spec
```
## FDA with rose data 67.36
```{r}
set.seed(123)

FDA.r.Data <- obj2.data
FDA.r.Data$y <- as.numeric(as.factor(FDA.r.Data$y))

training.samples <- FDA.r.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
FDA.r.train.data <- ROSE(y ~ .,data = FDA.r.Data[training.samples, ])$data
FDA.r.test.data <- FDA.r.Data[-training.samples, ]

# Fit the model
fda.r.model <- fda(y ~ ., data = FDA.r.train.data)

# Make predictions
fda.r.predictions <- fda.r.model %>% predict(FDA.r.test.data)

# Error
fda.r.error <- mean(FDA.r.test.data$y != fda.r.predictions )
fda.r.error

# Confusion Matrix
fda.r.cm <- caret::confusionMatrix(fda.r.predictions, as.factor(FDA.r.test.data$y))
fda.r.cm

# Model accuracy
fda.r.acc <- 0.6737
fda.r.acc

# Sensitivity and Specificity
fda.r.sens <- 0.6799
fda.r.sens
fda.r.spec <- 0.6228
fda.r.spec
```

# --------------------------------------------------------

## RDA with original data 89.85%
```{r}
set.seed(123)

RDA.o.Data <- obj2.data
RDA.o.Data$y <- as.numeric(as.factor(RDA.o.Data$y))

training.samples <- RDA.o.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
RDA.o.train.data <- RDA.o.Data[training.samples, ]
RDA.o.test.data <- RDA.o.Data[-training.samples, ]

# Fit the model
rda.o.model <- rda(y ~ ., data = RDA.o.train.data)

# Make predictions
rda.o.predictions <- rda.o.model %>% predict(RDA.o.test.data)

# Error
rda.o.error <- mean(RDA.o.test.data$y != rda.o.predictions$class)
rda.o.error

# Confusion Matrix
rda.o.cm <- caret::confusionMatrix(rda.o.predictions$class, as.factor(RDA.o.test.data$y))
rda.o.cm

# Model accuracy
rda.o.acc <- 0.8986
rda.o.acc

# Sensitivity and Specificity
rda.o.sens <- 0.9840
rda.o.sens
rda.o.spec <- 0.2034
rda.o.spec
```
## RDA with balanced data 85.36
```{r}
set.seed(123)

RDA.b.Data <- obj2.data
RDA.b.Data$y <- as.numeric(as.factor(RDA.b.Data$y))

# Train Index
trainIndex <- createDataPartition(RDA.b.Data$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.numeric(as.factor(balanced_split_data$y))

# Generate train/test datasets
RDA.b.train.data <- balanced_split_data
RDA.b.test.data <- RDA.b.Data[-training.samples, ]

# Fit the model
rda.b.model <- rda(y ~ ., data = RDA.b.train.data)

# Make predictions
rda.b.predictions <- rda.b.model %>% predict(RDA.b.test.data)

# Error
rda.b.error <- mean(RDA.b.test.data$y != rda.b.predictions$class)
rda.b.error

# Confusion Matrix
rda.b.cm <- caret::confusionMatrix(rda.b.predictions$class, as.factor(RDA.b.test.data$y))
rda.b.cm

# Model accuracy
rda.b.acc <- 0.8618
rda.b.acc

# Sensitivity and Specificity
rda.b.sens <- 0.9202
rda.b.sens
rda.b.spec <- 0.3868
rda.b.spec
```
## RDA with rose data 80.67%
```{r}
set.seed(123)

RDA.r.Data <- obj2.data
RDA.r.Data$y <- as.numeric(as.factor(RDA.r.Data$y))

training.samples <- RDA.r.Data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
RDA.r.train.data  <- ROSE(y ~ .,data = RDA.r.Data[training.samples, ])$data
RDA.r.test.data <- RDA.r.Data[-training.samples, ]

# Fit the model
rda.r.model <- rda(y ~ ., data = RDA.r.train.data)

# Make predictions
rda.r.predictions <- rda.r.model %>% predict(RDA.r.test.data)

# Error
rda.r.error <- mean(RDA.r.test.data$y != rda.r.predictions$class)
rda.r.error

# Confusion Matrix
rda.r.cm <- caret::confusionMatrix(rda.r.predictions$class, as.factor(RDA.r.test.data$y))
rda.r.cm

# Model accuracy
rda.r.acc <- 0.8833
rda.r.acc

# Sensitivity and Specificity
rda.r.sens <- 0.9526
rda.r.sens
rda.r.spec <- 0.3195
rda.r.spec
```

# --------------------------------------------------------

## RF with original data 89.64%
```{r}
# Split the data into training (70%) and test set (30%)
set.seed(123)
rf.o.data <- obj2.data
training.samples <- rf.o.data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
rf.o.train.data <- rf.o.data[training.samples, ]
rf.o.test.data <- rf.o.data[-training.samples, ]

# Converting ‘y’ to a factor
rf.o.train.data$y <- factor(rf.o.train.data$y)

# set cross validation parameters
trControl <- trainControl(method = "cv",
    number = 10,
    search = "grid")

# Training using ‘random forest’ algorithm
rf.o.model <- train(y~.,data = rf.o.train.data, method = "rf", 
                    metric = "Accuracy", importance = TRUE,
                    trControl = trControl,  ntree = 50)

print(rf.o.model)
plot(rf.o.model)

rf.o.model$bestTune$mtry
rf.o.acc <- max(rf.o.model$results$Accuracy)
rf.o.acc

# Predict
rf.o.predictions <- rf.o.model %>% predict(rf.o.test.data)

# Error
rf.o.error <- mean(rf.o.test.data$y != rf.o.predictions)
rf.o.error

# Confusion Matrix
rf.o.cm <- caret::confusionMatrix(rf.o.predictions, as.factor(rf.o.test.data$y))
rf.o.cm

# Model accuracy
rf.o.acc <- 0.8978
rf.o.acc

# Sensitivity and Specificity
rf.o.sens <- 0.9877
rf.o.sens
rf.o.spec <- 0.1897
rf.o.spec
```
## RF with balanced data 73.88%
```{r}
# Split the data into training (70%) and test set (30%)
set.seed(123)
rf.b.data <- obj2.data

# Train Index
trainIndex <- createDataPartition(rf.b.data$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.factor(balanced_split_data$y)

# Generate train/test datasets
rf.b.train.data <- balanced_split_data
rf.b.test.data <- rf.b.data[-training.samples, ]

# Converting ‘y’ to a factor
rf.b.train.data$y <- factor(rf.b.train.data$y)

# set cross validation parameters
trControl <- trainControl(method = "cv",
    number = 10,
    search = "grid")

# Training using ‘random forest’ algorithm
rf.b.model <- train(y~.,data = rf.b.train.data, method = "rf", 
                    metric = "Accuracy", importance = TRUE,
                    trControl = trControl, ntree = 50)

print(rf.b.model)
plot(rf.b.model)

rf.b.model$bestTune$mtry

# Predict
rf.b.predictions <- rf.b.model %>% predict(rf.b.test.data)

# Error
rf.b.error <- mean(rf.b.test.data$y != rf.b.predictions)
rf.b.error

# Confusion Matrix
rf.b.cm <- caret::confusionMatrix(rf.b.predictions, as.factor(rf.b.test.data$y))
rf.b.cm

# Model accuracy
rf.b.acc <- 0.8501
rf.b.acc

# Sensitivity and Specificity
rf.b.sens <- 0.8734
rf.b.sens
rf.b.spec <- 0.6667
rf.b.spec
```
## RF with rose data 88.68%
```{r}
# Split the data into training (70%) and test set (30%)
set.seed(123)
rf.r.data <- obj2.data
training.samples <- rf.r.data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
rf.r.train.data  <- ROSE(y ~ .,data = rf.r.data[training.samples, ])$data
rf.r.test.data <- rf.r.data[-training.samples, ]

# Converting ‘y’ to a factor
rf.r.train.data$y <- factor(rf.r.train.data$y)

# set cross validation parameters
trControl <- trainControl(method = "cv",
    number = 10,
    search = "grid")

# Training using ‘random forest’ algorithm
rf.r.model <- train(y~.,data = rf.r.train.data, method = "rf", 
                    metric = "Accuracy", importance = TRUE,
                    trControl = trControl, ntree = 50)

print(rf.r.model)
plot(rf.r.model)

rf.r.model$bestTune$mtry

# Predict
rf.r.predictions <- rf.r.model %>% predict(rf.r.test.data)

# Error
rf.r.error <- mean(rf.r.test.data$y != rf.r.predictions)
rf.r.error

# Confusion Matrix
rf.r.cm <- caret::confusionMatrix(rf.r.predictions, as.factor(rf.r.test.data$y))
rf.r.cm

# Model accuracy
rf.r.acc <- 0.8926
rf.r.acc

# Sensitivity and Specificity
rf.r.sens <- 0.9737
rf.r.sens
rf.r.spec <- 0.2536
rf.r.spec
```

# --------------------------------------------------------

## Ridge with original data 87.85%
```{r}
set.seed(123)

ridge.o.lr <- obj2.data

ridge.o.lr$y <- as.numeric(as.factor(ridge.o.lr$y))

ridge.Index <- sample(1:nrow(ridge.o.lr), 0.7*nrow(ridge.o.lr)) # indices for 70% training data
ridge.o.train <- ridge.o.lr[ridge.Index, ] # training data
ridge.o.test <- ridge.o.lr[-ridge.Index, ] # test data

ridge.o.model <- linearRidge(y ~ ., data = ridge.o.train)

summary(ridge.o.model)

ridge.o.pred <- as.numeric(predict(ridge.o.model, ridge.o.test))  # predict on test data
ridge.o.pred <- round(ridge.o.pred)
ridge.o.compare <- cbind (actual=ridge.o.test$y, ridge.o.pred)  # combine

# Accuracy
ridge.o.acc <- mean(apply(ridge.o.compare, 1, min)/apply(ridge.o.compare, 1, max)) 
ridge.o.acc

# Confusion Matrix
ridge.o.cm <- caret::confusionMatrix(as.factor(ridge.o.pred), as.factor(ridge.o.test$y))
ridge.o.cm

# Model accuracy
ridge.o.acc <- 0.899
ridge.o.acc

# Sensitivity and Specificity
ridge.o.sens <- 0.9868
ridge.o.sens
ridge.o.spec <- 0.1842
ridge.o.spec
```
## Ridge with balanced data 74.58%
```{r}
set.seed(123)

ridge.b.lr <- obj2.data

ridge.b.lr$y <- as.numeric(as.factor(ridge.b.lr$y))

# Train Index
ridge.Index <- createDataPartition(ridge.b.lr$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.numeric(as.factor(balanced_split_data$y))

# Generate train/test datasets
ridge.b.train <- balanced_split_data
ridge.b.test <- ridge.b.lr[-ridge.Index, ] # test data

ridge.b.model <- linearRidge(y ~ ., data = ridge.b.train)

summary(ridge.b.model)

ridge.b.pred <- predict(ridge.b.model, ridge.b.test)  # predict on test data
ridge.b.pred <- round(ridge.b.pred)
ridge.b.compare <- cbind (actual=ridge.b.test$y, ridge.b.pred)  # combine

# Confusion Matrix
ridge.b.cm <- caret::confusionMatrix(as.factor(ridge.b.pred), as.factor(ridge.b.test$y))
ridge.b.cm

# Model accuracy
ridge.b.acc <- 0.6819
ridge.b.acc

# Sensitivity and Specificity
ridge.b.sens <- 0.6895
ridge.b.sens
ridge.b.spec <- 0.6206
ridge.b.spec
```
## Ridge with rose data 74.18%
```{r}
set.seed(123)

ridge.r.lr <- obj2.data

ridge.r.lr$y <- as.numeric(as.factor(ridge.r.lr$y))

ridge.Index <- sample(1:nrow(ridge.r.lr), 0.7*nrow(ridge.r.lr)) # indices for 70% training data
ridge.r.train <- ROSE(y ~ .,data = ridge.r.lr[ridge.Index, ])$data
ridge.r.test <- ridge.r.lr[-ridge.Index, ] # test data

ridge.r.model <- linearRidge(y ~ ., data = ridge.r.train)

summary(ridge.r.model)

ridge.r.pred <- predict(ridge.r.model, ridge.r.test)  # predict on test data
ridge.r.pred <- round(ridge.r.pred)
ridge.r.compare <- cbind (actual=ridge.r.test$y, ridge.r.pred)  # combine

# Confusion Matrix
ridge.r.cm <- caret::confusionMatrix(as.factor(ridge.r.pred), as.factor(ridge.r.test$y))
ridge.r.cm

# Model accuracy
ridge.r.acc <- 0.6658
ridge.r.acc

# Sensitivity and Specificity
ridge.r.sens <- 0.6708
ridge.r.sens
ridge.r.spec <- 0.6250
ridge.r.spec
```

# --------------------------------------------------------

## LASSO Regression original data 89.68%
```{r}
set.seed(123)

lasso.o.lr <- obj2.data
lasso.o.lr$y <- as.factor(lasso.o.lr$y)

training.samples <- lasso.o.lr$y %>%
  createDataPartition(p = 0.7, list = FALSE)
lasso.o.train.data <- lasso.o.lr[training.samples, ]
lasso.o.test.data <- lasso.o.lr[-training.samples, ]

dat.o.train.x <- model.matrix(y~.,lasso.o.train.data)
dat.o.train.y <- lasso.o.train.data[,14]

cvfit <- cv.glmnet(dat.o.train.x, dat.o.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")

#CV misclassification error rate 
print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

#For final model predictions go ahead and refit lasso using entire
lasso.o.model<-glmnet(dat.o.train.x, dat.o.train.y, family = "binomial", type.measure = "class", nlambda = 1001)

dat.o.test.x<-model.matrix(y~.,lasso.o.test.data)
fit.o.pred.lasso <- predict(lasso.o.model, newx = dat.o.test.x, type = "response", s = cvfit$lambda.min, exact = FALSE)
fit.o.pred.lasso <- round(fit.o.pred.lasso)

#Lets use the predicted probablities to classify the observations and make a final confusion matrix for the two models.
cutoff<-0.5
class.o.lasso<-factor(ifelse(fit.o.pred.lasso>cutoff,"yes","no"),levels=c("no","yes"))

# Confusion Matrix
lasso.o.cm <- caret::confusionMatrix(as.factor(class.o.lasso), as.factor(lasso.o.test.data$y))
lasso.o.cm

# Model accuracy
lasso.o.acc <- 0.8969
lasso.o.acc

# Sensitivity and Specificity
lasso.o.sens <- 0.9903
lasso.o.sens
lasso.o.spec <- 0.1609
lasso.o.spec
```
## LASSO Regression balanced data 60.04%
```{r}
set.seed(123)

lasso.b.lr <- obj2.data
lasso.b.lr$y <- as.factor(lasso.b.lr$y)

# Train Index
training.samples <- createDataPartition(lasso.b.lr$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.numeric(as.factor(balanced_split_data$y))

# Generate train/test datasets
lasso.b.train.data <- balanced_split_data
lasso.b.test.data <- lasso.b.lr[-training.samples, ]

dat.b.train.x <- model.matrix(y~.,lasso.b.train.data)
dat.b.train.y <- lasso.b.train.data[,14]

cvfit <- cv.glmnet(dat.b.train.x, dat.b.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#CV misclassification error rate 
print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

#For final model predictions go ahead and refit lasso using entire
lasso.b.model<-glmnet(dat.b.train.x, dat.b.train.y, family = "binomial",lambda=cvfit$lambda.min, type.measure = "class", nlambda = 1001)

dat.b.test.x<-model.matrix(y~.,lasso.b.test.data)
fit.b.pred.lasso <- predict(lasso.b.model, newx = dat.b.test.x, type = "response", s = cvfit$lambda.min)

#Lets use the predicted probablities to classify the observations and make a final confusion matrix for the two models
cutoff<-0.5
class.b.lasso<-factor(ifelse(fit.b.pred.lasso>cutoff,"yes","no"))

# Confusion Matrix
lasso.b.cm <- caret::confusionMatrix(as.factor(class.b.lasso), as.factor(lasso.b.test.data$y))
lasso.b.cm

# Model accuracy
lasso.b.acc <- 0.6696
lasso.b.acc

# Sensitivity and Specificity
lasso.b.sens <- 0.6733
lasso.b.sens
lasso.b.spec <- 0.6401
lasso.b.spec
```
## LASSO Regression Rose data 82.74
```{r}
set.seed(123)

lasso.r.lr <- obj2.data
lasso.r.lr$y <- as.factor(lasso.r.lr$y)

training.samples <- lasso.r.lr$y %>%
  createDataPartition(p = 0.7, list = FALSE)
lasso.r.train.data <- ROSE(y ~ .,data = lasso.r.lr[training.samples, ])$data
lasso.r.test.data <- lasso.r.lr[-training.samples, ]

dat.r.train.x <- model.matrix(y~.,lasso.r.train.data)
dat.r.train.y <- lasso.r.train.data[,14]

cvfit <- cv.glmnet(dat.r.train.x, dat.r.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#CV misclassification error rate is little below .1
print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

#For final model predictions go ahead and refit lasso using entire
lasso.r.model<-glmnet(dat.r.train.x, dat.r.train.y, family = "binomial",lambda=cvfit$lambda.min)

dat.r.test.x<-model.matrix(y~.,lasso.r.test.data)
fit.r.pred.lasso <- predict(lasso.r.model, newx = dat.r.test.x, type = "response")

#Lets use the predicted probablities to classify the observations and make a final confusion matrix for the two models.  
cutoff<-0.5
class.r.lasso<-factor(ifelse(fit.r.pred.lasso>cutoff,"yes","no"))

# Confusion Matrix
lasso.r.cm <- caret::confusionMatrix(as.factor(class.r.lasso), as.factor(lasso.r.test.data$y))
lasso.r.cm

# Model accuracy
lasso.r.acc <- 0.8275
lasso.r.acc

# Sensitivity and Specificity
lasso.r.sens <- 0.8951
lasso.r.sens
lasso.r.spec <- 0.2945
lasso.r.spec
```

# --------------------------------------------------------

## Net Regression original data 89.81%
```{r}
set.seed(123)

net.o.lr <- obj2.data
net.o.lr$y <- as.factor(net.o.lr$y)

index = sample(1:nrow(net.o.lr), 0.7*nrow(net.o.lr)) 

train = net.o.lr[index,] # Create the training data 
test = net.o.lr[-index,] # Create the test data

dummies <- dummyVars(y ~ ., data = net.o.lr)
train_dummies = predict(dummies, newdata = train)
test_dummies = predict(dummies, newdata = test)

x = as.matrix(train_dummies)
y_train = train$y

x_test = as.matrix(test_dummies)
y_test = test$y

# Set training control
train_cont <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 5,
                              search = "random",
                              verboseIter = TRUE)

# Train the model
elastic_reg <- train(y ~ .,
                           data = train,
                           method = "glmnet",
                           preProcess = c("center", "scale"),
                           tuneLength = 10,
                           trControl = train_cont)


# Best tuning parameter
elastic_reg$bestTune

# Make predictions on test set
net.o.pred <- predict(elastic_reg, x_test)

# Confusion Matrix
net.o.cm <- caret::confusionMatrix(net.o.pred, as.factor(y_test))
net.o.cm

# Model accuracy
net.o.acc <- 0.8981
net.o.acc

# Sensitivity and Specificity
net.o.sens <- 0.9887
net.o.sens
net.o.spec <- 0.1605
net.o.spec
```
## Net Regression balanced data 68.34%
```{r}
set.seed(123)

net.b.lr <- obj2.data
net.b.lr$y <- as.factor(net.b.lr$y)

# Train Index
index <- createDataPartition(net.b.lr$y, p = .7,
                                  list = FALSE,
                                  times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <-filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.factor(balanced_split_data$y)

# Generate train/test datasets
train <- balanced_split_data
test = net.b.lr[-index,] # Create the test data

dummies <- dummyVars(y ~ ., data = net.b.lr)
train_dummies = predict(dummies, newdata = train)
test_dummies = predict(dummies, newdata = test)

x = as.matrix(train_dummies)
y_train = train$y

x_test = as.matrix(test_dummies)
y_test = test$y

# Set training control
train_cont <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 5,
                              search = "random",
                              verboseIter = TRUE)

# Train the model
elastic_reg <- train(y ~ .,
                           data = train,
                           method = "glmnet",
                           preProcess = c("center", "scale"),
                           tuneLength = 10,
                           trControl = train_cont)


# Best tuning parameter
elastic_reg$bestTune


# Make predictions on test set
net.b.pred <- predict(elastic_reg, x_test)

# Confusion Matrix
net.b.cm <- caret::confusionMatrix(net.b.pred, as.factor(y_test))
net.b.cm

# Model accuracy
net.b.acc <- 0.6834
net.b.acc

# Sensitivity and Specificity
net.b.sens <- 0.6891
net.b.sens
net.b.spec <- 0.6386
net.b.spec
```
## Net Regression rose data 67.99
```{r}
set.seed(123)

net.r.lr <- obj2.data
net.r.lr$y <- as.factor(net.r.lr$y)

index = sample(1:nrow(net.r.lr), 0.7*nrow(net.r.lr)) 

lasso.r.train.data <- ROSE(y ~ .,data = net.r.lr[training.samples, ])$data
test = net.r.lr[-index,] # Create the test data

dummies <- dummyVars(y ~ ., data = net.r.lr)
train_dummies = predict(dummies, newdata = train)
test_dummies = predict(dummies, newdata = test)

x = as.matrix(train_dummies)
y_train = train$y

x_test = as.matrix(test_dummies)
y_test = test$y

# Set training control
train_cont <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 5,
                              search = "random",
                              verboseIter = TRUE)

# Train the model
elastic_reg <- train(y ~ .,
                           data = train,
                           method = "glmnet",
                           preProcess = c("center", "scale"),
                           tuneLength = 10,
                           trControl = train_cont)


# Best tuning parameter
elastic_reg$bestTune


# Make predictions on test set
net.r.pred <- predict(elastic_reg, x_test)

# Confusion Matrix
net.r.cm <- caret::confusionMatrix(net.r.pred, as.factor(y_test))
net.r.cm

# Model accuracy
net.r.acc <- 0.6799
net.r.acc

# Sensitivity and Specificity
net.r.sens <- 0.6861
net.r.sens
net.r.spec <- 0.6294
net.r.spec
```

# --------------------------------------------------------

## KNN original data (not working)
```{r}
set.seed(123)

knn.o.data <- obj2.data
knn.o.data$y <- as.numeric(as.factor(knn.o.data$y))

# Test/Train Data
training.samples <- knn.o.data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
default_trn = knn.o.data[training.samples, ]
default_tst = knn.o.data[-training.samples, ]
default_cl = default_trn$y

# training data
X_default_trn = default_trn[, -14]
y_default_trn = default_trn$y

# testing data
X_default_tst = default_tst[, -14]
y_default_tst = default_tst$y

knn.o.model <- knn3Train(default_trn, default_tst, default_cl, k = 51, prob = TRUE)
cm = as.matrix(table(default_cl, knn.o.model))
```

## KNN
```{r}
knn.o.data <- obj2.data
knn.o.data$y <- as.numeric(as.factor(knn.o.data$y))

##Tells the percentage of the dataset that will go into the training set
splitPerc = .75

##Searching for the best k by looping for many k and the average of many training / test partition 
iterations = 5
numks = 30

masterAcc = matrix(nrow = iterations, ncol = numks)
  
for(j in 1:iterations)
{
accs = data.frame(accuracy = numeric(30), k = numeric(30))
trainIndices = sample(1:nrow(knn.o.data), 0.7*nrow(knn.o.data)) 
train = knn.o.data[trainIndices,]
test = knn.o.data[-trainIndices,]
for(i in 1:numks)
{
  classifications = knn(train,test,train$y, prob = TRUE, k = i)
  table(classifications,test$y)
  CM = confusionMatrix(table(classifications,test$y))
  masterAcc[j,i] = CM$overall[1]
}

}

MeanAcc = colMeans(masterAcc)

##Plot the outcome
plot(seq(1,numks,1),MeanAcc, type = "l")

##External Validation, k=5, unstandardized
classifications = knn(train,test,train$y, prob = TRUE, k=5)
table(classifications,test$y)
confusionMatrix(table(classifications,test$y))
```


# --------------------------------------------------------

## Objective 2 Results
```{r}
OBJ1.results <- cbind("Objective 1 Model", obj1.mod.acc, obj1.mod.sens, obj1.mod.spec)
SR.O.results <- cbind("Simple LR Original",simple.o.acc, simple.o.sens, simple.o.spec)
SR.B.results <- cbind("Simple LR Balanced",simple.b.acc, simple.b.sens, simple.b.spec)
SR.R.results <- cbind("Simple LR Rose",simple.r.acc, simple.r.sens, simple.r.spec)
LDA.O.results <- cbind("LDA Original",lda.o.acc, lda.o.sens, lda.o.spec)
LDA.B.results <- cbind("LDA Balanced",lda.b.acc, lda.b.sens, lda.b.spec)
LDA.R.results <- cbind("LDA Rose",lda.r.acc, lda.r.sens, lda.r.spec)
QDA.O.results <- cbind("QDA Original",qda.o.acc, qda.o.sens, qda.o.spec)
QDA.B.results <- cbind("QDA Balanced",qda.b.acc, qda.b.sens, qda.b.spec)
QDA.R.results <- cbind("QDA Rose",qda.r.acc, qda.r.sens, qda.r.spec)
MDA.O.results <- cbind("MDA Original",mda.o.acc, mda.o.sens, mda.o.spec)
MDA.B.results <- cbind("MDA Balanced",mda.b.acc, mda.b.sens, mda.b.spec)
MDA.R.results <- cbind("MDA Rose",mda.r.acc, mda.r.sens, mda.r.spec)
FDA.O.results <- cbind("FDA Original",fda.o.acc, fda.o.sens, fda.o.spec)
FDA.B.results <- cbind("FDA Balanced",fda.b.acc, fda.b.sens, fda.b.spec)
FDA.R.results <- cbind("FDA Rose",fda.r.acc, fda.r.sens, fda.r.spec)
RDA.O.results <- cbind("RDA Original",rda.o.acc, rda.o.sens, rda.o.spec)
RDA.B.results <- cbind("RDA Balanced",rda.b.acc, rda.b.sens, rda.b.spec)
RDA.R.results <- cbind("RDA Rose",rda.r.acc, rda.r.sens, rda.r.spec)
RIDGE.O.results <- cbind("Ridge Original",ridge.o.acc, ridge.o.sens, ridge.o.spec)
RIDGE.B.results <- cbind("Ridge Balanced",ridge.b.acc, ridge.b.sens, ridge.b.spec)
RIDGE.R.results <- cbind("Ridge Rose",ridge.r.acc, ridge.r.sens, ridge.r.spec)
LASSO.O.results <- cbind("Lasso Original",lasso.o.acc, lasso.o.sens, lasso.o.spec)
LASSO.B.results <- cbind("Lasso Balanced",lasso.b.acc, lasso.b.sens, lasso.b.spec)
LASSO.R.results <- cbind("Lasso Rose",lasso.r.acc, lasso.r.sens, lasso.r.spec)
NET.O.results <- cbind("Net Original",net.o.acc, net.o.sens, net.o.spec)
NET.B.results <- cbind("Net Balanced",net.b.acc, net.b.sens, net.b.spec)
NET.R.results <- cbind("Net Rose",net.r.acc, net.r.sens, net.r.spec)
RF.O.results <- cbind("RF Original",rf.o.acc, rf.o.sens, rf.o.spec)
RF.B.results <- cbind("RF Balanced",rf.b.acc, rf.b.sens, rf.b.spec)
RF.R.results <- cbind("RF Rose",rf.r.acc, rf.r.sens, rf.r.spec)

accuracy <- rbind(OBJ1.results,
                  SR.O.results, SR.B.results, SR.R.results,
                  LDA.O.results, LDA.B.results, LDA.R.results,
                  QDA.O.results, QDA.B.results, QDA.R.results,
                  MDA.O.results, MDA.B.results, MDA.R.results,
                  FDA.O.results, FDA.B.results, FDA.R.results,
                  RDA.O.results, RDA.B.results, RDA.R.results,
                  RIDGE.O.results, RIDGE.B.results, RIDGE.R.results,
                  LASSO.O.results, LASSO.B.results, LASSO.R.results,
                  NET.O.results, NET.B.results, NET.R.results,
                  RF.O.results, RF.B.results, RF.R.results)

colnames(accuracy) <- c("Method", "Accuracy", "Sensitivity", "Specificity")

accuracytbl <- as.data.frame(accuracy)
accuracytbl$Accuracy <- as.numeric(accuracytbl$Accuracy)
accuracytbl$Sensitivity <- as.numeric(accuracytbl$Sensitivity)
accuracytbl$Specificity <- as.numeric(accuracytbl$Specificity)


# Neat table
customGreen = "#009900"
customRed = "#ff7f7f"

formattable(accuracytbl, digits = 3,
            align = c("l",rep("r", NCOL(accuracytbl) - 1)),
            list(`Method` = formatter("span", style = ~ style(color = "black", font.weight = "bold")), 
                 'Accuracy' = percent, 
                 'Sensitivity' = percent,
                 'Specificity' = percent,
                 'Accuracy' = color_text(customRed, customGreen),
                 'Sensitivity' = color_text(customRed, customGreen),
                 "Specificity" = color_text(customRed, customGreen)))
```


